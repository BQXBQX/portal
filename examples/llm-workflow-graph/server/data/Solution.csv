id|name|description
32545439-d5a9-533d-9b41-fa0fc5274da1|Task Splitting Technique|The task splitting technique is designed to handle the skewed workloads of local search tasks brought by the power law degree distribution in real-world data graphs. This technique splits large tasks into smaller subtasks to achieve a more balanced workload distribution among reducers.
d9a3aa25-e33b-5dad-ba54-29d43f6924e9|Local Database Cache|The local database cache is designed to reduce the communication cost by storing the adjacency sets fetched from the distributed database in an in-memory cache. This cache can capture both intra-task and inter-task locality, reducing the number of database queries.
e4196641-1351-54b1-a788-8ba54aae1a40|On-Demand Shuffle Technique|The on-demand shuffle technique is designed to reduce the communication cost by querying the database as needed during enumeration, rather than shuffling the entire data graph before enumeration.
52035306-c5fe-536b-87ef-bf41991ad57f|Backtracking-Based Framework|The backtracking-based framework is designed to efficiently enumerate subgraphs by incrementally mapping each pattern vertex to data vertices in the match, following a given matching order.
7dd64307-dbd4-533f-a2d1-b94e2cd2d36b|On-Demand Shuffling Technique|The authors propose an on-demand shuffling technique to minimize communication overhead by storing the edges of the data graph in a distributed database and querying the database as needed during enumeration. This approach avoids shuffling the entire data graph before enumeration, reducing unnecessary data transfer and communication costs.
9552912a-4059-512a-b763-a63ac86316e2|Symmetric Rectilinear Partitioning|The authors propose a novel symmetric rectilinear partitioning strategy to address the scalability challenges in graph processing. This approach partitions the graph into smaller subgraphs, allowing for more efficient processing and reducing memory requirements. The symmetric rectilinear partitioning strategy generates blocks with matching source and destination vertex IDs, reducing the amount of edge traversal and memory usage. This approach is different from existing methods, which often use 1D partitioning or other techniques that may lead to poor load balancing and increased memory usage. The paper demonstrates that the symmetric rectilinear partitioning strategy achieves better performance and scalability compared to existing methods, with up to 18x speedup on large graphs and up to 22x speedup on smaller instances.
63c7907f-10c8-5352-98db-0816c5689d28|Block-Based Triangle Counting Formulation|The authors propose a block-based triangle counting formulation to address the scalability challenges in graph processing. This approach divides the computation into smaller tasks, each depending on three blocks where edges of a triangle can appear. The block-based formulation uses a divide-and-conquer approach, partitioning the set of triangles in the graph and streaming these small tasks to compute resources. This approach is different from existing methods, which often use 1D task-based approaches or other techniques that may lead to poor load balancing and increased memory usage. The paper demonstrates that the block-based triangle counting formulation achieves better performance and scalability compared to existing methods, with up to 11x speedup over the fastest published end-to-end CPU execution times.
5bb12fa2-b5b1-5093-b515-8eecec120a31|Dynamic Task Scheduling|The authors propose a dynamic task scheduling scheme to address the scalability challenges in graph processing. This approach schedules tasks on multi-core CPUs and uses multiple streams on multiple GPUs to effectively utilize the massive computing capabilities on the GPUs. The dynamic task scheduling scheme uses a lightweight scheduler to schedule tasks on available devices, taking into account the workload of each task and the available resources. This approach is different from existing methods, which often use static scheduling or other techniques that may lead to poor load balancing and increased memory usage. The paper demonstrates that the dynamic task scheduling scheme achieves better performance and scalability compared to existing methods, with up to 5.6x speedup on the Friendster graph.
04fc1ddc-1e60-5bd5-b7d4-d68dc65536d9|Hybrid Execution and Cut-Off|The authors propose a hybrid execution approach with a cut-off mechanism to address the scalability challenges in graph processing. This approach assigns tasks to CPUs or GPUs based on their workload and available resources. The hybrid execution approach uses a cut-off mechanism to determine whether a task should be executed on a CPU or GPU, taking into account the workload of the task and the available resources. This approach is different from existing methods, which often use static assignment or other techniques that may lead to poor load balancing and increased memory usage. The paper demonstrates that the hybrid execution approach with a cut-off mechanism achieves better performance and scalability compared to existing methods, with up to 10x speedup on the Twitter graph.
bd552936-5761-5c8e-b693-91e8332deda0|Block-Based Triangle Counting (bbTC) Algorithm|The bbTC algorithm is a novel approach to triangle counting that addresses the load balancing challenge by partitioning the graph into smaller blocks and processing them in parallel. This approach ensures that the computational workload is evenly distributed across processing units, reducing the likelihood of bottlenecks and improving overall performance.
3aaaeb06-8e4a-5841-bde2-cbdbee4eb221|Asynchronous Data Movement and Computation Overlap|The authors propose an approach to overlap communication with computation by using asynchronous data movement. This allows the system to continue processing tasks while data is being transferred between nodes, reducing the overall communication overhead. The authors utilize CUDA streams to execute multiple tasks on GPUs simultaneously, which enables the overlap of communication and computation. They also experimentally determine the optimal number of CUDA streams to use, finding that four streams provide the best performance. The paper reports that the proposed approach achieves a 5.6 times faster execution time compared to the fastest published CPU-only time, and a 20% faster execution time compared to the state-of-the-art multi-GPU implementation.
18e18e9a-fe71-5bbf-bee1-ee1394b9f786|Task-Based Execution Model with Dynamic Scheduling|The authors propose a task-based execution model that dynamically schedules tasks on multi-core CPUs and multiple GPUs. This approach allows for efficient utilization of resources and minimizes communication overhead by assigning computationally heavy tasks to GPUs and lighter tasks to CPUs. The authors implement a dynamic task scheduling scheme that schedules tasks on multi-core CPUs and multiple GPUs. They also propose a cut-off mechanism to separate bottleneck tasks from lightweight tasks, ensuring that heavy tasks are assigned to GPUs. The paper reports that the proposed approach achieves a 6:1 and 8:8 speedup compared to state-of-the-art multi-core CPU algorithms, and a 2:2 speedup compared to the state-of-the-art multi-GPU algorithm.
63162715-0d07-5673-a40d-f712dd38f9bd|Asynchronous Distributed Algorithm for Graph Pruning|The authors propose an asynchronous distributed algorithm for graph pruning, which enables the efficient processing of large-scale graphs by pruning the graph to a precise, enumeration of all vertices and edges that participate in a match for arbitrary templates.
fbe59545-258d-56de-a382-b6e802bacbcd|Optimized Distributed Implementation|The authors offer an efficient implementation of their algorithm on top of HavoqGT, an open-source asynchronous graph processing framework, which enables scalability and high performance.
846e1da2-2758-5129-ba87-7935e8993e88|Work Aggregation Optimization|The authors propose a work aggregation optimization technique, which reduces the number of messages exchanged between nodes by detecting and eliminating redundant messages.
5427a350-871d-592a-89f5-bc345550415d|Load Balancing and Rebalancing|The authors propose a load balancing and rebalancing technique, which ensures that the graph is evenly distributed across processing cores, minimizing memory requirements and optimizing data structures.
48b7423f-33aa-5d83-a776-0e4c7b9a2497|Pseudo-Dynamic Load Balancing Strategy|The authors propose a pseudo-dynamic load balancing strategy to address the load balancing challenge. This strategy involves checkpointing the current state of execution, rebalancing the workload, and relaunching the computation. The rebalancing step redistributes the vertices and edges across processing cores to ensure even distribution.
771e5d2a-4b26-5633-b051-c75f22120075|Work Aggregation|Work Aggregation is a technique used to minimize communication overhead in distributed graph processing systems. It involves skipping duplicate checks in non-local constraint checking, thus preventing possible combinatorial explosion. Work Aggregation uses an unordered set at each vertex to detect if another copy of a token has already visited the vertex, taking a different path. This allows the system to avoid redundant checks and reduce the number of messages exchanged between nodes. The paper shows that Work Aggregation enables a 1050x performance gain for the WDC patterns, demonstrating its effectiveness in reducing communication overhead.
32c0d292-aed3-52c1-a4e0-0b8dbf75ede5|Edge Elimination|Edge Elimination is a technique used to reduce communication overhead by eliminating edges that do not participate in matches. This reduces the number of edges that need to be processed and communicated between nodes. Edge Elimination uses a token passing approach to identify and eliminate edges that do not meet the neighborhood constraints of the template graph. The paper shows that Edge Elimination reduces the number of active edges by a factor of 10-15, demonstrating its effectiveness in reducing communication overhead.
15399c3e-65ba-5f23-9584-2e9e1fd66504|Asynchronous NLCC|Asynchronous NLCC is a technique used to minimize communication overhead by allowing all walks to progress independently without synchronization overheads. Asynchronous NLCC uses a token passing approach to verify non-local constraints, allowing each walk to progress independently and reducing the need for synchronization. The paper shows that Asynchronous NLCC achieves a 3.1x and 1.3x speedup for the WDC 1 pattern on 64 and 128 nodes, respectively, demonstrating its effectiveness in reducing communication overhead.
d24063da-1cc8-5ada-a890-a5d08777924f|Distributed Neighbor Assignment Function (NAF) Algorithm|The authors propose a distributed algorithm to compute a Neighbor Assignment Function (NAF) in a radio network, which addresses the load balancing challenge by assigning each node to one of its neighbors to be its backup device, ensuring that each node has a load of one node to back up and is directly adjacent to its backup device. The algorithm uses a distributed and low-energy approach, where nodes wake up at random times and try to recruit one of their neighbors to pair with them. The algorithm also uses a three-step handshake protocol to ensure that both endpoints of the edge agree about who they are paired with. The authors show that the algorithm produces a NAF with a maximum load that is at most O(log n) times the optimum, where n is the number of nodes in the network.
6bb72384-08f6-57d2-8b23-6ae69c482f4d|Partial NAF Algorithm|The authors propose a partial NAF algorithm that addresses the load balancing challenge by assigning a subset of nodes to their neighbors, ensuring that each node has a load of one node to back up and is directly adjacent to its backup device. The algorithm uses a distributed and low-energy approach, where nodes wake up at random times and try to recruit one of their neighbors to pair with them. The algorithm also uses a three-step handshake protocol to ensure that both endpoints of the edge agree about who they are paired with. The authors show that the algorithm produces a partial NAF with a maximum load that is at most O(log n) times the optimum, where n is the number of nodes in the network.
49ec190b-7f1e-508a-a0e0-882f4ef11fa3|Maximal Matching Algorithm|The authors propose a maximal matching algorithm that addresses the load balancing challenge by finding a maximal matching in the network graph, which ensures that each node is paired with at most one neighbor. The algorithm uses a distributed and low-energy approach, where nodes wake up at random times and try to recruit one of their neighbors to pair with them. The algorithm also uses a three-step handshake protocol to ensure that both endpoints of the edge agree about who they are paired with. The authors show that the algorithm produces a maximal matching with a maximum load that is at most O(log n) times the optimum, where n is the number of nodes in the network.
9658b511-bb15-51c9-b062-be318fc65f13|Low-Energy Distributed Maximal Matching Algorithm|The authors propose a distributed algorithm for finding a maximal matching in a radio network, which is designed to minimize communication overhead by reducing the number of messages exchanged between nodes. The algorithm uses a randomized approach, where nodes wake up at random times and try to recruit one of their neighbors to pair with them. This approach reduces the number of collisions and minimizes the energy cost of communication. The paper shows that the algorithm achieves a maximal matching with high probability, while keeping the energy cost per node low, specifically O(log n log Δ) where n is the number of nodes and Δ is the maximum degree.
c44b78a0-d052-55c7-a6f9-7748a6a8c8fd|H Partition with Degree d and Size O(log n)|The authors propose an H partition with degree d and size O(log n) to address the scalability challenges in distributed graph coloring. This solution involves partitioning the nodes into log n disjoint subsets, such that every node v in subset Hj has at most d neighbors in subsets y jHj.
cfcdeb8c-da12-5449-bffc-31662baba123|Low Out Degree Orientation via H Partition|The authors propose a low out degree orientation via H partition to address the scalability challenges in distributed graph coloring. This solution involves computing an acyclic orientation of the edges such that the maximum out degree is at most d, where d is a parameter that can be adjusted based on the graph's arboricity.
2c0bcd4c-c0b2-5345-babc-6c99ed575716|Randomized Distributed Algorithm for Partial Coloring|The authors propose a randomized distributed algorithm for partial coloring to address the scalability challenges in distributed graph coloring. This solution involves each node selecting a random color from a palette of size 2^d, where d is the degree of the node, and then sending the selected color to its neighbors.
d4cb5495-53d8-530b-a7ea-9f4ca99c9340|Trade-off Low Arb Coloring Algorithm|The authors propose a trade-off low arb coloring algorithm to address the scalability challenges in distributed graph coloring. This solution involves each node selecting a random color from a palette of size 2, and then sending the selected color to its neighbors.
7bb9f83c-c187-57ae-bf9d-068e4dd0b1b5|H Partition with Degree d and Size log 2 2 n|The authors propose a deterministic distributed algorithm to compute an H partition of the graph with degree d and size log 2 2 n in O log 2 2 n rounds. This solution specifically addresses the communication overhead minimization challenge by reducing the number of messages exchanged between nodes. The algorithm achieves this by partitioning the nodes into layers, where each layer has a limited number of nodes, and then orienting the edges between layers to minimize the out-degree of each node.
e670bbdf-1dc1-5ea6-8c30-3ccc19cbb6ab|Low Arb Deterministic Coloring Algorithm|The authors propose a deterministic distributed algorithm for coloring the remaining graph after the first step of the algorithm. This solution specifically addresses the communication overhead minimization challenge by minimizing the number of colors used and the number of rounds required to color the graph.
8e9d5ea8-1560-597e-8428-a214d767c54a|Tradeo Low Arb Coloring Algorithm|The authors propose a randomized distributed algorithm for coloring the graph, which is a trade-off between the number of colors used and the number of rounds required. This solution specifically addresses the communication overhead minimization challenge by providing a flexible algorithm that can be tuned to minimize communication overhead.
ce76f75e-e283-54e2-89b8-a3983c67867c|Distributed 2-Approximation Algorithm for Minimum Weighted Vertex Cover (MWVC)|The authors propose a distributed 2-approximation algorithm for the Minimum Weighted Vertex Cover (MWVC) problem, which is a fundamental problem in graph theory. The algorithm is designed to efficiently handle large-scale graphs with billions of edges and vertices.
bd5a9dcc-f2e6-5fd1-94ec-38bbe4d48ed4|Local Computation with Reduced Message Size|The authors propose a modification to their algorithm to accommodate O(log n) bit messages in the CONGEST model. They achieve this by slightly modifying the vault value and using a pre-defined message for accepting requests, which reduces the message size.
1d4ce95b-dcc3-5ea5-8331-8bafd0dc5652|GRAMER Architecture|The authors propose a novel graph mining accelerator, GRAMER, which addresses the scalability challenges by providing a specialized memory hierarchy and pipeline parallelization. GRAMER’s architecture is designed to exploit the extension locality in graph mining, reducing the memory bandwidth requirements and improving computational parallelism.
0df8bfa4-01b3-5589-890f-86c9f6bf8181|Locality-Aware Memory Hierarchy|The authors propose a locality-aware memory hierarchy that is designed to exploit the extension locality in graph mining. The memory hierarchy is organized into a high-priority memory and a low-priority memory, which are used to store frequently accessed data and less frequently accessed data, respectively.
9e8287da-ac56-5490-9bb6-35f7baec555f|Pipeline Parallelization|The authors propose a pipeline parallelization technique that is designed to maximize computational parallelism in graph mining. The pipeline parallelization is designed to process the most intermediate results on-chip, reducing the memory bandwidth requirements.
aec8a945-6a14-5d0b-926d-ad5147e93b1e|Work Stealing Mechanism|The authors propose a work stealing mechanism that is designed to reduce load imbalance in graph mining. The work stealing mechanism is designed to dynamically allocate tasks to processing units (PUs) based on their workload.
04aca63d-84f4-596a-8e6a-9e4b23e1a7df|Locality Aware Memory Hierarchy|The authors propose a novel memory hierarchy that exploits the extension locality in graph mining to minimize communication overhead. This hierarchy consists of a high-priority memory for frequently accessed data and a low-priority memory for less frequently accessed data.
84e76f24-7cf6-52aa-a320-588d57ed1126|Pipelined Processing Units (PUs)|The authors design pipelined PUs to maximize computational parallelism and minimize off-chip communications. Each PU is responsible for handling multiple initial embeddings that may evolve into a significantly different number of workloads.
56ccb945-325e-584e-a785-c5d1710014d3|Thermal Load Balancing|This solution addresses the load balancing challenge by taking into account the thermal characteristics of the data center and distributing workload to minimize heat production and reduce energy consumption.
1e1ecc46-93dc-5646-be6f-0d9e9bb6b57e|Workload Consolidation|This solution addresses the load balancing challenge by consolidating workload on fewer nodes to reduce the number of nodes that need to be powered on.
6c994ec5-6f1c-5a62-aae8-5cb86eba170e|Dynamic Voltage and Frequency Scaling (DVFS)|This solution addresses the load balancing challenge by dynamically adjusting the voltage and frequency of processing units to match the workload.
639cfb7b-642b-51cf-8e8a-5f2768fa2929|Cost-Sensitive Adaptation Engine|This solution addresses the load balancing challenge by using a cost-sensitive adaptation engine to dynamically adjust the allocation of workload based on the cost of adaptation actions.
c85fcf44-cceb-50a4-bc2a-c7a4c7031fd1|Pipeline Operators|The authors propose the use of pipeline operators to improve the scalability of the system. This solution involves partitioning the messages processed by a vertex in one superstep to keep a limited number of messages in memory.
9f241666-daea-53cd-81da-157a24682dd5|Temporary Tables|The authors suggest the use of temporary tables to store intermediate results, which can help reduce the memory requirements and improve the scalability of the system.
9414fe94-9f3f-5817-a520-5b53e0d0adcb|Out-of-Core Capabilities|The authors propose the use of out-of-core capabilities to store data that does not fit in memory, which can help improve the scalability of the system.
b2ca42f0-ac03-56b6-91f4-0dcb759b0f03|Materialization Algebra|The authors propose the use of a materialization algebra to express queries in a way that allows for more efficient processing and scalability.
0cd5e626-1f39-53e6-bade-1a738aead29f|Query Optimization|The authors propose the use of query optimization techniques to improve the scalability of the system.
cd4ebca7-2da6-5832-93da-e2adaa614523|Distributed Environment|The authors propose the use of a distributed environment to improve the scalability of the system.
c786d3e3-7a48-50d3-93ec-8497bdccd773|Sharded Aggregators|The authors propose using sharded aggregators to address the load balancing challenge. In this approach, each aggregator is assigned to one worker, which receives partial values from all other workers and performs the aggregation. The worker in charge of an aggregator then sends the final value to the master, eliminating the need for the master to perform computations.
3c421f4c-46ed-5be0-918d-19eab28468ee|Late Projection|The authors propose using late projection to minimize communication overhead. This involves delaying the projection of data until later in the computation, reducing the amount of data that needs to be sent between nodes.
bd72dd1a-c104-53a3-acd3-bf16876bbdf4|Distributed Sketching Model|The authors propose a distributed sketching model to address the scalability challenges in graph processing. This model allows each vertex to send a message to a referee, who then computes the final output based on the received messages. The model uses a combination of public and private randomness to enable efficient computation of graph problems. The public randomness allows vertices to coordinate their messages, while the private randomness enables the referee to make random choices. The authors show that this model can be used to solve various graph problems, including maximal matching and maximal independent set, with a communication complexity of O(log^3 n) bits per vertex.
1ecde26b-1122-5b37-92ff-c85cfcff0386|Ruzsa-Szemerédi Graphs|The authors use Ruzsa-Szemerédi graphs to construct a hard input distribution for the maximal matching problem. This distribution is designed to be incompressible, making it difficult for algorithms to compress the input graph. The Ruzsa-Szemerédi graphs are constructed using a combination of random and deterministic steps. The random steps ensure that the graph is incompressible, while the deterministic steps ensure that the graph has a large number of induced matchings. The authors show that any algorithm for maximal matching that uses less than n^(1/2) bits of communication must err with a small constant probability.
54241dde-3b6a-5edc-97aa-dc95d0e908b9|Information-Theoretic Analysis|The authors use information-theoretic tools to analyze the communication complexity of graph algorithms. They show that the information revealed by the messages of vertices can be bounded using entropy and mutual information. The authors use the chain rule for entropy and mutual information to bound the information revealed by the messages of vertices. They also use the fact that conditioning can only decrease entropy to bound the information revealed by the messages of unique players. The authors show that the information revealed by the messages of vertices can be bounded by O(log^3 n) bits per vertex, which implies that any algorithm for maximal matching or maximal independent set must use at least this amount of communication.
6c980e0a-7abe-5276-920e-a415afb6452d|Information Theoretic Analysis|The authors use information theoretic tools to analyze the communication complexity of the distributed sketching model. They provide a lower bound on the communication complexity of the model for maximal matching and maximal independent set problems.
5b8e2db0-7777-5716-8113-4dbb63f43f57|Reduction from Maximal Matching to Maximal Independent Set|The authors propose a reduction from the maximal matching problem to the maximal independent set problem. This reduction allows them to analyze the communication complexity of the maximal independent set problem using the results from the maximal matching problem.
df50b367-ae6c-5738-a13d-1e58cbd5a1b9|Relaxed Greed and Memory-Based Algorithm (RGMA)|The RGMA is a distributed learning algorithm that addresses the scalability challenges of the Minimal Weighted Vertex Cover (MWVC) problem in large-scale graphs. It is designed to efficiently handle graphs with billions of edges and vertices by utilizing a relaxed greedy rule and finite memory to update the actions of vertices in a synchronous manner.
ad612ab2-0d99-5f96-a4b7-38c11a2eafad|Potential Game Theoretic Learning|The authors propose a potential game theoretic learning approach to address the scalability challenges of the MWVC problem. This approach formulates the MWVC problem as a potential game, where each vertex is a player that interacts with its neighbors to maximize its utility.
05a7e143-42d6-5618-9a4e-80fd92b4c7c5|Distributed Two-Level Path (DTLP) Index|The DTLP index is a novel index structure designed to support distributed KSP query processing over dynamic graphs. It consists of a two-level structure, where the first level indexes each subgraph by maintaining a list of bounding paths between any pair of boundary vertices, and the second level indexes the shortest path between any pair of boundary vertices within each subgraph.
355184f7-da94-5256-b08b-d0845d54f891|KSP DG Algorithm|The KSP DG algorithm is a distributed algorithm designed to process KSP queries based on the DTLP index. It decomposes the problem of identifying KSPs in the entire graph into searching for partial KSPs in different subgraphs in parallel, which can be implemented in a distributed fashion on a cluster of servers.
648345d7-7ba0-5578-a668-71065948884d|K-Shortest Paths (KSP) Decomposition|The KSP decomposition technique is designed to minimize communication overhead by decomposing the problem of identifying KSPs in the entire graph into searching for partial KSPs in different subgraphs in parallel.
3d6768cc-1748-52d2-bcaf-6c5e0bdf1e7e|Parallel Computation of Partial KSPs|The parallel computation of partial KSPs is a technique designed to minimize communication overhead by computing partial KSPs in different subgraphs in parallel.
fe158fd6-30c8-5487-8ed7-d8387f3c80dd|Distributed Vertex Hashing TRI EST BASE (DVHT b)|DVHT b is a distributed improvement of the streaming algorithm TRI EST BASE, which uses the Master Worker Aggregator architecture to assign edges from the graph stream to different workers by comparing the mapping values of the two vertices of each edge. DVHT b uses a hash function to map edges to workers, ensuring that every edge participates in the counter update algorithm of each worker. This approach reduces the number of triangle pairs sharing one edge in a worker, effectively decreasing the variance of estimates. The paper presents experimental results showing that DVHT b reduces the estimation error and is several times more accurate than state-of-the-art streaming algorithms.
7591cec7-22d2-52f4-b92c-6a8836eb9e5f|Distributed Edge Hashing TRI EST IMPR (DEHT i)|DEHT i is a distributed improvement of the streaming algorithm TRI EST IMPR, which uses the improved Master Worker Aggregator architecture with grouped and hierarchical aggregators to assign edges to workers with a fixed probability. DEHT i uses a hash function to map edges to workers directly with a fixed probability, and workers execute TRI EST IMPR algorithm, which is more accurate than TRI EST BASE. This approach allows for faster and more accurate estimation, which is extendible. The paper presents experimental results showing that DEHT i reduces the estimation error and is several times more accurate than state-of-the-art streaming algorithms.
3322a035-aec0-5739-8b82-753262369af4|Distributed Vertex Hashing (DVHT) Algorithm|The DVHT algorithm addresses the load balancing challenge by distributing the computational workload of triangle counting across multiple workers using a hash function. The algorithm assigns edges from the graph stream to different workers based on the hash values of the vertices, ensuring that each worker processes a balanced workload. The DVHT algorithm uses a hash function to map vertices to workers, allowing for efficient and balanced distribution of the workload. This approach differs from existing methods that rely on random sampling or partitioning, which can lead to uneven workloads. The paper presents experimental results showing that the DVHT algorithm achieves better load balancing and scalability compared to existing methods, with a significant reduction in communication overhead.
9403df8d-ad72-5933-b219-9a547da29b0c|Distributed Edge Hashing (DEHT) Algorithm|The DEHT algorithm addresses the load balancing challenge by distributing the computational workload of triangle counting across multiple workers using a hash function. The algorithm assigns edges from the graph stream to different workers based on the hash values of the edges, ensuring that each worker processes a balanced workload. The DEHT algorithm uses a hash function to map edges to workers, allowing for efficient and balanced distribution of the workload. This approach differs from existing methods that rely on random sampling or partitioning, which can lead to uneven workloads. The paper presents experimental results showing that the DEHT algorithm achieves better load balancing and scalability compared to existing methods, with a significant reduction in communication overhead.
2ea308a8-4102-5c16-ac58-6a27148131d5|Distributed Edge Hashing with TRI EST IMPR (DEHT) Algorithm|The DEHT algorithm is designed to minimize communication overhead by using a hash function to map edges to workers with a fixed probability. This approach ensures that each edge is processed by the necessary workers, reducing unnecessary communication.
f68f65ab-e70d-5d2c-bd37-d6cc9ddcef6f|Fine-grained Framework for Distributed Clustering|The authors propose a fine-grained framework for distributed clustering that divides the local vertices in each machine into batches and processes each of them separately. This approach reduces the memory consumption and communication overhead by only fetching and storing the remote adjacency lists for each batch of vertices when needed.
f249fc7f-e01c-5d9c-a462-e704a1fda3ea|Dynamic Work-Stealing Mechanism|The authors propose a dynamic work-stealing mechanism to handle skewed workloads in distributed graph processing. When a machine finishes processing its tasks, it steals tasks from other machines to maintain a balanced workload.
1915b695-9d03-54f9-8ec5-66872df67cef|Fixed-Size Cache for Remote Adjacency Lists|The authors propose using a fixed-size cache to store the fetched adjacency lists of remote vertices. This approach reduces the memory consumption by limiting the amount of data stored in each machine.
2d6b8dc8-bcae-5c25-bccc-22fc5c5e4aee|Pruning Technique for Similarity Computation|The authors propose a pruning technique to reduce the number of similarity computations required for clustering. The technique uses similar and effective degrees to early terminate the core checking process.
05bedcd2-574f-544a-b32c-7b86e57580af|Dynamic Work Stealing Mechanism|The authors propose a dynamic work stealing mechanism to address the load balancing challenge. This mechanism allows idle workers to automatically steal unprocessed workload from busy workers, thereby accelerating the overall process.
8950e15e-f316-5c67-9480-cc22676893c4|Fine-Grained Framework for Distributed Clustering|The authors propose a fine-grained framework for distributed clustering, which divides the local vertices into batches and processes each batch separately. This approach reduces the communication overhead by minimizing the amount of data exchanged between nodes.
4b351792-328d-5068-95c1-5d538beef1f5|Fixed-Size Cache-Based Approach|The authors propose a fixed-size cache-based approach, which stores the adjacency lists of remote vertices in a fixed-size cache, reducing the communication overhead by minimizing the amount of data exchanged between nodes.
d89559ea-5010-593d-8ce2-13d8a344b5c2|Weighted Memory-based Algorithm (WMA)|The WMA is a distributed algorithm designed to address the scalability challenges in solving the Minimum Weighted Vertex Cover (MWVC) problem. It introduces a weighted memory-based heuristic into the framework of learning in games, allowing each node to update its action based on local information, including node degrees and weights.
b7757d41-451d-5f30-8e48-f07361469ee4|Potential Game Model|The potential game model is designed to address the scalability challenges in solving the MWVC problem. It views each node as a rational player that interacts with its neighbors to maximize its local utility, allowing for a more efficient and scalable solution.
841c4a5d-c126-510d-8118-4f62c37498d6|Restricted Best Response (RBR)|The RBR is a mechanism designed to address the scalability challenges in solving the MWVC problem. It allows each node to update its action based on a relaxed greedy rule, taking into account the weights and degrees of its neighbors.
d4c69e36-c88e-57fa-970a-a026741fe39b|Distributed Hybrid Algorithm for Graph Coloring Problem (DH GCP)|The authors propose a distributed hybrid algorithm for the graph coloring problem, which is a specific instance of the scalability challenges mentioned in the challenge description. The algorithm is designed to efficiently handle large-scale graphs by distributing the computation across multiple agents, each with its own local view of the problem. The algorithm uses a combination of tabu search, perturbation, and crossover operators to explore the search space. The agents communicate with each other through a shared archive of elite solutions, and the mediator agent uses reinforcement learning to make decisions about which agents to activate. This approach allows the algorithm to scale to large graphs by distributing the computation and leveraging the strengths of different search strategies. The paper reports that the algorithm is able to reach the previous best-known results for 16 out of 23 difficult DIMACS coloring benchmarks, and remains competitive compared to many coloring algorithms. The algorithm is also able to handle large graphs with up to 2000 vertices and 999,836 edges.
4c1dfbf3-1341-5eb8-a088-c8d148be3a96|Vertex-Cut Partition Hierarchy (VPH)|The authors propose a novel approach to address the scalability challenges in graph coloring by introducing a Vertex-Cut Partition Hierarchy (VPH). This solution specifically addresses the challenge by representing the common subgraphs of the graphs as the common connected components (CCs) of the vertex-cut partitions (VPs) of the graphs. This allows for the common subgraphs to be colored only once, reducing the overall computation time.
68798a6f-93ff-5b46-9b12-2d623ae017e8|Cost Model for Coloring Time|The authors propose a cost model to estimate the optimal values of the parameters that minimize the running time on a given set of graphs. This solution specifically addresses the challenge by providing a mathematical model that captures the relationship between the parameters and the coloring time.
93ac669c-d900-556e-9fd2-c983f8ca21f9|Sampling-Based Search Method|The authors propose a sampling-based search method to estimate the values of the parameters that minimize the running time on a given set of graphs. This solution specifically addresses the challenge by providing an efficient method to search for the optimal values of the parameters.
abf99f4a-4625-5748-b50e-07c1b6e8431a|Distributed Strong Simulation Algorithm|The authors propose a distributed strong simulation algorithm that can efficiently handle large-scale graphs by redistributing the data graph to make it locally determinable and then computing the p-match graph in parallel. The algorithm uses a novel notion of p-match graph, which over-approximates strong simulation, and a partition-based approach to compute the p-match graph in parallel. The algorithm also uses a tree-based pattern splitting and data redistribution mechanism to minimize data shipment and optimize query time. The authors demonstrate the effectiveness of their algorithm through experiments on real-life and synthetic data, showing that it can significantly reduce the data sets for strong simulation and outperform existing algorithms in terms of query time and data shipment.
134fbb1c-5fca-5711-8bc5-b66218373240|Parallel Algorithm for Computing p-match Graph|The authors propose a parallel algorithm for computing the p-match graph, which is a key component of the distributed strong simulation algorithm. The algorithm uses a partition-based approach to compute the p-match graph in parallel, where each machine computes the local p-match graph and then merges the results to obtain the final p-match graph. The authors demonstrate the effectiveness of their algorithm through experiments, showing that it can significantly reduce the query time and data shipment compared to existing algorithms.
6049fc1c-3786-5afc-bd71-ee7c51be59e0|Data Redistribution Mechanism|The authors propose a data redistribution mechanism that redistributes the data graph to make it locally determinable, which is a key step in the distributed strong simulation algorithm. The mechanism uses a tree-based pattern splitting and data redistribution approach to minimize data shipment and optimize query time. The authors demonstrate the effectiveness of their mechanism through experiments, showing that it can significantly reduce the data sets for strong simulation and outperform existing algorithms in terms of query time and data shipment.
0d5b95d9-4e03-57aa-986e-c3ea0605c232|P-Match Graph Computation|The authors propose computing the p-match graph as an intermediate result to reduce the computational workload and improve load balancing. The p-match graph is a smaller graph that over-approximates the strong simulation result, allowing for more efficient computation.
06eea6b7-4333-5f97-98ac-2b501d9ed7c1|Maximal Diameter-Based Redistribution|The authors propose using a maximal diameter-based approach to redistribute the data graph and ensure load balancing. This approach ensures that each node in the distributed system has a balanced workload, reducing the likelihood of bottlenecks.
2d057eee-838f-5d33-be14-dd3b5544cd94|P-Match Graph Redistribution|The authors propose a novel approach to minimize communication overhead by redistributing the data graph to make it locally determinable. This involves partitioning the pattern graph and redistributing the data graph based on the partition, allowing for more efficient computation of the p-match graph.
1e9a6c58-bb7d-5fb8-8800-a7bf574ce6b4|Tree-Based Pattern Splitting|The authors propose a tree-based pattern splitting approach to minimize communication overhead. This involves splitting the pattern graph into smaller subgraphs and computing the p-match graph for each subgraph separately.
6f60ebe6-913a-541e-ac8c-baa379353c17|Distributed P-Match Graph Computation|The authors propose a distributed approach to compute the p-match graph in parallel. This involves computing the local p-match graph for each partition of the pattern in parallel, and then merging the results to obtain the final p-match graph.
5ba4d3a6-193b-5c9f-b0e3-a9e5d6913f1a|Distributed Vertex-Centric Load Balancing (DVCLB)|DVCLB is a load balancing strategy that dynamically adjusts the workload distribution among processing units based on the vertex degrees and edge distributions in the graph. It uses a distributed vertex-centric approach, where each vertex is responsible for managing its own workload and communicating with neighboring vertices to achieve a balanced distribution.
46341b49-76ec-5d75-9404-87c938177bb0|Graph-Aware Load Balancing (GALB)|GALB is a load balancing approach that leverages graph structural properties to optimize workload distribution. It uses a graph-aware partitioning strategy to divide the graph into subgraphs, each with a balanced workload, and assigns these subgraphs to processing units.
01fa2ca8-2597-5700-aacf-65b13be55c30|Hierarchical Graph Partitioning|The authors propose a hierarchical graph partitioning approach to minimize communication overhead. This approach involves recursively partitioning the graph into smaller subgraphs, which are then processed in parallel. By doing so, the amount of data that needs to be exchanged between nodes is significantly reduced, leading to lower communication overhead.
4b7c1e83-f8e0-5d48-9687-51bac1b0d792|Asynchronous Data Transfer|The authors propose an asynchronous data transfer mechanism to minimize communication overhead. This mechanism allows nodes to transfer data in the background while continuing to process other tasks, reducing the overall communication overhead.
9491ef60-2805-5cf7-aa66-593837aa9167|Adaptive Task Scheduling|The authors propose an adaptive task scheduling approach to minimize communication overhead. This approach involves dynamically scheduling tasks based on the available computational resources and communication bandwidth, reducing the overall communication overhead.
ce50f060-b6c8-5cba-b309-779691e0d2bb|Single Sink DAG (SSD) Plan|The SSD plan is a solution proposed by the authors to address the scalability challenges in graph processing. It involves converting a pattern query into a Single Sink DAG (SSD) and designing an evaluation plan with message transitions on the SSD to detect the pattern in a large dynamic graph.
22659e4f-3157-5995-9f55-1d5e296a0e16|Neighborhood-based Transition Rule Attachment|This solution is an optimization technique proposed by the authors to reduce the evaluation cost of the SSD plan. It involves attaching transition rules to data vertices based on their neighborhood structural requirements.
20101e07-b0fc-58ac-9267-4d1a4e1c848e|Incremental Evaluation|The authors propose an incremental evaluation approach to support continuous pattern detection over dynamic graphs. This approach allows the system to evaluate the query in an incremental way, rather than re-evaluating the query from scratch when the graph changes.
7b8c5237-faed-5a11-acf9-802dd17a1b7f|Neighborhood-based Optimized Rule Attachment|This solution addresses the communication overhead minimization challenge by attaching message transition rules to data vertices based on their neighborhood structural requirements. This approach reduces the number of unnecessary messages and minimizes the memory allocation and message transfer cost in the query evaluation.
11163110-7443-52da-b062-cb3caa5357d4|Triangle Complete Subgraph (TC Subgraph) Construction|The authors propose constructing a Triangle Complete Subgraph (TC Subgraph) for each computing node to address the scalability challenges in processing large-scale graphs. This solution involves creating a subgraph that contains all the internal edges and cross edges of the original graph, allowing for efficient local computation and reducing the need for communication between nodes.
719e74a0-b4e0-5f34-96cb-06888c718a7b|Subgraph-Oriented Model|The authors propose a subgraph-oriented model to address the scalability challenges in processing large-scale graphs. This solution involves treating the local subgraph partition as the minimal operating unit and allowing users to access and update the local graph directly.
80004dda-a1db-5f73-98be-d489120e5555|Edge Support Law|The authors propose the Edge Support Law to address the scalability challenges in processing large-scale graphs. This solution involves identifying the power-law distribution between the frequency and edges' initial supports, which ensures that the parallel algorithm achieves a low space cost for graphs in the real world.
007d35e1-7a0a-5378-bc46-1a2c5d3036f0|Core Edge Balanced Partition Scheme|The authors propose a core edge balanced partition scheme to address the scalability challenges in processing large-scale graphs. This solution involves partitioning the graph in a way that balances the core edges and minimizes the edge cut ratio.
584105cc-d6b7-5518-af64-28c686b76b72|Edge-Balanced Partition Scheme|The authors propose an edge-balanced partition scheme to address the load balancing challenge. This scheme involves partitioning the graph into subgraphs with a small edge cut ratio, which reduces the communication cost and improves the overall performance.
022851f5-bc97-57d0-9285-ea2de96997a4|Seamless Detection|The authors propose a seamless detection approach that avoids the repeated triangle counting and eliminates the need for synchronization between successive iterations. This approach enables the algorithm to detect local k-trusses in parallel within a few iterations.
d7711247-f3d7-5a03-8576-121a83af9fda|Distributed Memory Triangle Counting by Exploiting the Graph Structure|The authors propose a distributed memory triangle counting algorithm, TriC, that exploits the graph structure to minimize the volume of communication. This approach differs from previous methods by using a vertex-based graph distribution and attempting to distribute the edges equally, reducing the standard deviation of edge distribution across processes and improving load balance. Mechanisms/Techniques: TriC uses a standard vertex-based graph partitioning and tries to distribute the edges equally, reducing the standard deviation of edge distribution across processes. This approach also involves batching communications if the data count is beyond the integer range allowed by MPI, reducing the communication pressure. Results: The authors demonstrate a speedup of up to 90 relative to previous work on 32 processor cores of a NERSC Cori node and exhibit good strong scaling characteristics for large graphs.
afc50ee8-3b31-5b0d-bab0-f9f316fdc3f8|Edge Distribution Balancing|The authors propose a strategy to balance the number of edges across processes, which helps to reduce the load imbalance caused by irregular graph structures and varying vertex degrees.
aeaa01d8-4fcf-57de-b84a-c4f86f4fbf80|Batching and Asynchronous Communication|The authors propose to explore batching and asynchronous communication to alleviate the load imbalance issue at scale.
eb174a82-9bd2-510c-bc65-9ebc94b090d8|Overlapping Local Computation with Communication|The authors propose to overlap local computation with communication in batches to reduce the severity of synchronization burden.
017e7810-5fe8-5977-89c7-78b6aa94d9b7|Batching of Communication Rounds|The authors propose batching of communication rounds to reduce the communication pressure by aggregating the outgoing data and implementing a policy that trades off computation with communication. The solution involves storing the vertices for e.g., 7, 9, 11, 14, 17, and demarcating vertex boundaries such that the remote side can translate the sequence of vertex IDs in its incoming message buffer into the correct combination of vertices as edges. Since the sequence of vertices in the outgoing communication buffer can supersede the integer range, the authors internally batch communications if the data count is beyond the integer range allowed by MPI.
6650be54-2f48-5903-8e05-2b6f05375de9|Adaptive Routing Options for MPI Alltoallv|The authors experiment with the available adaptive routing options for MPI Alltoallv using the MPICH GNI A2A ROUTING MODE environment variable to minimize the communication overhead. The solution involves using different routing algorithms, such as the default adaptive routing scheme (A1), A2, A3, and turning off the Cray Aries R specific MPI Alltoallv optimizations (OFF). The results show that the default adaptive routing scheme (A1) usually works well, but it is possible to further improve the performance by selecting another scheme. For example, the authors observed up to 13% improvement in execution time for the com orkut graph.
82f946db-ed40-5da0-8d11-89e3ffbc4b77|Subgraph-Centric Data Model|The authors propose a subgraph-centric data model to address the scalability challenges in graph processing. This approach involves partitioning the data graph into disjoint subgraphs, where each subgraph is mapped to a single vertex in the GPS system. The messages from a subgraph are merged and encapsulated before transmission to reduce network traffic.
bb0038eb-391c-58bc-b2cd-43a9d900d648|Incremental Pattern Matching Algorithm|The authors propose an incremental pattern matching algorithm to address the scalability challenges in graph processing. This algorithm only triggers computations on the vertices whose matching status is changed by the graph update events, reducing the amount of computation required.
84bed4c7-ca51-545d-8b9d-a0c4e9dea544|Dynamic Subgraph Sizing|The authors propose dynamically adjusting the subgraph size to optimize performance. This approach involves finding the optimal subgraph size that balances the trade-off between network traffic and computation time.
d6ae01d7-9ae4-5ad5-a28b-e8cca03d3c3d|Subgraph Centric Data Model|The authors propose a subgraph centric data model to address the load balancing challenge. This approach involves partitioning the data graph into disjoint subgraphs, where each subgraph is mapped to a single vertex in the GPS system. By doing so, the authors aim to reduce the communication overhead and balance the workload among processing units.
a0de4046-bf4b-509a-82d4-30d550d728a6|Incremental Algorithm with Message Packaging|The authors propose an incremental algorithm that packages messages from a subgraph before transmission, reducing communication overhead. The algorithm triggers computations only on vertices whose matching status is changed by graph update events, minimizing the number of messages exchanged between nodes.
e993242d-9111-5c4b-ad13-d6dfdc0687d9|CentLocal Algorithm for Maximum Cardinality Matching|The authors propose a CentLocal algorithm for maximum cardinality matching, which is a centralized local computation algorithm that can be simulated by a distributed local algorithm. This solution addresses the scalability challenge by providing a framework for efficient computation of maximum cardinality matching in large-scale graphs.
470969b8-57cb-5dd2-bb41-11e94edc6717|CentLocal Algorithm for Maximum Weighted Matching|The authors propose a CentLocal algorithm for maximum weighted matching, which is a centralized local computation algorithm that can be simulated by a distributed local algorithm. This solution addresses the scalability challenge by providing a framework for efficient computation of maximum weighted matching in large-scale graphs.
3097237a-4270-5abd-a0d6-b35e00deebca|Distributed Algorithm for Maximum Cardinality Matching|The authors propose a distributed algorithm for maximum cardinality matching, which is based on the CentLocal algorithm. This solution addresses the scalability challenge by providing a framework for efficient computation of maximum cardinality matching in large-scale graphs.
0b7964a2-3240-5a73-a076-0942ee6e22ba|Distributed Algorithm for Maximum Weighted Matching|The authors propose a distributed algorithm for maximum weighted matching, which is based on the CentLocal algorithm. This solution addresses the scalability challenge by providing a framework for efficient computation of maximum weighted matching in large-scale graphs.
3655f94f-cc73-5707-b8f1-242609c32925|CentLocal Algorithm for Distributed Maximum Matching|The authors propose a CentLocal algorithm for distributed maximum matching, which is designed to minimize communication overhead by performing local computations on the graph. The algorithm works by simulating a global algorithm for maximum matching using local probes, which reduces the need for communication between nodes.
f400e4d7-9a03-599f-a9fb-f327f888930c|Distributed Vertex Coloring Algorithm|The authors propose a distributed vertex coloring algorithm that is used as a building block for the CentLocal algorithm. The algorithm is designed to minimize communication overhead by using a local coloring technique.
b1b25f1f-a45f-56d7-a3bc-8c2ce45c1bb6|Probe Radius Bounding Technique|The authors propose a technique called 
867fa4be-af8d-5ae7-98a8-d1dfe810c8db|Local Improvement Technique|The authors propose a local improvement technique that is used to improve the performance of the CentLocal algorithm. The technique is designed to minimize communication overhead by performing local computations on the graph.
8041cde3-47b7-5728-8bb0-b18faa39401c|Matrix Algebraic Primitives for Scalable Matching|The authors propose using matrix algebraic primitives to develop distributed memory parallel algorithms for computing maximal cardinality matching in bipartite graphs. This approach allows for a higher degree of parallelism and scalability on distributed memory platforms. The authors employ sparse matrix vector multiplication (SpMV) and vector manipulations to update the current matching. They also use a 2D processor grid to distribute the sparse matrix and vectors, ensuring load balance across processors. The authors achieve an average of 202 speedup up to 300 on 1024 cores of a Cray XC30 supercomputer, demonstrating the effectiveness of their approach in addressing scalability challenges.
85a9a460-a57f-58db-bd53-846239237d26|Distributed Data Structure for Bipartite Graphs|The authors propose using a distributed data structure that can be used to traverse the bipartite graph from both sides without storing the transpose of the matrix. This approach reduces storage requirements and improves scalability. The authors use a 2D processor grid to distribute the sparse matrix and vectors, ensuring load balance across processors. They also employ a compressed sparse blocks (CSB) data structure to store the matrix, allowing for efficient SpMV on both the matrix and its transpose. The authors demonstrate that their approach can reduce storage requirements by up to 50%, making it more suitable for large-scale graphs.
ef4ec283-393f-55f7-a0f6-0f14ff69c536|Semiring-based Graph Traversal|The authors propose using a semiring-based approach to traverse the bipartite graph from both sides. This approach allows for efficient graph traversal and matching. The authors use a select2nd, min semiring to traverse the graph from both sides, ensuring that each vertex is visited only once. They also employ a 2D processor grid to distribute the sparse matrix and vectors, ensuring load balance across processors. The authors demonstrate that their approach can achieve high performance and scalability on distributed memory platforms, with an average of 202 speedup up to 300 on 1024 cores of a Cray XC30 supercomputer.
35d6018a-883b-540d-bfa6-f2916e3a7f5d|Random Permutation of Input Matrix|The authors propose a solution to address the load balancing challenge by randomly permuting the input matrix A before running the matching algorithms. This approach helps to balance the load across processors by distributing the nonzeros in A more evenly.
890a9a83-7497-5fd3-8b22-02b3266f2221|2D Processor Grid|The authors use a 2D processor grid to distribute the sparse matrix A and the vectors across processors. This approach helps to balance the load by dividing the workload into smaller sub-problems that can be solved in parallel.
7c091a50-8cb4-5509-a9fe-e3bce0389a28|Doubly Compressed Sparse Columns (DCSC) Format|The authors use the DCSC format to store the local sub-matrices of the sparse matrix A. This approach helps to reduce the storage requirements and improve the performance by compressing the sparse matrix.
f1371de8-c2a4-50a5-846b-6aba58586c08|Distributed Memory Parallel Algorithm using Matrix Algebra|The authors propose a distributed memory parallel algorithm for computing maximal cardinality matching in a bipartite graph using matrix algebra building blocks. This approach exposes a higher degree of parallelism on distributed memory platforms, allowing for efficient computation and minimizing communication overhead. The algorithm relies on matrix algebra operations, such as sparse matrix-vector multiplication (SpMV) and vector manipulations, to search for unmatched rows in the matrix from unmatched columns and update the current matching. This approach enables the algorithm to scale to tens of thousands of processors while maintaining a stable approximation ratio. The authors report up to 300 speedup on 1024 cores of a Cray XC30 supercomputer and good scaling on up to 16,384 processors for larger synthetically generated graphs.
72c152f4-e15c-5ac2-a216-0329667d218c|Quantum Distributed Algorithm for Triangle Finding in the CONGEST Model|The authors propose a quantum distributed algorithm for triangle finding in the CONGEST model, which addresses the scalability challenges by utilizing quantum computing principles to improve the efficiency of distributed graph processing.
6a7a6654-6969-59b6-8219-ac4601b3e2c2|Expander Decomposition|The authors use an expander decomposition algorithm to partition the graph into components with low mixing time, which helps to address the scalability challenges by reducing the number of edges that need to be processed.
7f5c5910-d9a0-5ac6-afa8-153a43b6e586|Distributed Quantum Search|The authors propose a distributed quantum search algorithm that enables the detection of triangles in a graph by searching for triangles in parallel across the network.
09ddc003-c2dc-55ba-9c9c-11f7121319db|Quantum Distributed Search Framework|The authors propose a quantum distributed search framework to address the load balancing challenge. This framework enables the detection of a triangle in a subnetwork with low mixing time, which is a crucial step in the overall triangle finding algorithm. The framework uses a quantum search algorithm to find a triple of vertices that form a triangle, which helps to balance the workload across the network.
c1b56d1b-2d10-5648-8f23-9411cdfac2a8|Classical Routing Techniques|The authors propose using classical routing techniques to address the load balancing challenge. These techniques are used to gather information about the edges in the subnetwork, which is necessary for the triangle finding algorithm. The techniques help to balance the workload across the network by distributing the communication overhead evenly.
95a607f3-9d54-5b53-9bb9-e2475758ff62|Distributed Routing Scheme|The authors propose a distributed routing scheme to minimize communication overhead in distributed graph processing systems. This scheme enables the efficient routing of messages between nodes by using a combination of classical and quantum techniques.
4e6718c5-44b3-5282-b6d0-9695c7bcdb7a|ID Assignment and Degree Estimation|The authors propose an ID assignment and degree estimation technique to minimize communication overhead in distributed graph processing systems. This technique enables the efficient estimation of the degree of each node, which reduces the communication overhead.
cef4f878-3fb5-587a-bb75-07318ba482cd|Hybrid Parallelization using MPI and Cilk|The authors propose a hybrid parallelization approach that combines MPI for distributed memory parallelism and Cilk for shared memory parallelism to address scalability challenges in triangle counting.
6e89eb1e-b6a1-5f9f-99ea-37bb12651315|2D Cartesian Partitioning Scheme|The authors use a 2D Cartesian partitioning scheme to distribute the graph among MPI processes, which reduces communication overheads and improves scalability.
85c90b0a-8c2c-5c26-9d1a-fd1c3bd99634|Cilk-based Shared Memory Parallelism|The authors use Cilk for shared memory parallelism within each MPI process, which provides efficient work stealing and multithreading.
5eb9c450-e52f-5cf7-80df-0e818b35b0d1|Over-Partitioning Rate|The authors propose using an over-partitioning rate to divide the rows of Lq,r into smaller blocks for shared memory parallelism using Cilk. This approach ensures that the number of nonzeros in each block is balanced, reducing the load imbalance between threads.
ea6c1d4e-e327-51ff-95ce-136897f4ecf2|Compressed Row Storage (CRS) Format|The authors propose using the Compressed Row Storage (CRS) format to minimize communication overhead in distributed graph processing systems. The CRS format is used to store the graph data in a compact form, which reduces the amount of data that needs to be communicated between processors.
45f3cd8d-d270-541d-a759-2b5aed2d300d|Over-partitioning|The authors propose using over-partitioning to minimize communication overhead in distributed graph processing systems. Over-partitioning involves dividing the graph into smaller subgraphs than necessary, which allows for more efficient communication between processors.
af396597-9810-5f2c-a39f-71df31d4be80|Mixed Structure-Based Approach|The authors propose a mixed structure-based approach to address the scalability challenges in truss maintenance. This approach involves representing the graph as a mixed structure, which is composed of inserted/deleted edges and vertices. The mixed structure is used to simplify the insertion/deletion of edges and vertices, allowing for efficient truss maintenance.
af131499-de3b-5e54-ae8c-783ccb098a78|Parallel Implementations|The authors propose parallel implementations of their mixed structure-based approach to further improve scalability. The parallel implementations involve assigning tasks to different processes for different k values, allowing for efficient truss maintenance on multicore processors.
c9c29960-e856-5dc8-8a25-4a8322856de9|Triangle Support-Based Approach|The authors propose a triangle support-based approach to address the scalability challenges in truss maintenance. This approach involves using a novel data structure called the triangle support to determine whether the trussness of an edge will change after graph changes.
7cec2185-f754-5f7a-9ce1-8dc46713c8e4|Pre-Truss-Based Approach|The authors propose a pre-truss-based approach to address the scalability challenges in truss maintenance. This approach involves using a novel data structure called the pre-truss to get an initial trussness of the inserted edges, allowing for efficient truss maintenance.
8f656f77-641b-51f3-9e73-23cee90f94c9|Parallel Traversal Algorithm|The authors propose a parallel traversal algorithm to address the load balancing challenge in truss maintenance. This algorithm allows for the simultaneous processing of multiple edges and vertices, reducing the computational workload and improving overall system performance.
0851cb79-bf06-58ba-98ad-8a3109c4076a|Parallel Truss Maintenance Algorithm|The authors propose a parallel truss maintenance algorithm to minimize communication overhead in distributed graph processing systems. This algorithm allows for the parallel processing of edges with the same trussness, reducing the need for communication between nodes.
64a2bea0-462f-51b8-bf35-d8b3d825c865|Thread Pool Technique|The authors employ a thread pool technique to reuse threads and minimize the overhead of thread creation and destruction. This technique allows the algorithm to efficiently handle a large number of tasks in parallel.
ec7aab93-955c-5f7d-9f01-aa81225c201d|Mixed Structure Construction|The authors propose a mixed structure construction algorithm to minimize communication overhead in distributed graph processing systems. This algorithm constructs a mixed structure that contains a set of edges and vertices that can be processed in parallel.
57896132-383f-549a-a545-11da680a7bdb|2D Distributed Graph Partitioning|The authors propose a 2D distributed graph partitioning approach to address scalability challenges in processing large-scale graphs. This approach involves partitioning the graph into smaller sub-matrices and distributing them across multiple processors, allowing for parallel processing and reducing memory requirements.
9720583b-5daf-5d18-b464-1a8647704055|Hybrid Parallel BFS Algorithm|The authors propose a hybrid parallel BFS algorithm that combines distributed memory graph partitioning with shared memory traversal parallelism. This approach addresses scalability challenges by allowing for parallel processing of the graph while minimizing memory requirements.
c0d5a425-7168-5088-b2de-29ace2e2bbd9|2D Vector Distribution|The authors propose a 2D vector distribution approach to address the load balancing challenge. This involves distributing the vectors over all processors, rather than just the diagonal processor, to alleviate severe load imbalance.
c1b66cc7-adc8-53b0-97ec-5e25db18cc4f|Random Shuffling of Vertex Identifiers|The authors propose random shuffling of vertex identifiers prior to partitioning the graph to address the load balancing challenge. This helps to distribute the vertices and edges evenly across processors.
b2e6b4ca-0c60-5f85-96bd-4340aec30270|Hybrid Parallel BFS with Vertex Partitioning|The authors propose a hybrid parallel BFS approach with vertex partitioning to address the load balancing challenge. This involves partitioning the graph among processors and using a hybrid parallel approach to balance the workload.
7ce1a2d3-63cd-531e-8527-e5526e294aa7|2D Partitioning-based Approach|The authors propose a 2D partitioning-based approach to minimize communication overhead in distributed graph processing systems. This approach involves partitioning the graph into smaller subgraphs and distributing them across multiple processors, reducing the need for communication between nodes.
8026aebb-81f0-5028-a452-cf6ab5749a65|Intra-Node Multithreading|The authors propose using intra-node multithreading to reduce communication overhead in distributed graph processing systems. This involves using multiple threads within a node to perform computations in parallel, reducing the need for communication between nodes.
d719ec94-a697-5bbd-b69f-06a2ef60f8ae|Parallel Well Separated Pair Decomposition (PWSPD)|The authors propose a parallel well separated pair decomposition (PWSPD) algorithm to address the scalability challenges in computing the Euclidean minimum spanning tree (EMST) and hierarchical spatial clustering (HDBSCAN). The PWSPD algorithm is designed to efficiently handle large-scale graphs with billions of edges and vertices.
41092733-5fac-5fa8-ba84-83fb52ec15a5|MemoGFK Optimization|The authors propose a memory optimization technique, called MemoGFK, to reduce memory usage and improve performance in computing the EMST and HDBSCAN. The MemoGFK optimization is designed to minimize memory requirements and optimize data structures.
9ff4a4af-d06b-5656-99b2-0d1c32240ec3|Parallel Dendrogram Construction|The authors propose a parallel dendrogram construction algorithm to address the scalability challenges in computing the EMST and HDBSCAN. The parallel dendrogram construction algorithm is designed to efficiently handle large-scale graphs with billions of edges and vertices.
ed841343-d7b2-5add-aef3-341754de52a7|Parallel WSPD Construction|The parallel WSPD construction algorithm is designed to reduce the communication overhead between nodes by parallelizing the construction of the well-separated pair decomposition (WSPD). This approach enables the algorithm to scale more efficiently and reduce the overall computation time.
0e0eb226-9041-502c-8f87-e65f759896c3|CSMR: A Scalable Algorithm for Text Clustering with Cosine Similarity and MapReduce|The authors propose a method for pairwise text similarity on massive data sets, using the Cosine Similarity metric and the tf-idf Term Frequency-Inverse Document Frequency normalization method. The research approach is mainly focused on the MapReduce paradigm, a model for processing large data sets in parallel manner, with a distributed algorithm on computer clusters.
16f96279-c7de-56ea-98fe-cb7ecd571919|3-Level Degree-Aware 1.5D Graph Partitioning|This solution addresses the scalability challenges by proposing a novel graph partitioning method that divides vertices into three levels of degree: Extremely Heavy (E), Heavy (H), and Light (L). E vertices are delegated globally, H vertices are delegated on columns and rows of the communication mesh, and L vertices are treated normally. This approach reduces the number of shared vertices and edges, minimizing communication overhead and improving load balance.
5d8637b6-029d-5415-a91d-a2b45e4c4226|Sub-Iteration Direction Optimization|This solution addresses the scalability challenges by proposing a direction optimization technique that applies different directions to different degree-aware subgraphs. It allows for efficient visiting of E and H vertices in early iterations while preventing unnecessary visits from L vertices in late iterations.
5243aa34-75e7-542f-bb3c-ab2457692ed1|Core Group-Aware Core Subgraph Segmenting|This solution addresses the scalability challenges by proposing a core subgraph segmenting technique that divides the core subgraph into smaller segments and processes them in parallel. This approach improves the load balance and reduces the communication overhead.
dd416a32-85b0-5f14-a88c-db04dd506b58|On-Chip Sorting with RMA|This solution addresses the scalability challenges by proposing an on-chip sorting technique that uses Remote Memory Access (RMA) to sort messages. This approach improves the memory bandwidth utilization and reduces the communication overhead.
b011c6a6-ec9e-53e3-b147-2df53d7e7637|Edge Aware Vertex Cut Load Balancing|This solution addresses the load balancing challenge by adopting an edge aware vertex cut method, which calculates the prefix sum of locally available frontier vertices’ degree at each EH2EH top-down traversal. This approach ensures that the workload is distributed evenly among CPEs, preventing severe load imbalance.
ad554787-1504-563c-9c96-7b805be6b944|3-Level Degree Aware 1.5D Graph Partitioning|This solution addresses the load balancing challenge by proposing a novel graph partitioning method that divides vertices into three levels of degree: Extremely Heavy (E), Heavy (H), and Light (L). E vertices are delegated on all nodes, H vertices are delegated on columns and rows of the communication mesh, and L vertices are treated normally.
b5b96bb4-9ccc-52b6-a76a-d22800ce893c|Core Group Aware Core Subgraph Segmenting|This solution addresses the load balancing challenge by segmenting the core subgraph into smaller segments and processing them in parallel. This approach ensures that the workload is distributed evenly among CPEs.
790eda99-5f56-5ba4-85da-c23334d62f9d|Delayed Reduction of Parent Array|This solution addresses the communication overhead minimization challenge by delaying the reduction of the parent array until the BFS run finishes. This approach reduces collective communication volume during the BFS run.
0a8cb7db-c7cb-58bd-8b6d-be8d9caa6eaf|Edge-Aware Vertex Cut Load Balancing in EH2EH Push|This solution addresses the communication overhead minimization challenge by proposing an edge-aware vertex cut load balancing method in the EH2EH push phase. This approach reduces communication by minimizing the number of edges that need to be accessed.
5aa82bee-7813-50cf-9bbd-d5d4c5f8147b|On-Chip Sorting with RMA (OCS RMA)|This solution addresses the communication overhead minimization challenge by proposing an on-chip sorting method that uses Remote Memory Access (RMA) to sort messages. This approach reduces communication by minimizing the number of messages that need to be sent.
4df6b6ac-5f2e-5ba1-a80e-3c3a3b78c1a3|Asynchronous Updates|The authors propose an asynchronous update mechanism to address scalability challenges in their parallel algorithm for updating Single Source Shortest Path (SSSP) in large-scale dynamic networks. This solution allows for the reduction of synchronization overhead by enabling processors to update the affected vertices without waiting for other processors to finish their updates.
8150d76b-412b-54bf-aa5b-7c76b0c5f2d6|Batch Processing of Changed Edges|The authors propose processing changed edges in batches to improve the performance of their parallel algorithm for updating SSSP in large-scale dynamic networks. This solution helps to reduce memory hotspots and improve load balancing.
86969c2a-b42d-5730-a9a8-bb65af241dd4|Vertex Marking Functional Block (VMFB)|The authors propose a novel functional block-based approach, called Vertex Marking Functional Block (VMFB), to improve the performance of their parallel algorithm for updating SSSP in large-scale dynamic networks on GPUs. This solution helps to reduce redundant computations and improve scalability.
fd7a4391-40d8-506b-88e3-e44e819faa17|Dynamic Scheduling|The authors propose using dynamic scheduling to address the load balancing challenge. This involves using a dynamic scheduler to balance the workload among processing units, ensuring that each unit processes a similar number of vertices.
4447c866-a364-5c2a-ae00-c6b5a8ccad53|Processing in Batches|The authors propose processing changed edges in batches to improve load balancing. This involves dividing the changed edges into smaller batches and processing each batch separately.
bf7c8760-80e7-57f4-91ab-9d8856c41629|DistG Distributed Graph Coloring Algorithm|The DistG algorithm is a novel graph coloring algorithm designed for utilizing the simple parallelization technique provided by the Giraph framework or any other vertex-centric paradigm. It is specifically designed to address the scalability challenges of large graph processing by efficiently distributing the computation over clusters of machines.
e129c201-2135-5863-aa71-862af373f4bb|Giraph Framework|The Giraph framework is an open-source implementation of the Pregel system, designed for large-scale graph processing. It is specifically designed to address the scalability challenges of large graph processing by providing a flexible and fault-tolerant framework for distributed graph processing.
efb3ff3d-90c7-5063-a038-7f1085f3db55|Combiner and Aggregator Functions|The authors propose using combiner and aggregator functions to minimize communication overhead in the Giraph framework. These functions allow users to define custom methods for combining and aggregating messages, reducing the number of messages sent between nodes.
8d308d82-8cff-5983-a671-d3112fb0bc3b|Local Smallest Largest Degree First Algorithm|The Local Smallest Largest Degree First Algorithm is a distributed graph coloring algorithm designed to address the scalability challenges in processing large-scale graphs. This algorithm works by identifying the smallest and largest degree vertices in each superstep and coloring them differently, reducing the number of supersteps and improving runtime performance. The algorithm uses a greedy approach with a focus on vertex degree, which is different from existing approaches that rely on random numbers or vertex IDs. The algorithm also maintains two sets, ColoredSet and NotColoredSet, to keep track of colored and uncolored vertices, respectively. The paper presents experimental results showing that the Local Smallest Largest Degree First Algorithm performs better than other heuristic-based algorithms in terms of runtime and number of colors used.
30effe25-75cb-5220-809e-5e6818a3bd7c|Local Largest Degree First Algorithm|The Local Largest Degree First Algorithm is another distributed graph coloring algorithm proposed to address the scalability challenges in processing large-scale graphs. This algorithm works by identifying the largest degree vertices in each superstep and coloring them, reducing the number of supersteps and improving runtime performance. The algorithm uses a greedy approach with a focus on vertex degree, which is different from existing approaches that rely on random numbers or vertex IDs. The algorithm also uses a tie-breaking mechanism based on vertex IDs to resolve conflicts. The paper presents experimental results showing that the Local Largest Degree First Algorithm performs better than other heuristic-based algorithms in terms of number of colors used, but takes more computation time.
8c458932-8ee0-5b09-b901-5f93cb98dee0|Local Minima Maxima First Algorithm|The Local Minima Maxima First Algorithm is a distributed graph coloring algorithm designed to address the scalability challenges in processing large-scale graphs. This algorithm works by identifying the smallest and largest degree vertices in each superstep and coloring them differently, reducing the number of supersteps and improving runtime performance. The algorithm uses a greedy approach with a focus on vertex degree, which is different from existing approaches that rely on random numbers or vertex IDs. The algorithm also maintains two sets, ColoredSet and NotColoredSet, to keep track of colored and uncolored vertices, respectively. The paper presents experimental results showing that the Local Minima Maxima First Algorithm performs better than the Local Maxima First Algorithm in terms of runtime, but takes more computation time than the Local Smallest Largest Degree First Algorithm.
97e0b522-6b48-5ccf-a3c6-6fa7c7745609|Local Maxima First Algorithm|The Local Maxima First Algorithm is a distributed graph coloring algorithm designed to address the scalability challenges in processing large-scale graphs. This algorithm works by identifying the largest degree vertices in each superstep and coloring them, reducing the number of supersteps and improving runtime performance. The algorithm uses a greedy approach with a focus on vertex IDs, which is different from existing approaches that rely on random numbers or vertex degrees. The algorithm also uses a tie-breaking mechanism based on vertex IDs to resolve conflicts. The paper presents experimental results showing that the Local Maxima First Algorithm performs worse than other heuristic-based algorithms in terms of number of colors used and runtime.
6d354907-d8ac-543c-ba12-723a7d77d1f3|2D Cyclic Distribution of the Graph|The authors propose a 2D cyclic distribution of the graph to address the scalability challenges. This approach involves distributing the graph across a 2D processor grid, where each processor is responsible for a block of the adjacency matrix. The 2D cyclic distribution allows for better load balancing and reduces the communication overhead.
a81fb211-26b3-561a-ace4-6b8013badf33|Doubly Compressed Sparse Row Structure|The authors propose the use of a doubly compressed sparse row structure to store the task matrix and the upper and lower triangular portions of the adjacency matrix. This approach reduces the memory requirements and improves the performance of the algorithm.
84a07c43-c798-5f81-a61c-aa27cf3c7e41|Modifying the Hashing Routine for Sparser Vertices|The authors propose modifying the hashing routine for sparser vertices to improve the performance of the algorithm. This approach involves using a different hashing routine for vertices with shorter adjacency lists.
7be4abb4-7de3-5286-9d42-913749af97aa|Reducing Overheads Associated with Communication|The authors propose reducing the overheads associated with communication by allocating the memory associated with the adjacency lists in a contiguous block. This approach reduces the overheads associated with serialization and deserialization of the adjacency lists.
38979d60-b353-5590-8547-2fbbedd1f60b|Eliminating Unnecessary Intersection Operations|The authors propose eliminating unnecessary intersection operations by traversing the adjacency lists in a specific order. This approach reduces the number of intersection operations and improves the performance of the algorithm.
d61302cf-2a0f-52ae-a16b-23f0529fb20c|2D Cyclic Distribution|The authors propose a 2D cyclic distribution of the graph to balance the computations and reduce the communication overheads. This approach involves decomposing the graph into smaller blocks and distributing them across a 2D processor grid, ensuring that each processor has a similar number of non-zero tasks and a similar number of light and heavy tasks.
bb917a11-4267-57c5-a5ae-5449591355dc|Doubly Sparse Traversal|The authors propose a doubly sparse traversal of the CSR structure to reduce the number of unnecessary intersection operations. This approach involves traversing the adjacency lists of vertices in a way that minimizes the number of comparisons required to find common vertices.
b4dd96ee-8d34-5a0e-b72f-074aec9b5ada|Modifying the Hashing Routine|The authors propose modifying the hashing routine for sparser vertices to reduce the number of unnecessary intersection operations. This approach involves using a different hashing strategy for vertices with smaller adjacency lists.
48aa89d2-d8ce-52c0-9530-0cc4b6459749|2D Cyclic Distribution of the Adjacency Matrix|The authors propose a 2D cyclic distribution of the adjacency matrix to minimize communication overhead. This approach involves distributing the matrix across a 2D grid of processors, where each processor is responsible for a block of the matrix. The 2D cyclic distribution allows for a more balanced workload and reduces the need for communication between processors.
6ed575ec-d66e-50ea-85ea-b7d3b1746134|Doubly Compressed Sparse Row (CSR) Structure|The authors propose using a doubly compressed sparse row (CSR) structure to store the adjacency matrix. This data structure allows for efficient storage and access to the matrix, reducing the need for communication between processors.
8cb71aa0-4a95-50b8-8864-a662ac182f5d|Cannon's 2D Parallel Matrix Multiplication Algorithm|The authors propose using Cannon's 2D parallel matrix multiplication algorithm to minimize communication overhead. This algorithm is designed to perform matrix multiplication in a 2D grid of processors, reducing the need for communication between processors.
eb1f3e54-607d-5202-bbd2-387b998625fe|PECO Parallel Enumeration of Cliques using Ordering|PECO is a parallel algorithm designed to efficiently enumerate maximal cliques in large graphs using the MapReduce framework. It addresses scalability challenges by utilizing a carefully chosen total ordering among all vertices in the graph, which is used to eliminate redundant work among subproblems and improve load balancing.
cb4866a9-4112-5ac6-813f-6d791902d3f1|Degree Ordering Strategy|The authors propose using a degree ordering strategy to address the load balancing challenge. This involves ordering the vertices in the graph based on their degrees, with higher-degree vertices being processed first. The idea is that higher-degree vertices are more likely to be part of larger cliques, and processing them first will help to distribute the workload more evenly across the processing units.
aa1bae20-cedd-51fd-8025-f3312beeed35|Triangle Counting Strategy|The authors also propose using a triangle counting strategy to address the load balancing challenge. This involves counting the number of triangles that each vertex is part of and using this information to order the vertices. The idea is that vertices that are part of more triangles are more likely to be part of larger cliques, and processing them first will help to distribute the workload more evenly across the processing units.
3ff18578-7681-54fc-8fec-58ab1723eabc|Vertex Ordering-based Communication Reduction|The authors propose a solution that utilizes a carefully chosen total ordering among all vertices in the graph to reduce communication overhead. This ordering is used to decide which cliques to enumerate within each task and to eliminate redundant search paths within the enumeration search tree.
290b2126-466b-51b6-8cd8-94f591f1e833|Subgraph Rank (SGRK) Algorithm|The SGRK algorithm is a novel solution proposed by the authors to address the scalability challenges in graph processing. It is designed to efficiently compute PageRank values for large-scale graphs by leveraging the subgraph centric programming abstraction. The SGRK algorithm uses a combination of local PageRank (LPR) and global PageRank (GPR) phases to compute the PageRank values. The LPR phase computes the PageRank values for each subgraph, while the GPR phase aggregates the PageRank values from each subgraph to obtain the final PageRank values. The algorithm also uses a fair initialization vector, SG by G, to ensure that the PageRank values are fairly distributed among the subgraphs. The paper presents experimental results showing that the SGRK algorithm outperforms the native PageRank and BlockRank algorithms in terms of scalability. Specifically, the SGRK algorithm achieves a speedup of 23-74 for most graphs evaluated, while achieving an equivalent PageRank quality.
374e736d-832e-5bab-beb9-16122349dd50|BlockRank with PageRank-like Distribution Logic (BRDL)|The BRDL algorithm is another solution proposed by the authors to address the scalability challenges in graph processing. It is designed to improve the performance of the BlockRank algorithm by using a PageRank-like distribution logic. The BRDL algorithm uses a PageRank-like distribution logic to distribute the PageRank values among the subgraphs. This approach is different from the native BlockRank algorithm, which uses a block-level PageRank distribution logic. The paper presents experimental results showing that the BRDL algorithm outperforms the native BlockRank algorithm in terms of scalability. Specifically, the BRDL algorithm achieves a speedup of 2-5 for most graphs evaluated, while achieving an equivalent PageRank quality.
089af030-e356-567f-ad7f-04563ceaa746|BlockRank with SG by G Initialization Vector (BRIV)|The BRIV algorithm is another solution proposed by the authors to address the scalability challenges in graph processing. It is designed to improve the performance of the BlockRank algorithm by using a fair initialization vector, SG by G. The BRIV algorithm uses a fair initialization vector, SG by G, to ensure that the PageRank values are fairly distributed among the subgraphs. This approach is different from the native BlockRank algorithm, which uses a block-level PageRank initialization logic. The paper presents experimental results showing that the BRIV algorithm outperforms the native BlockRank algorithm in terms of scalability. Specifically, the BRIV algorithm achieves a speedup of 2-5 for most graphs evaluated, while achieving an equivalent PageRank quality.
6b0a5880-fd01-5c62-9612-c1cca03afecf|BlockRank with PageRank-like Distribution Logic and SG by G Initialization Vector (BRDI)|The BRDI algorithm is another solution proposed by the authors to address the scalability challenges in graph processing. It is designed to improve the performance of the BlockRank algorithm by using a combination of PageRank-like distribution logic and a fair initialization vector, SG by G. The BRDI algorithm uses a combination of PageRank-like distribution logic and a fair initialization vector, SG by G, to ensure that the PageRank values are fairly distributed among the subgraphs. This approach is different from the native BlockRank algorithm, which uses a block-level PageRank distribution logic and initialization logic. The paper presents experimental results showing that the BRDI algorithm outperforms the native BlockRank algorithm in terms of scalability. Specifically, the BRDI algorithm achieves a speedup of 2-5 for most graphs evaluated, while achieving an equivalent PageRank quality.
057743f5-2fcc-55b4-b5b9-4265ac80f922|Subgraph-Centric Load Balancing|The authors propose a subgraph-centric approach to load balancing, where the graph is partitioned into subgraphs, and each subgraph is processed independently. This approach allows for more efficient use of resources and reduces communication overhead.
37cc40f0-9030-5b3d-8ee4-5fd7318c9d55|BlockRank with SG by G Initialization Vector|The authors propose a variation of the BlockRank algorithm that uses a novel initialization vector, SG by G, to improve load balancing. This approach allows for more efficient computation of PageRank values and reduces the risk of bottlenecks.
c42009c6-2373-522b-93f2-58c5972360d1|Subgraph Rank with PageRank-like Distribution Logic|The authors propose a variation of the Subgraph Rank algorithm that uses a PageRank-like distribution logic to improve load balancing. This approach allows for more efficient computation of PageRank values and reduces the risk of bottlenecks.
2a04ad71-759e-53ac-8a91-5db5f506e37d|Subgraph-Centric Programming Abstraction|The authors propose a subgraph-centric programming abstraction as a solution to minimize communication overhead in distributed graph processing systems. This approach involves partitioning the graph into subgraphs and processing each subgraph independently, reducing the need for communication between nodes.
0c1e0d6e-01dc-5160-86cd-8c4728c95e5a|BlockRank Algorithm with Subgraph-Centric Initialization|The authors propose a variation of the BlockRank algorithm that uses a subgraph-centric initialization vector to reduce communication overhead. This approach involves initializing the PageRank values of each subgraph using a localized computation, reducing the need for global synchronization and communication.
457c9576-4dbb-5c8d-9559-23b75e7a50ab|GoFFish Platform with Subgraph-Centric Storage|The authors propose the use of the GoFFish platform, which provides a subgraph-centric storage model that reduces communication overhead. This approach involves storing each subgraph independently and only exchanging necessary information between subgraphs.
aee9fd6e-909a-58a9-a56a-11fbc826d614|Tight Simulation|Tight simulation is a novel graph pattern matching model that addresses scalability challenges by providing a more stringent notion than strong simulation while being looser than subgraph isomorphism. It extracts a ball from the data graph with a radius equal to the radius of the query graph, ensuring that the result match graph preserves all subgraph isomorphic matches of the query graph.
232c2e51-0fb6-584a-b881-07f4e396737e|Vertex-Centric Distributed Algorithm|The authors propose a vertex-centric distributed algorithm for graph pattern matching, which addresses scalability challenges by distributing the computation among multiple workers. Each worker handles a partition of vertices, and vertices can exchange messages through successive supersteps to learn about their neighborhood or accomplish a computing task.
3683ac8c-70ad-5a95-a722-0d196503dd5c|Distributed Dual Simulation|The authors propose a distributed algorithm for dual simulation, which addresses scalability challenges by extending the distributed algorithm for graph simulation to check parent relationships as well.
569a7de8-8964-5a72-926d-26bff6c40f12|Strict Simulation|The authors propose a strict simulation model, which addresses scalability challenges by providing a more stringent notion than strong simulation while being looser than subgraph isomorphism. It extracts a ball from the data graph with a radius equal to the diameter of the query graph, ensuring that the result match graph preserves all subgraph isomorphic matches of the query graph.
51fc999d-26cf-5074-9a8f-641ef9ad81db|Tight Simulation Model|The authors propose a novel graph simulation model called tight simulation, which is designed to minimize communication overhead by reducing the number of messages exchanged between nodes. Tight simulation achieves this by selecting a single vertex as a candidate match to the center of a potential ball on the data graph, and then creating a ball around it with a radius equal to the radius of the query graph. This approach reduces the number of messages required for ball creation, resulting in lower communication overhead.
b2fd529c-0128-53ec-97f0-f58185cde61b|Ball Creation Optimization|The authors propose an optimization technique for ball creation, which is designed to minimize communication overhead by reducing the number of messages exchanged between nodes. The technique uses a BFS-style algorithm for discovering balls, which reduces the number of supersteps required for the algorithm to terminate.
b647fc01-4242-5996-a6ce-edfb9647a2b5|Sparsity Optimization|The authors propose a sparsity optimization technique to reduce the computational cost of the algorithm by dynamically selecting between sparse matrix-vector multiplication (SpMV) and sparse matrix-sparse vector multiplication (SpMSpV) based on the number of vertices modified in each iteration.
b5638106-67ec-5f9e-b423-04ce747bb134|Broadcasting-based Implementation|The authors propose a broadcasting-based implementation for the extract and assign operations to reduce the load balancing issue caused by skewed access patterns.
4f88c2d4-2952-5459-a727-11b41f050748|Manual Implementation of Extract and Assign Operations|The authors propose a manual implementation of the extract and assign operations to avoid the load balancing issue caused by the default implementation in CombBLAS.
c17d0f3a-8e5f-5a9a-87ee-2a3db0cee6f4|Early Termination Detection|The authors propose an early termination detection technique to reduce the number of iterations required by the algorithm.
7be62ede-43ce-517d-a1b6-cc6205b317b3|Broadcasting-based Implementation for Extract and Assign Operations|The authors propose a broadcasting-based implementation for the extract and assign operations to address the load balancing challenge. This solution involves broadcasting the data to all processing units, ensuring that each unit has access to the required data, thereby reducing the load imbalance caused by uneven data distribution.
4d9c74e4-e5d4-5214-854d-3a799063f295|SpMSpV-based Optimization for Last Few Iterations|The authors propose an optimization technique that uses SpMSpV (Sparse Matrix-Sparse Vector multiplication) instead of SpMV (Sparse Matrix-Vector multiplication) for the last few iterations of the algorithm. This solution addresses the load balancing challenge by reducing the computational workload and communication overhead.
42055203-6612-5936-ad38-bf8cb52b4f56|Hierarchically Grouped Distributed Vertex Hashing TRI ST IMPR (DVHT i)|DVHT i is a distributed algorithm that addresses scalability challenges by utilizing a hierarchically grouped architecture with multiple masters and aggregators. This approach enables the efficient processing of large-scale graph data streams by distributing the workload across multiple machines.
96707817-cc1a-5a0f-8a4f-d391fa8430e4|Distributed Edge Hashing TRI ST BASE (DEHT b)|DEHT b is a distributed algorithm that addresses scalability challenges by utilizing an edge hash function to map edges to workers. This approach enables the efficient distribution of edges across workers, reducing communication overhead and improving workload balance.
c04d6f12-b8aa-59fc-a8ac-899b5a4cc4c3|Distributed Edge Hashing TRI ST IMPR (DEHT i)|DEHT i is a distributed algorithm that addresses scalability challenges by utilizing an edge hash function to map edges to workers. This approach enables the efficient distribution of edges across workers, reducing communication overhead and improving workload balance.
a12ed6a5-7871-5dfa-880b-03f0e07d0594|Distributed Vertex Hashing TRI ST BASE (DVHT b)|DVHT b is a distributed algorithm that addresses scalability challenges by utilizing a vertex hash function to map vertices to workers. This approach enables the efficient distribution of edges across workers, reducing communication overhead and improving workload balance.
93c5d164-cfc1-5f0f-9e9c-c36e24852cdf|Distributed Edge Hashing (DEH)|The authors propose a solution called Distributed Edge Hashing (DEH), which addresses the load balancing challenge by distributing edges to workers using an edge hash function. This approach ensures that each edge is distributed equally to workers, resulting in a more balanced workload.
5bcd9b2f-492c-58f6-b1b4-bd363a34e86e|Hierarchically Grouped Distributed Vertex Hashing (HGDVH)|The authors propose another solution called Hierarchically Grouped Distributed Vertex Hashing (HGDVH), which addresses the load balancing challenge by using a hierarchical grouping of workers and aggregators. This approach allows for a more efficient distribution of edges and reduces communication overhead.
e098f8c8-8ef1-5e3f-bdd8-3b2870fc825b|Distributed Vertex Hashing (DVH)|The authors propose a solution called Distributed Vertex Hashing (DVH), which addresses the load balancing challenge by distributing edges to workers using a vertex hash function. This approach ensures that each edge is distributed to the corresponding worker, resulting in a more balanced workload.
5e6cbd62-afd0-51ef-92c7-21f5acbf200e|Distributed Edge Hashing with Multiple Masters (DEHMM)|The authors propose a solution called Distributed Edge Hashing with Multiple Masters (DEHMM), which addresses the load balancing challenge by using multiple masters to distribute edges to workers. This approach allows for a more efficient distribution of edges and reduces communication overhead.
ef2fabce-2e47-5866-b2d6-300fcb1dfa00|Distributed Edge Hashing (DEHT)|DEHT is a solution that addresses the communication overhead minimization challenge by using an edge hash function to map edges to workers directly. This approach reduces the communication overhead on the master by increasing unicast edges and enables the edge to be distributed in many manners, not just broadcast.
2356a36b-9f62-59c9-827b-04aafb6f0663|Distributed Vertex Hashing (DVHT)|DVHT is a solution that addresses the communication overhead minimization challenge by using a vertex hash function to map vertices to workers. This approach reduces the communication overhead on the master by distributing edges to workers based on the hash values of the vertices.
3c162770-edc9-5bb8-a892-9aebbed1f528|Improved Master-Worker-Aggregator Architecture|The improved Master-Worker-Aggregator architecture is a solution that addresses the communication overhead minimization challenge by using multiple masters and hierarchically grouped aggregators. This approach reduces the load on the single master and aggregator, performing the distributed processing ability better.
b3870d9b-5a6f-521e-89a9-6be135609a32|Path-Based Holistic Detection Plan|The authors propose a path-based holistic detection plan to address the scalability challenges in distributed graph processing frameworks. This plan involves constructing a holistic evaluation plan for multiple queries, which reduces the overall cost by finding frequent paths among queries and reusing the shared computation and communication.
0d3a2158-66e2-5d58-a13d-9b3458bb61fe|Query Set-Aware Sink Vertex Selection|The authors propose a query set-aware sink vertex selection strategy to address the scalability challenges in distributed graph processing frameworks. This strategy involves selecting the sink vertex for each query considering the query set, which reduces the redundant computation and communication cost.
edc68bb7-4635-5cbf-8b5c-8c6c522f3bdc|Rule Merging with Message Superset|The authors propose a rule merging strategy with message superset to address the scalability challenges in distributed graph processing frameworks. This strategy involves merging rules with a message superset, which reduces the redundant computation and communication cost.
fd0f0a45-3aaf-57a3-879e-3ccf10800d26|Rule Sharing with Common Join|The authors propose a rule sharing strategy with common join to address the scalability challenges in distributed graph processing frameworks. This strategy involves sharing rules with common join, which reduces the redundant computation and communication cost.
e5bbed8d-8960-571e-a86e-a522b278807f|Path-Based Edge Covered Plan|The authors propose a path-based edge covered plan to address the load balancing challenge. This plan involves decomposing the query graph into a set of paths, which are then used to guide the message passing process in the distributed graph processing system. The plan uses a path-based approach to distribute the workload, which allows for more even distribution of computational tasks across processing units. The plan also takes into account the structural constraints of the query graph, ensuring that the workload is balanced while maintaining the correctness of the query results. The paper reports that the proposed plan achieves a 30-70X performance improvement on 300 queries compared to existing methods.
edc1e32e-1d1d-5473-88b6-1a25256f5dc8|Frequent Path Selection|The authors propose a frequent path selection strategy to address the load balancing challenge. This strategy involves selecting the most frequent paths in the query graph, which are then used to guide the message passing process. The strategy uses a frequency-based approach to select the most important paths in the query graph, which helps to balance the workload across processing units. The strategy also takes into account the structural constraints of the query graph, ensuring that the workload is balanced while maintaining the correctness of the query results. The paper reports that the proposed strategy achieves a 20-40% reduction in evaluation time and a 30-50% reduction in messages compared to existing methods.
0f2a96bc-76ef-5d49-87df-be572a16db7a|Query Set Aware Sink Vertex Selection|The authors propose a query set aware sink vertex selection strategy to minimize communication overhead in distributed graph processing systems. This strategy involves selecting the sink vertex for each query based on the local structural information and the query set.
84c9d317-3362-5fba-9b23-2f3ff968df7a|Distributed Lovász Local Lemma (DLLL)|The authors propose a distributed algorithm for the Lovász Local Lemma (LLL), which is a powerful tool for solving graph coloring problems. The DLLL algorithm is designed to efficiently handle large-scale graphs by distributing the computation across multiple nodes.
d08bdc1d-1eb5-5900-83b4-7689ec730588|Graph Decomposition|The authors propose a graph decomposition technique that partitions the graph into smaller subgraphs, each of which can be colored independently. This technique is designed to reduce the computational complexity of the graph coloring problem and make it more scalable.
abcf922b-d137-5349-8663-9339d33437fd|Semi-Random Coloring|The authors propose a semi-random coloring algorithm that combines randomization and local computation to solve the graph coloring problem. The algorithm is designed to efficiently handle large-scale graphs by using a combination of randomization and local computation.
5ad34173-0dfd-5dc0-9231-64d1fc0f265e|Deg-1 List Coloring|The authors propose using a deg-1 list coloring algorithm to minimize communication overhead in distributed graph processing systems. This algorithm assigns colors to nodes based on their degree, reducing the need for communication between nodes.
118736ae-38ee-5b1a-8e60-4bf83ea7d222|2-4k Dense Decomposition|The authors propose using a 2-4k dense decomposition algorithm to minimize communication overhead in distributed graph processing systems. This algorithm partitions the graph into dense and sparse components, reducing the need for communication between nodes.
e3fcfebf-2607-5266-9deb-720e02d6cfa8|Buffered Distributed Memory Approach|The authors propose a buffered distributed memory approach to address the scalability challenges in graph processing. This approach involves using a user-defined buffer size to control the amount of data exchanged between processes, reducing the memory requirements and improving performance.
fddbfead-95d9-5b55-88d9-ddb9357be12f|Bloom Filter-based Edge Query|The authors propose using Bloom Filters to improve the edge membership query performance in graph processing. This approach reduces the false positive rate and improves performance by trading off quality.
c73c51e7-fa43-5d54-921c-df12192c1152|Communication-Avoiding Bloom Filter|The authors propose a communication-avoiding Bloom Filter approach to reduce the overall communication volume in graph processing. This approach uses Bloom Filters to encode the outgoing edges on the sender side, reducing the amount of data exchanged between processes.
b013ee9b-3b6f-58d5-8c05-e8fe5b6ac54c|C Associative Containers|The authors propose using C associative containers to store remote edges and improve edge lookups in graph processing. This approach reduces the memory requirements and improves performance.
5d6ecabe-df21-52cc-bde1-837f1a8b5b51|Vertex-Based Partitioning with Edge Balancing|The authors propose a variant of vertex-based partitioning, where they aim to balance the edges across processes, leading to fewer cut edges, a relatively compact process graph, and possibly dissimilar vertices per process. This solution involves a different approach to graph partitioning, focusing on balancing edges rather than vertices. This is achieved by considering the number of edges per process and trying to distribute them evenly, which can lead to a more balanced workload and reduced communication overhead. The paper does not provide specific results for this solution, but it is mentioned as a strategy to improve load balancing.
138ca04d-3011-59fd-890a-fc1fe73af329|Bloom Filter-Based Edge Query|The authors propose using Bloom Filters to accelerate the performance of edge lookups, which is invoked after the remote edges are received. This solution involves using a probabilistic data structure, Bloom Filters, to improve the edge membership query performance, trading off quality for performance. The paper reports that this approach can lead to a performance improvement of up to 10-7 for graphs with hundreds of millions of triangles.
f66a91e1-17a8-511e-8f68-11be3f303d18|MPI Nonblocking Point-to-Point Interface|The authors propose using MPI nonblocking point-to-point interface to reduce communication overhead by allowing for asynchronous data exchange between nodes.
4b553efd-676f-5e19-ac92-89abfd8f81e8|Message Aggregation Buffering|The authors propose using message aggregation buffering to reduce communication overhead by aggregating multiple messages into a single buffer.
8c250af9-1b44-54c7-b925-97a1b441c69c|Serialization and Transparent Caching|The authors propose using serialization and transparent caching to reduce communication overhead by minimizing data exchange and optimizing cache usage.
585b6f34-f380-5d32-b564-ea911a9574bf|GA LP (Genetic Algorithm based on Label Propagation)|GA LP is a local-based genetic algorithm that refines the results of the Label Propagation (LP) algorithm to detect communities in large-scale directed networks.
f9ad205f-eb8d-5547-8d45-9b77a94f0e9c|HavoqGT Framework|The authors propose the HavoqGT framework as a solution to address scalability challenges in graph processing. HavoqGT is a distributed graph processing framework that uses a vertex-centric approach and supports asynchronous processing.
836a1f8d-2e47-5ea9-bfff-54684a9387b2|Search Space Reduction|The authors propose a search space reduction technique to address scalability challenges in graph pattern matching. This technique reduces the search space by eliminating non-matching parts of the background graph and reloading the problem on a smaller set of nodes.
53dc234a-a6f4-5c5f-8b82-eb1cdd52e0c5|Multi-Level Parallelism|The authors propose a multi-level parallelism approach to address scalability challenges in graph processing. This approach offers multiple levels of parallelism, including vertex-level parallelism, edge-level parallelism, and node-level parallelism.
86687de7-52b5-52e9-bf8e-4f9b2ebb9563|Load Balancing through Reshuffling|The authors propose a load balancing technique through reshuffling to address scalability challenges in graph processing. This technique reshuffles the vertex-to-processor assignment to evenly distribute vertices and edges across processing cores.
93adf0de-a65d-5e72-97f1-f2497c239000|Load Balancing through Reloading on a Smaller Processor Set|The authors propose another load balancing solution that involves reloading the pruned graph on a smaller set of nodes, enabling parallel prototype search. This approach is specifically designed to address the load balancing challenge in distributed graph processing systems.
1673b49c-0af3-5a30-8bd8-a27ced978bfb|Asynchronous Processing with Vertex Delegates|The authors propose using asynchronous processing with vertex delegates to minimize communication overhead. This approach allows for overlapping communication with computation, reducing the time spent waiting for messages and improving overall system performance.
29ad3a6c-31b3-56c3-b583-dab0d51136d7|Redundant Work Elimination|The authors propose eliminating redundant work to minimize communication overhead. This approach involves reusing the results of constraint checking to avoid redundant checks and reduce communication overhead.
42794f76-ff8f-5aca-a0df-7bca4b90566c|iCENTRAL|iCENTRAL is a novel incremental algorithm for updating betweenness centrality in evolving graphs. It scales to large graphs by requiring space linear to the graph size.
30890c45-b894-57d7-a268-7f1a0366d528|Parallel iCENTRAL|Parallel iCENTRAL is a shared-memory multi-threaded implementation and a distributed memory implementation of the iCENTRAL algorithm. It scales to large graphs by performing many independent breadth-first traversals in parallel.
b3317387-c44d-533c-9ab1-770c38e6c69f|Biconnected Components Decomposition|Biconnected components decomposition is a graph decomposition technique used in iCENTRAL to reduce the size of the graph where the incremental computation is performed.
62e4d052-0890-5d26-bc5c-38d91fd99af5|Distributed Memory Implementation|The authors propose a distributed memory implementation of their algorithm, iCENTRAL, which addresses the load balancing challenge by dividing the nodes in set Q among the available machines and further dividing the assigned subset of Q among the cores of each machine. This approach allows for parallel processing of the workload, reducing the computational intensity and improving scalability.
0c552371-b496-5e2e-ae5b-193af341e884|Shared Memory Multi-Threaded Implementation|The authors also propose a shared memory multi-threaded implementation of iCENTRAL, which addresses the load balancing challenge by performing multiple breadth-first searches in parallel from different starting nodes. This approach allows for efficient utilization of multiple cores and reduces the computational intensity.
78e9db6c-22ab-5c80-9203-e46f883238be|Parallel iCENTRAL with Embarrassingly Parallel Implementation|The authors propose a parallel version of iCENTRAL that runs on multiple cores on one machine or on many machines. This solution addresses the communication overhead minimization challenge by performing many breadth-first searches from different starting nodes in parallel, with minimal synchronization at the beginning and end of the searches.
100cc070-f034-5df8-aa66-a16ccc9406db|Deterministic Low Diameter Decompositions|The authors propose a deterministic low diameter decomposition algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently handle large-scale graphs by partitioning the graph into smaller subgraphs with low diameter, allowing for faster processing and reduced memory requirements.
c4c1f301-46c8-587a-8f12-77b8cb4e248e|Sparse Neighborhood Covers|The authors propose a sparse neighborhood cover algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently cover the graph with a small number of sparse subgraphs, allowing for faster processing and reduced memory requirements.
1dd7be9c-e2d6-5ebc-87c6-573fa7b72051|High Girth Cycle Decompositions|The authors propose a high girth cycle decomposition algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently decompose the graph into cycles with high girth, allowing for faster processing and reduced memory requirements.
df796705-e47a-58e2-85b1-e381fb214900|Iterated Path Count Flows|The authors propose an iterated path count flow algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently process the graph by iteratively counting the number of paths between nodes, allowing for faster processing and reduced memory requirements.
2dab6d53-0265-53a8-a203-927f8080718b|Length Constrained Cutmatches|The authors propose a length constrained cutmatch algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently find the maximum flow between two nodes in a graph with length constraints, allowing for faster processing and reduced memory requirements.
734980ed-9b14-54f0-ba5f-8422afbfb21c|Deterministic CONGEST Algorithm|The authors propose a deterministic CONGEST algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently process the graph in a distributed manner, allowing for faster processing and reduced memory requirements.
37fa7004-84c0-502f-9c74-01c8dbff5bcb|Multi-Commodity Length Constrained Cutmatches|The authors propose a multi-commodity length constrained cutmatch algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently find the maximum flow between multiple pairs of nodes in a graph with length constraints, allowing for faster processing and reduced memory requirements.
2b50953b-0746-5605-a732-836f97a6bf60|Batched Multiplicative Weights|The authors propose a batched multiplicative weights algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently process the graph by iteratively updating the weights of the edges, allowing for faster processing and reduced memory requirements.
b778927f-4c99-5652-a451-4e250fdb157b|Deterministic Parallel Algorithm|The authors propose a deterministic parallel algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently process the graph in parallel, allowing for faster processing and reduced memory requirements.
2e5de526-8d01-59e3-8113-55d2851ae982|Randomized CONGEST Algorithm|The authors propose a randomized CONGEST algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently process the graph in a distributed manner, allowing for faster processing and reduced memory requirements.
e361be22-fb97-5011-a759-6e6f9f00e571|Deterministic CONGEST Algorithm with High Probability|The authors propose a deterministic CONGEST algorithm with high probability to address the scalability challenges in graph processing. This algorithm is designed to efficiently process the graph in a distributed manner, allowing for faster processing and reduced memory requirements.
1a29c4fc-459a-5dcc-b74a-e53f5ac41a99|Length Constrained Flow Algorithm|The authors propose a length constrained flow algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently find the maximum flow between two nodes in a graph with length constraints, allowing for faster processing and reduced memory requirements.
64015909-72d1-5087-925d-bd4a4737e8a3|Maximal Disjoint Path Algorithm|The authors propose a maximal disjoint path algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently find the maximum number of disjoint paths between two nodes in a graph, allowing for faster processing and reduced memory requirements.
54001cf3-79c9-5d61-94b9-12cb922c5e03|Maximum Disjoint Path Algorithm|The authors propose a maximum disjoint path algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently find the maximum number of disjoint paths between two nodes in a graph, allowing for faster processing and reduced memory requirements.
a165b4ad-d737-52b7-a549-257407eb1a5d|Deterministic Expander Decomposition Algorithm|The authors propose a deterministic expander decomposition algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently decompose the graph into expanders, allowing for faster processing and reduced memory requirements.
908e20a1-d431-5afd-9081-f0f742402892|Cycle Cover|The authors propose a solution called Cycle Cover to address the load balancing challenge. This solution involves computing a cycle cover of a graph, which can be used to distribute the workload evenly across processing units.
98f1496d-8a77-5cd3-be1c-1e1083d77574|Distributed Algorithm for Length-Constrained Flows|The authors propose a solution called Distributed Algorithm for Length-Constrained Flows to address the load balancing challenge. This solution involves computing a length-constrained flow in a distributed graph processing system.
23c704eb-c73e-5621-aaf9-c76883d6c0a3|Length-Constrained Cutmatches|The authors propose the use of length-constrained cutmatches to minimize communication overhead. This approach involves computing a low-congestion h-length collection of paths between two sets of nodes and certifying that there is no low-congestion way of extending the current collection of paths with a moving cut.
faea7e0e-4b19-55e8-8c66-1fe4a52a5c75|Distributed Expander Decompositions|The authors propose the use of distributed expander decompositions to minimize communication overhead. This approach involves decomposing a graph into expanders and then using these expanders to compute a near-optimal flow.
32b27183-75e1-5f26-9274-cef7fd6fe738|Overapproximating Directed Acyclic Graph (ODAG)|ODAG is a data structure used to compress embeddings and make it possible to mine large graphs that would otherwise be too large to fit in memory. ODAGs store embeddings as sequences of numbers representing vertex or edge ids, and use arrays to encode the relationships between these sequences. This allows for a more compact representation of the embeddings, reducing memory costs. The paper shows that ODAGs can reduce memory cost by several orders of magnitude, even in relatively small graphs such as CiteSeer. Removing ODAGs can increase execution time up to 4 times in some experiments.
1c72d8ed-0dbe-540c-b5db-c1ce85c10c37|Two-Level Pattern Aggregation|This solution is an optimization technique for pattern-based aggregation, which is a common operation in graph mining algorithms. The first level of aggregation occurs based on what the authors call 
90ba74df-6b80-5b29-94ca-d3381c621464|Coordination-Free Exploration Strategy|This solution addresses the load balancing challenge by allowing workers to explore the graph independently without the need for coordination. This approach reduces the communication overhead and allows workers to focus on exploring the graph, resulting in improved load balancing.
7e66b4fe-7db5-5846-9e85-524e87eb809e|Embedding Partitioning|This solution addresses the load balancing challenge by partitioning the embeddings among workers in a way that minimizes the load imbalance. This approach ensures that each worker has a similar number of embeddings to process, resulting in improved load balancing.
b2fb294a-049c-5182-9ed7-f8b5807333ee|ODAG-Based Embedding Storage|This solution addresses the load balancing challenge by storing embeddings in a compact and efficient manner using ODAGs. This approach reduces the memory requirements and improves the performance of the system, resulting in improved load balancing.
9dfbd77d-b9cb-541e-9b34-a9c67e8273d3|Ordered Directed Acyclic Graphs (ODAGs)|This solution addresses the communication overhead minimization challenge by compressing embeddings and reducing the number of messages exchanged between workers. ODAGs are used to store and partition embeddings ef ciently, resulting in lower serialization costs and less overhead due to garbage collection.
50e250a3-848d-5566-a112-2551d22bec70|Distributed Random Walk Algorithm|The authors propose a distributed algorithm to compute random walk betweenness centrality in linear time. This solution addresses the scalability challenges by utilizing a distributed approach, where each node in the network only needs to communicate with its neighbors, reducing the overall communication overhead.
3b1bad79-380e-5187-a81f-cba67f0cb0b8|Random Walk Simulation with Bounded Length Constraint|The authors propose a solution to minimize communication overhead by simulating random walks with a bounded length constraint. This approach reduces the number of messages exchanged between nodes, as each node only needs to maintain a limited number of random walks.
66a9c01f-36f3-50f9-a354-05b414deb09d|Distributed Random Walk Computation with O(log n) Random Walks|The authors propose a solution to minimize communication overhead by distributing the random walk computation among nodes, where each node maintains only O(log n) random walks.
3125e2ef-6827-50b4-aab3-243fcbedafa5|Count Aggregation with Degree Normalization|The authors propose a solution to minimize communication overhead by aggregating counts of random walks at each node and normalizing them by the node’s degree.
426be1c2-82ac-55b8-9ea7-d266ad8e8481|Hybridization|The authors propose a hybridization technique that combines the D-stepping algorithm with the Bellman-Ford algorithm to reduce the number of iterative steps and improve scalability.
ed361f9d-55d8-5fab-8335-90a80d88b337|Pruning|The authors propose a pruning technique that reduces the number of relaxations by avoiding redundant relaxations and focusing on long edges.
15eecaa9-6df5-562b-b39a-c873ea28b5e1|Load Balancing|The authors propose a load balancing technique that distributes the load of processing heavy-degree vertices across multiple threads and nodes.
85536a41-e870-5535-8869-5821285eb8c6|Inner-Outer Short (IOS) Heuristic|The authors propose an IOS heuristic that reduces the number of short edge relaxations by classifying short edges into inner and outer edges.
37b93049-f215-5ca2-aa95-ece199126422|Inter-Node Vertex Splitting|The authors propose an inter-node vertex splitting technique that splits heavy-degree vertices across multiple nodes to reduce load imbalance.
94174a9a-cfdf-5cc3-b7ad-19b7104fbacb|Intra-Node Thread-Level Load Balancing|The authors propose an intra-node thread-level load balancing technique that distributes the load of processing vertices across multiple threads within a node.
c96f0292-f429-5fba-97a4-129ccb80d7ca|Intra-Node Thread-Level Load Balancing Strategy|This solution addresses the load balancing challenge by classifying vertices into two groups based on their degree and distributing the load among threads within a node. The strategy involves setting a threshold p and assigning vertices with degree higher than p to a separate group, which is then distributed among threads.
b5f119a0-2a68-5c8e-8632-dbe2d2fbe777|Inter-Node Load Balancing using Vertex Splitting|This solution addresses the load balancing challenge by splitting vertices with extreme degree and distributing their incident edges among other processing nodes. The strategy involves creating new vertices called proxies and connecting them to the original vertex.
341d3e1b-c902-585c-87c2-64d9a70a27d3|Two-Tiered Load Balancing Strategy|This solution addresses the load balancing challenge by combining intra-node thread-level load balancing and inter-node load balancing using vertex splitting. The strategy involves first splitting vertices with extreme degree and then distributing the load among threads within a node.
6fe9607a-c0b4-5949-9234-bd8f443cc43e|Prune Heuristic|The Prune Heuristic is a solution proposed by the authors to minimize communication overhead in distributed graph processing systems. This heuristic focuses on reducing the number of relaxations performed by the algorithm, which in turn reduces the communication volume between nodes.
808deb25-ce2c-5aef-85e7-ea2761169b3c|Hybridization Strategy|The Hybridization Strategy is another solution proposed by the authors to minimize communication overhead. This strategy involves combining the Prune Heuristic with a bucket processing technique to reduce the number of phases and overheads associated with each phase.
4ff308f9-6353-59f4-9adf-a8d699f87f11|Load Balancing Technique|The Load Balancing Technique is a solution proposed by the authors to minimize communication overhead by balancing the load across nodes. This technique is particularly useful in large-scale graph computations where load imbalance can lead to decreased efficiency and scalability.
27c17ca6-ff23-59a5-abbe-2c24e540a31f|Push-Pull Model|The Push-Pull Model is a solution proposed by the authors to minimize communication overhead by optimizing the direction of communication between nodes.
7d03e90e-db4b-577c-94df-07a4b919b8d4|Histogram-Based Estimation|The Histogram-Based Estimation is a solution proposed by the authors to minimize communication overhead by estimating the number of edges belonging to a given weight range.
c1762687-2430-5480-94c9-949a57e67082|Modified DCSC Algorithm|The authors propose a modified version of the DCSC algorithm, which is a parallel algorithm for finding strongly connected components in distributed graphs. This solution specifically addresses the challenge of scalability by allowing for the efficient processing of large-scale graphs across multiple processors.
05cd1434-4b3d-5af5-8e65-800ddce84325|Simultaneous Work on Multiple Problem Instances|The authors propose a technique for simultaneous work on multiple problem instances, which allows for additional parallelism on multiple problem instances. This solution specifically addresses the challenge of scalability by enabling the efficient processing of multiple graphs concurrently.
4bd5f74c-6d36-5bb4-9c2b-8a87a0c51cb5|Task Queue-Based Parallelization|The authors propose a task queue-based parallelization technique, which allows for the efficient processing of graph operations across multiple processors. This solution specifically addresses the challenge of scalability by enabling the efficient processing of large-scale graphs.
203f18e9-7c3d-5ead-9665-1039705f0dea|Hybridization of Modified DCSC and Tarjan's Algorithm|The authors propose a hybridization of the modified DCSC algorithm and Tarjan's algorithm, which allows for the efficient processing of large-scale graphs. This solution specifically addresses the challenge of scalability by enabling the efficient processing of graphs with many strongly connected components.
54808596-d987-5d2f-9ee3-ed97b1e59a15|Hybridization of Modi edDCSC with Tarjan's Serial Algorithm|The authors propose a hybrid approach that combines the Modi edDCSC algorithm with Tarjan's serial algorithm to address the load balancing challenge. This approach involves performing Tarjan's serial algorithm on all processors independently as a first step to detect local strongly connected components, and then collapsing these components into single nodes (supernodes) before applying the Modi edDCSC algorithm.
aa0daed2-5105-596e-9be3-31d12545783a|Adaptive Partitioning of Graphs|The authors propose an adaptive partitioning approach to address the load balancing challenge. This approach involves partitioning the graph into smaller subgraphs based on the number of strongly connected components, and then applying the Modi edDCSC algorithm to each subgraph.
2b19d180-bc42-5d83-8ae8-87600dbfacd4|Binary Tree Termination Detection|The authors propose a binary tree topology for sending termination detection messages to minimize communication overhead. This approach sets up each processor as a node in a binary tree topology, allowing for efficient termination detection.
1d4357a5-b2f1-5cc9-a6ef-cae262349861|Task Queue Management|The authors propose managing task queues to minimize communication overhead. This approach involves checking the task list for emptiness and handling messages from other processors.
8fb65065-4cde-5832-8193-81b194088a5b|Adaptive Vector Sparsity|The authors propose using adaptive vector sparsity to address scalability challenges. This solution involves identifying and eliminating converged forests, which reduces the number of vertices that need to be processed in subsequent iterations. By doing so, the algorithm can take advantage of vector sparsity, leading to improved performance and reduced memory requirements.
fbf9993e-e488-5e3f-9ed3-1cf1479ad5bf|Customized All-to-All Operations|The authors propose using customized all-to-all operations to address scalability challenges. This solution involves identifying and eliminating imbalanced collective communication patterns inherent in the AS algorithm and replacing them with customized all-to-all operations.
a7d33150-58b2-5c5d-a47e-0d389e49978a|Load Balancing and Communication Efficiency|The authors propose using load balancing and communication efficiency techniques to address scalability challenges. This solution involves randomly permuting the rows and columns of the adjacency matrix to achieve load balancing and using a combination of GraphBLAS primitives to reduce communication overhead.
4e69ff45-cfb3-50be-a4a4-1f5e2944316f|Hypercube-based All-to-All Communication|The authors propose replacing the MPI Alltoallv calls with a hypercube-based implementation by Sundar et al. to improve the load balancing challenge. This solution specifically addresses the challenge by reducing the latency cost of all-to-all communication from p-1 to log p, making it more efficient for large-scale distributed graph processing.
35e690ed-ef55-5caa-99f4-0b07ca2232e1|Broadcasting Entries from Low-Ranked Processes|The authors propose broadcasting entries from low-ranked processes instead of participating in all-to-all collective calls to improve load balancing. This solution specifically addresses the challenge by reducing the number of messages exchanged between nodes and minimizing the latency cost.
4cdb2c99-51cf-577d-a52a-2d34c4f2ca27|Non-Blocking MPI Ibcast|The authors propose using non-blocking MPI Ibcast to allow multiple processes to broadcast data independently. This solution specifically addresses the challenge by reducing the latency cost of broadcasting data and improving the overall performance of the algorithm.
17f13ba8-13b5-5f5c-82b0-38936edb4969|Vector Sparsity Exploitation|The authors propose exploiting vector sparsity to reduce communication overhead. They use sparse vectors in the hooking, shortcutting, and starcheck operations to minimize the amount of data communicated. The authors use the sparsity of the input graph and the vectors to reduce the amount of data communicated. They also use the GrB mxv function to multiply a matrix with a vector on a semiring, outputting another vector. The authors report that exploiting vector sparsity helps to reduce the communication overhead and improve the performance of the algorithm.
0bf46ec4-9dbc-5d5e-94d7-5ff69b993733|Distributed Graph Coloring Algorithm|The authors propose a distributed graph coloring algorithm that can efficiently handle large-scale graphs with billions of edges and vertices. The algorithm is designed to improve performance, minimize memory requirements, and optimize data structures and algorithms.
785d3c62-acf3-534a-bbce-2dd4b4e663cb|Distributed 6-Coloring Algorithm for Planar Graphs|The authors propose a distributed 6-coloring algorithm for planar graphs, which is designed to minimize communication overhead by reducing the number of colors used and the number of communication rounds required.
56ba7cf7-604a-5171-bcf2-44acd9d36a44|Multi-Round HCubeJoin|The authors propose a multi-round HCubeJoin approach to minimize communication cost in distributed subgraph counting. This solution specifically addresses the challenge of scalability by reducing the number of duplications of input relations, which leads to lower communication cost.
25eb9bef-28e2-52e8-bf17-e8d688a7276d|Leapfrog with Aggregation|The authors propose a modified Leapfrog algorithm that supports join with group by and aggregation in a pipeline fashion. This solution specifically addresses the challenge of scalability by reducing the group by and aggregation computing cost.
e2006aeb-d8d3-5006-9694-95ffe32f797b|Attribute Ordering|The authors propose an attribute ordering approach that satisfies both the j-order and g-order. This solution specifically addresses the challenge of scalability by reducing the group by and aggregation computing cost.
075fb720-03ea-56d2-a217-376d48122990|Pipeline Approach|The authors propose a pipeline approach that expands attributes one by one based on an attribute order using iterators. This solution specifically addresses the challenge of scalability by reducing the group by and aggregation computing cost.
200f2100-c4ee-5f47-ace1-07e3338c7e8d|Generalized Hypertree Decomposition (GHD)|The authors propose a GHD approach to address the load balancing challenge. This approach involves decomposing a join query into a collection of smaller queries using a generalized hypertree decomposition.
34c360f5-e14d-51a7-a723-5cf38956b009|Modified Leapfrog with Aggregation|The authors propose a modified version of the Leapfrog algorithm that supports join with group by and aggregation in a pipeline fashion. This approach enables the processing of join queries with aggregation without blocking the pipeline mechanism.
692de7c3-2825-5da8-b7f2-fdae7339fdec|Parallel Complex Coloring Algorithm|The authors propose a parallel complex coloring algorithm to address the scalability challenges in graph processing. This algorithm is designed to efficiently color the edges of a bipartite graph, which represents the scheduling problem in input-queued switches. The algorithm uses a parallel and distributed approach to color the edges of the graph, allowing for simultaneous color exchange operations on vertices in the graph. This approach enables the algorithm to scale to large graphs and handle massive amounts of data. The authors demonstrate the effectiveness of the algorithm through simulations, showing that it can achieve nearly 100% throughput and provide acceptable queuing delay even when the switch size is large.
f97befaa-4557-59d7-8dda-94cfe43fe34e|Frame-Based Scheduling Algorithm|The authors propose a frame-based scheduling algorithm that uses the parallel complex coloring algorithm to schedule packets in input-queued switches. This algorithm is designed to address the scalability challenges in graph processing by efficiently scheduling packets in large-scale switches. The algorithm uses a frame-based approach, where packets are accumulated in the input buffer to form a frame, and then the scheduler calculates a set of permutations for packets in the frame. This approach enables the algorithm to handle large amounts of data and scale to large switches. The authors demonstrate the effectiveness of the algorithm through simulations, showing that it can achieve nearly 100% throughput and provide acceptable queuing delay even when the switch size is large.
af28c1fc-d901-54a1-bfb6-5b4c6826c6a4|C DLPA Algorithm|The C DLPA algorithm is a novel method that combines DLPA with the notion of maximal cliques and utilizes a new updating mechanism that updates each node label by probability of its adjacent nodes.
babb95f7-7b10-5021-943a-4704f4deac14|MapReduce Framework|The MapReduce framework is used to parallelize the LPA algorithm, which helps to improve the scalability of the algorithm.
d2e1edd4-df44-5d9d-bb23-8b8305c7d03d|Distributed LPA|The distributed LPA algorithm is a parallel version of the LPA algorithm, which helps to improve the scalability of the algorithm.
a3260b66-2272-5b17-abfc-96cd6bdaf88c|Maximal Cliques|The maximal cliques are used to improve the accuracy of community detection.
d1deae80-6a96-5ab2-8cd1-897b8d669c94|Synchronous Updating Mechanism|The synchronous updating mechanism is used to update the labels of the nodes in the graph.
27bb8482-7acb-5f2a-888e-854b2e0d96f4|Iterative MapReduce KCminer|The authors propose an iterative MapReduce solution, KCminer, to address the scalability challenges of processing large-scale graphs. KCminer is designed to handle massive amounts of data by breaking down the graph processing task into smaller, manageable chunks that can be processed in parallel across multiple machines.
bd23142d-1dfd-5d4e-bad4-aa868f5c2629|Non-Iterative MapReduce KCminer|The authors also propose a non-iterative MapReduce solution, KCminer, to address the scalability challenges of processing large-scale graphs. This solution is designed to process smaller graphs that can fit into the main memory of a single machine.
341e0cbb-aba5-57f6-b428-ccba4f39186a|Parallel Shared Memory KCminer|The authors propose a parallel shared memory solution, KCminer, to address the scalability challenges of processing large-scale graphs on a single multi-processor machine.
ac2f98b6-26ec-5e36-bd5b-8ad762b9c181|Dynamic Load Balancing|The authors propose a dynamic load balancing approach to address the load balancing challenge. This approach involves dynamically adjusting the workload distribution across processing units based on the graph structure and vertex degrees.
2693e1f2-87e7-55c7-b044-e0f691635bd4|Iterative MapReduce|The authors propose an iterative MapReduce approach to address the load balancing challenge. This approach involves breaking down the graph processing task into smaller subtasks and executing them iteratively using the MapReduce framework.
b7aa4f00-8803-5275-9451-d3117a5d1b74|Non-Iterative MapReduce|The authors propose a non-iterative MapReduce approach to address the load balancing challenge. This approach involves executing the graph processing task in a single MapReduce job, without the need for iteration.
09579853-7e5e-53b5-af09-717897a3f3ec|Iterative MapReduce with Shared Queue|The authors propose an iterative MapReduce approach that utilizes a shared queue to minimize communication overhead between nodes. This solution specifically addresses the challenge by reducing the number of MapReduce jobs and the associated I/O costs.
f3cd2cb9-3934-545c-9c4d-dd6850f00429|Non-Iterative MapReduce with In-Memory Data Processing|The authors propose a non-iterative MapReduce approach that processes data in-memory, eliminating the need for intermediate data storage and transfer. This solution specifically addresses the challenge by minimizing the communication overhead associated with data exchange between nodes.
d7f3014f-5303-51a1-ab35-6798be53e10e|Parallel Shared Memory with Multithreading|The authors propose a parallel shared memory approach that utilizes multithreading to minimize communication overhead between threads. This solution specifically addresses the challenge by reducing the need for data exchange and synchronization between threads.
91bde2bb-3722-583d-9862-b59d941ab2d7|Distributed Assembly|The authors propose a distributed assembly approach to address the scalability challenges in processing large-scale RDF graphs. This approach involves assembling local partial matches in parallel across multiple sites, reducing the communication cost and assembly computation cost.
cc851b99-5217-5375-8029-c63a7fdbc2c4|Partitioning-based Join|The authors propose a partitioning-based join approach to address the scalability challenges in processing large-scale RDF graphs. This approach involves partitioning the RDF graph into smaller fragments and processing each fragment in parallel.
786771d9-30c9-5c08-b983-cd1fad7a7df4|Partial Evaluation|The authors propose a partial evaluation approach to address the scalability challenges in processing large-scale RDF graphs. This approach involves evaluating a query on each graph fragment in parallel to obtain local partial matches.
5a113147-5f1f-5bdd-accf-a99d1b222947|Divide and Conquer|The authors propose a divide and conquer approach to address the scalability challenges in processing large-scale RDF graphs. This approach involves dividing the assembly process into smaller sub-problems and solving each sub-problem in parallel.
da6c7c93-4a2e-5329-afcd-84f4467d0509|Distributed Assembly with Partitioning-Based Join Strategy|The authors propose a distributed assembly approach that utilizes a partitioning-based join strategy to address the load balancing challenge. This solution involves partitioning the local partial matches at each site based on the optimal vertex order, which is determined using a divide-and-conquer approach. The partitioned local partial matches are then combined at multiple sites in parallel, reducing the communication cost and assembly computation cost.
e241e7f8-7838-5ac1-86e9-83900e0b9a1b|Divide and Conquer Approach|The authors propose a divide and conquer approach to minimize communication overhead in distributed assembly. This approach involves dividing the search space among multiple sites to avoid duplicate result computation and reduce the amount of data exchanged between sites.
2ed8f5ce-8e74-5fff-8a91-8176ac2ba361|Optimal Partitioning of Local Partial Matches|The authors propose an optimal partitioning strategy for local partial matches to minimize communication overhead in distributed assembly. This strategy involves finding the optimal partitioning of local partial matches that minimizes the join cost.
9881b53f-ddb5-5a60-8007-773e87f5f16b|Timely Dataflow System Implementation|The authors propose implementing the CliqueJoin algorithm on the Timely dataflow system to address scalability challenges. This solution involves re-designing the algorithm to take advantage of Timely’s features, such as its ability to handle large-scale data processing and its support for distributed computing.
7e09d2ee-3bbb-5c43-b535-336125e0ad21|Labelled Cost Evaluation Function|The authors propose a new cost evaluation function for labelled graphs, which extends the original CliqueJoin algorithm to handle labelled subgraph matching. This solution involves refining the result size estimation strategy to consider label information in the graph.
ce1e230b-a8d5-5bc3-ab20-a6e8a39ac6f3|Batch-Based Join Processing|The authors propose a batch-based join processing approach to reduce memory consumption and improve performance. This solution involves buffering data in batches and processing joins in a batch-by-batch manner.
87f134b9-4408-50fa-832a-20e5a8bd1472|Timely Dataflow System|The authors propose using the Timely dataflow system to minimize communication overhead in distributed graph processing. This system allows for more flexible programming and lower system costs compared to other popular engines.
43d91dd0-2772-5a51-9ec1-8e6b397cf8e6|BatchJoin Algorithm|The authors propose the BatchJoin algorithm to minimize communication overhead in distributed graph processing. This algorithm implements external hash join following buffer and batch ideas to save memory and reduce communication.
580107e7-e955-58ba-90da-a41343bce38b|Distributed Greedy Approximation (DistGreedy)|DistGreedy is a distributed algorithm that addresses the scalability challenges in wireless networks by approximating the Maximum Weighted Independent Set (MWIS) problem. It operates in a distributed manner, allowing each vertex to make local decisions based on its neighbors' weights, and achieves a competitive ratio arbitrarily close to 1/2 of the optimal throughput.
572bd8d0-dd5e-5f02-9222-eaa536e5e6f6|Distributed Greedy Approximation (DistGreedy) Algorithm|The DistGreedy algorithm is designed to minimize communication overhead in distributed graph processing systems by approximating the Maximum Weighted Independent Set (MWIS) problem. It operates in a distributed manner, where each node calculates its local maximum weight and selects vertices with weights greater than a certain threshold. The algorithm repeats this process until the graph becomes empty, resulting in a feasible schedule with reduced communication overhead.
88870536-e515-59c9-9dc7-abd7eb6fbfe8|Asynchronous DistGreedy Implementation|The asynchronous DistGreedy implementation is a variant of the DistGreedy algorithm that operates in an asynchronous manner, allowing nodes to transmit data frames at different times. This implementation is designed to work with per-node queues, similar to the IEEE 802.11 DCF protocol.
925ad9a3-54ef-5541-a05b-6855e506326b|Hyper Stepping Algorithm|The Hyper Stepping algorithm is a novel solution proposed by the authors to address the scalability challenges in graph processing. It combines the existing highly optimized stepping and the work-efficient PDH SP algorithms, and adjusts them toward massively parallel systems. The algorithm optimizes the short iteration in stepping by replacing it with a bounded PDH SP algorithm, which reduces the number of iterations and enables a better utilization of parallelism.
5138c5d8-037e-5b9f-be4a-316eb83b8cea|Adaptive Sparse-Dense Selection|The adaptive sparse-dense selection is a technique proposed by the authors to optimize the communication volume and improve load balancing in graph processing. It dynamically switches between sparse and dense representations during computation, based on the number of updated vertices.
fd70f087-bc2f-5f38-b0c3-c8e6660747b0|Quick Preprocessing|The quick preprocessing is a technique proposed by the authors to reduce the time and cost of preprocessing in graph processing. It uses the R-MAT generating information to quickly construct the graph, without removing repeated edges.
58e5539c-0b0e-5d8b-9055-daae7cec1d66|Bounded Edge Weights|The bounded edge weights is a technique proposed by the authors to improve the scalability of graph processing. It sets a non-zero lower bound to the edge weights, which reduces the number of hops in the shortest paths and improves the scalability of the algorithm.
e5e3ad05-44ab-54b8-8de9-1d6351040cf1|3-Level 1.5D Partitioning|The authors propose a 3-Level 1.5D partitioning method to address the load balancing challenge. This method classifies vertices into three types (Extremely High, High, and Low) based on their degree and organizes processors into a 2D mesh. The partitioning strategy aims to reduce communication complexity and achieve load balancing by distributing the workload evenly across processing units.
9e9df8bd-22e2-56ce-8f7d-3821e856f07f|Adaptive Sparse-Dense Mode Selection|The authors propose an adaptive sparse-dense mode selection approach to minimize communication overhead in distributed graph processing. This approach dynamically switches between sparse and dense representations of the graph data during computation, depending on the number of updated vertices.
38412fc3-435c-509c-9c19-b408ace4300b|Direction Optimization|The authors employ direction optimization to minimize communication overhead in distributed graph processing. This technique optimizes the direction of communication between nodes based on the number of updated vertices.
8f4bedc1-01f8-58b7-ae15-432db4001f74|Sub-iteration Asynchrony|The authors propose sub-iteration asynchrony to minimize communication overhead in distributed graph processing. This technique allows for asynchronous optimizations not allowed in the traditional BSP model graph processing.
dd380ea6-fabb-5260-82c3-453c0a0354e4|Iterative Expander Decomposition|The authors propose an iterative expander decomposition approach to address the scalability challenges in listing Kp instances in large-scale graphs. This solution involves iteratively applying an expander decomposition algorithm to the graph, which breaks down the graph into smaller clusters with good mixing times. Each cluster is then assigned a Kp listing problem, and the algorithm ensures that the number of input edges and the bandwidth available are closely related.
e2873031-1921-56d5-88fc-fd9cece38175|Sparsity-Aware Listing Algorithm|The authors propose a sparsity-aware listing algorithm that takes advantage of the sparsity of the graph to improve the performance of the Kp listing algorithm. This solution involves using a sparsity-aware algorithm within each cluster to list Kp instances, which is faster than a general listing algorithm.
9339bca8-b56c-525b-9b04-80f3d568d550|Load-Balanced Partitioning|The authors propose a load-balanced partitioning approach to address the scalability challenges in listing Kp instances in large-scale graphs. This solution involves partitioning the graph into smaller clusters with good mixing times and ensuring that each cluster has a balanced load.
9004888b-be36-58b2-b519-419197dfa764|Iterative Expander Decomposition with Adaptive Edge Removal|The authors propose an iterative approach to expander decomposition, where they repeatedly apply the decomposition algorithm with decreasing arboricity and minimal degree values. This process involves removing edges from the graph, which helps to balance the workload across clusters. The authors also introduce a novel technique for adaptive edge removal, where they identify and remove edges that are likely to cause load imbalance.
221cbf02-339e-5ee9-8a80-d0070bc7fc7c|Sparsity-Aware Kp Listing Algorithm|The authors propose a sparsity-aware Kp listing algorithm that takes into account the graph's arboricity and minimal degree. The algorithm uses a novel partitioning strategy that ensures that each cluster has a balanced workload, and it employs a load-balanced routing scheme to distribute the computational workload evenly across nodes.
0e56ba54-a7c4-594a-a29c-860c60e9ad64|Expander Decomposition with Arboricity Reduction|The authors propose an expander decomposition technique that reduces the arboricity of the graph, which in turn minimizes communication overhead. This technique is used in conjunction with the sparsity-aware Kp listing algorithm.
7973dd1d-af2b-5d01-894e-64f95445920b|Intra-Component Routing Algorithm|The authors propose an intra-component routing algorithm that minimizes communication overhead within each cluster. This algorithm is designed to work in conjunction with the expander decomposition technique and the sparsity-aware Kp listing algorithm.
ad684288-d4a8-56c4-8375-0506ce9b5246|Distributed Fractional Matching Algorithm|The Distributed Fractional Matching Algorithm is designed to address the scalability challenges in graph processing by efficiently computing a fractional weighted matching in a distributed setting. This algorithm is specifically tailored to handle large-scale graphs with billions of edges and vertices.
a947d94d-e804-5db6-a396-ed8a80d1649c|Deterministic Rounding Algorithm|The Deterministic Rounding Algorithm is designed to address the scalability challenges in graph processing by efficiently rounding a fractional weighted matching to an integer solution. This algorithm is specifically tailored to handle large-scale graphs with billions of edges and vertices.
c916045d-0167-571e-9388-dad699193085|Reduction to Bipartite Case|The authors propose a reduction technique that addresses the communication overhead minimization challenge by reducing the problem of computing a fractional weighted matching in a general graph to the problem of computing a fractional weighted matching in a bipartite graph.
5db1bbbb-2a02-5ef9-8b7d-287810d1b003|Path Setup and Augmentation|The authors propose a technique for setting up and augmenting paths in the graph, which addresses the communication overhead minimization challenge by reducing the number of messages exchanged between nodes.
f4053090-9bf4-5bde-aa41-257bfafd8acb|Distributed PageRank Computation using Monte Carlo Methods|The authors propose a distributed algorithm for computing PageRank using Monte Carlo methods, which addresses the scalability challenges by allowing for efficient computation of PageRank in large-scale graphs.
c3d52772-5e01-590d-9058-69a8e3d9e8f6|Improved Distributed PageRank Computation using Short Random Walks|The authors propose an improved distributed algorithm for computing PageRank using short random walks, which addresses the scalability challenges by reducing the number of random walks required to compute PageRank accurately.
26a5ef72-b42c-51f1-bf8d-1cd750afa526|Coupon-Based Random Walk Stitching|This solution addresses the communication overhead minimization challenge by introducing a coupon-based random walk stitching technique. The authors propose a two-phase approach, where short random walks are first performed in parallel, and then stitched together to form longer walks. This approach reduces the need for direct communication between non-neighboring nodes, minimizing the communication overhead.
74afcced-ecd6-53f8-8531-afed1731edf6|Token-Based Walk Extension|This solution addresses the communication overhead minimization challenge by introducing a token-based walk extension technique. The authors propose a method where each node extends the length of its random walk by one step in each round, using tokens to keep track of the walk's progress.
eb64bab4-a907-592b-93b9-90d7f0c30f85|Message Size Reduction|This solution addresses the communication overhead minimization challenge by reducing the message size required for communication between nodes. The authors propose a method where each node only needs to send the count of the number of walks that pass through it, rather than the actual walks themselves.
b6178067-35e3-5e60-9316-d184e06ab49a|TreeDAll|TreeDAll is a distributed subgraph similarity matching algorithm that relaxes a query into a set of spanning trees, decomposes each tree into h-trees, and conducts exact matching and joining in parallel.
0493afcc-f235-59b9-b6f7-1ccd014a6345|Graph Repartitioning|Graph repartitioning is a technique used to redistribute the workload of a graph across a cluster of machines to improve query performance and reduce network overhead.
9bd8f2a6-3d8f-5523-841f-a9c73187c5ce|Query Relaxation and Decomposition|Query relaxation and decomposition involve relaxing a query into a set of sub-queries and decomposing each sub-query into smaller trees to reduce the complexity of the query.
a2d69432-84ba-5abf-bcd5-e9c6d8affd25|Workload Balance Strategy|The workload balance strategy is used to speed up query processing by redistributing the workload of a graph across a cluster of machines.
347790a6-7aba-54e7-9dbc-4ecf1c05d674|Summary Graph|The summary graph is a data structure used to capture the data locality of a graph and reduce network overhead.
6bf9cd57-4e7c-5f24-b49d-1371f34f980c|Workload-Aware Graph Repartitioning Algorithm|The authors propose a workload-aware graph repartitioning algorithm to address the load balancing challenge. This algorithm redistributes the workload of the cluster so that each slave has a similar size workload and communication costs in the cluster are minimized. The algorithm uses a summary graph to monitor the imbalance of the cluster and updates the graph partitioning based on the workload information. It also uses a network flow-based heuristic to maximize the benefits of moving vertices between slaves while maintaining a balanced partition. The experimental results show that the algorithm can significantly reduce the query time and communication cost, and it can adapt to changes in query load and available computing resources.
83b6626c-cea1-5c8f-9779-796c24b761c7|Baseline Repartitioning Algorithm|The authors also propose a baseline repartitioning algorithm that improves an existing partitioning by decreasing the number of cross edges and shifting workloads from overloaded slaves to underloaded slaves. The algorithm uses a directed graph to model the intuitional heuristic and maximizes the benefits of moving vertices between slaves while maintaining a balanced partition. The experimental results show that the algorithm can reduce the number of cross edges and improve the load balance, but it has limitations in handling certain scenarios.
c7114f48-17ff-5198-b062-10103f2fd428|NFlow Algorithm|The authors propose the NFlow algorithm, which is a network flow-based algorithm that maximizes the benefits of moving vertices between slaves while maintaining a balanced partition. The algorithm uses a modified directed graph to model the intuitional heuristic and maximizes the benefits of moving vertices between slaves while maintaining a balanced partition. The experimental results show that the NFlow algorithm can achieve better load balance and reduce the query time and communication cost compared to the baseline repartitioning algorithm.
b66600e9-9551-537c-9251-6127af17d89f|Summary Graph-based Communication Minimization|The authors propose a summary graph-based approach to minimize communication overhead in distributed graph processing. The summary graph captures the data locality of the distributed graph, allowing each slave to hold a small-sized summary graph that can be used to bound remote accesses. The summary graph is used to reduce the number of remote accesses by identifying the slaves that need to be accessed for matching. This approach is different from existing methods that rely on random or sequential data accesses, which can lead to excessive communication. The paper reports that the summary graph-based approach reduces communication costs by up to 90% compared to other algorithms.
14f65b59-79f9-535a-abc1-99220f0a733b|Prefetching-based Communication Minimization|The authors propose a prefetching-based approach to minimize communication overhead in distributed graph processing. The approach involves prefetching h-hop graph data to reduce the number of remote accesses. The prefetching approach is based on the idea of reducing the number of remote accesses by prefetching data that is likely to be accessed in the future. This approach is different from existing methods that rely on on-demand data access, which can lead to excessive communication. The paper reports that the prefetching-based approach reduces communication costs by up to 75% compared to other algorithms.
f8aa0aef-ce72-5ceb-a6fe-c1cd5c2b743a|Workload-aware Graph Repartitioning|The authors propose a workload-aware graph repartitioning approach to minimize communication overhead in distributed graph processing. The approach involves redistributing the workload of the cluster to minimize communication costs. The workload-aware graph repartitioning approach is based on the idea of redistributing the workload to minimize communication costs. This approach is different from existing methods that rely on random or static graph partitioning, which can lead to excessive communication. The paper reports that the workload-aware graph repartitioning approach reduces communication costs by up to 60% compared to other algorithms.
e996a810-ff1a-5164-9f36-a3005f1d8388|Network Flow-based Heuristic|The authors propose a network flow-based heuristic approach to minimize communication overhead in distributed graph processing. The approach involves using a network flow-based algorithm to optimize the graph partitioning. The network flow-based heuristic approach is based on the idea of using a network flow-based algorithm to optimize the graph partitioning. This approach is different from existing methods that rely on random or static graph partitioning, which can lead to excessive communication. The paper reports that the network flow-based heuristic approach reduces communication costs by up to 50% compared to other algorithms.
db66416e-1964-5e55-b6c0-fbbb71f7b331|Distributed , Core Decomposition Algorithm|The authors propose a distributed , core decomposition algorithm that extends the centralized algorithm to distributed graph processing frameworks. This solution addresses the scalability challenges by allowing the algorithm to process large-scale graphs in a distributed manner, reducing the memory requirements and improving performance.
938e6daa-1513-524b-bd32-ac6f8ce991ad|Optimization 1: Reducing Candidate Pairs Checking|The authors propose an optimization technique that reduces the number of candidate pairs to be checked during the , core decomposition process. This solution addresses the scalability challenges by reducing the computational overhead and improving performance.
a94170f5-2a79-570c-aec8-32a747ead9ed|Optimization 2: Leveraging Intermediate Results|The authors propose an optimization technique that leverages intermediate results to reduce the computational overhead of the , core decomposition process. This solution addresses the scalability challenges by reducing the number of iterations and improving performance.
65da7616-389a-5df1-9b52-51a3545fe668|Block-Centric Distributed , Core Decomposition Algorithm|The authors propose a block-centric distributed , core decomposition algorithm that partitions the graph into blocks and processes them in parallel. This solution addresses the scalability challenges by allowing the algorithm to process large-scale graphs in a distributed manner, reducing the memory requirements and improving performance.
2f9f8e6b-9178-59ba-ab4a-889a7da7e018|Bi-Indexes-Based Load Balancing|The authors propose a novel approach to load balancing in distributed graph processing systems by leveraging Bi-Indexes, which are iteratively computed for each vertex based on its neighbors’ information. This approach enables the system to adaptively distribute the workload across processing units, ensuring that no single node becomes a bottleneck.
9084b332-cedf-5c8c-91dc-3e0befde3206|Vertex-Centric Load Balancing|The authors also propose a vertex-centric load balancing approach, which is designed to work in conjunction with the Bi-Indexes-Based Load Balancing solution. This approach involves distributing the workload across processing units based on the vertex-centric framework, where each vertex corresponds to a computing node.
4d3cdbcc-6d8c-5d88-8e83-42bea863f871|Block-Centric Load Balancing|The authors also propose a block-centric load balancing approach, which is designed to work in conjunction with the Bi-Indexes-Based Load Balancing solution. This approach involves distributing the workload across processing units based on the block-centric framework, where the graph is partitioned into blocks and each block is processed by a single computing node.
fe2ff8a1-8d33-5bfe-9864-5b41529ce9a9|Optimization 1 - Reducing Candidate Pairs Examination|Optimization 1 aims to minimize communication overhead by reducing the number of candidate pairs that need to be examined during the computation of n-order Bi-indexes. This is achieved by introducing lower and upper bounds for BnU,u and BnV,v, which allows the algorithm to skip unnecessary computations and reduce the amount of data that needs to be exchanged between nodes.
208e9705-cfb8-5446-b440-9914af6f711f|Optimization 2 - Computing A_v via B_v|Optimization 2 aims to minimize communication overhead by reducing the amount of data that needs to be exchanged between nodes during the computation of n-order Bi-indexes. This is achieved by computing A_v via B_v, which allows the algorithm to reuse intermediate results and reduce the number of messages that need to be sent between nodes.
47da0c18-4908-53e4-b3b5-19393b85fda0|Block-Centric Distributed Algorithm|The block-centric distributed algorithm aims to minimize communication overhead by partitioning the graph into blocks and processing each block in parallel. This approach reduces the amount of data that needs to be exchanged between nodes, as each block can be processed independently.
934d467d-a9e4-51d1-8db1-2f9472ffe1e9|Vertex-Centric Distributed Algorithm with Optimizations|The vertex-centric distributed algorithm with optimizations aims to minimize communication overhead by applying the proposed optimizations (Optimization 1 and Optimization 2) to the vertex-centric algorithm. This approach reduces the amount of data that needs to be exchanged between nodes, as the optimizations prune the search space and reuse intermediate results.
88ff7935-ccec-5c63-99bc-ded68f6abe1f|Workload Aware Distribution Strategy|This solution addresses the challenge of scalability by proposing a workload aware distribution strategy that dynamically chooses the traversal order for each partial subgraph instance. This strategy helps to achieve a good workload balance among workers in each iteration, reducing the impact of imbalance on performance.
0bec497b-c6d0-5ffd-9c53-50496e71e4c9|Light Weight Edge Index|This solution addresses the challenge of scalability by proposing a light weight edge index that can efficiently prune invalid partial subgraph instances. This index helps to reduce the memory and communication overhead, improving the overall performance of the system.
144d7cea-b37c-5343-a867-146035c4b2a6|Automorphism Breaking of the Pattern Graph|This solution addresses the challenge of scalability by proposing a method to break the automorphism of the pattern graph. This approach helps to reduce the number of duplicated partial subgraph instances during runtime, improving the overall performance of the system.
c39d9325-c830-5783-ab36-7d0d25eb15ae|Cost Model Based Initial Pattern Vertex Selection|This solution addresses the challenge of scalability by proposing a cost model based initial pattern vertex selection method. This approach helps to select a good initial pattern vertex that can improve the overall performance of the system.
b4b5d642-3e59-5abd-b22c-d9d968092ae3|Roulette Wheel Distribution Strategy|This solution addresses the load balancing challenge by using a roulette wheel selection method to choose the next expanding vertex for a partial subgraph instance. The probability of selecting a vertex is based on the degree of the data vertex.
8b41da0d-61cb-52b8-91b0-b9ed7f2b9a48|Cost Model-Based Initial Pattern Vertex Selection|This solution addresses the load balancing challenge by selecting an initial pattern vertex that helps to balance the workload across workers. The selection is based on a cost model that estimates the workload of each vertex.
89f46bf3-3fe6-5924-988b-57b40c840901|Automorphism Breaking|The authors propose breaking the automorphism of the pattern graph to reduce communication overhead by minimizing the number of duplicate partial subgraph instances. This approach ensures that each subgraph instance is found exactly once, reducing the number of instances that need to be communicated.
1be514ef-ab11-5b13-bea0-f303389b730c|dGPM (Distributed Graph Pattern Matching)|dGPM is a distributed graph pattern matching algorithm that addresses the scalability challenges by leveraging both partial evaluation and message passing. It conducts local evaluation on a fragment with effective optimization and adopts asynchronous message passing to direct partial results among fragments.
dcf8bf5a-636a-53ef-9909-4da42446ea0c|dGPMd (Distributed Graph Pattern Matching for DAGs)|dGPMd is a distributed graph pattern matching algorithm specifically designed for DAGs (Directed Acyclic Graphs). It addresses the scalability challenges by reducing the number of messages sent and leveraging the topological ranks of query nodes in Q.
55c74f8d-bd0d-575f-b3da-251843bf1864|dGPMt (Distributed Graph Pattern Matching for Trees)|dGPMt is a distributed graph pattern matching algorithm specifically designed for trees. It addresses the scalability challenges by using two rounds of communications between the coordinator and each site.
eed2c278-52b5-5b80-bd99-c4483b8177ef|dGPMd|dGPMd is a distributed graph pattern matching algorithm designed to address the load balancing challenge by scheduling the shipment of updated Boolean variables following the topological ranks of query nodes in Q. This approach reduces the number of messages sent and allows for more efficient processing.
89d9c3ed-4fc2-5eb6-9876-3785dc07fd4c|dGPMt|dGPMt is a distributed graph pattern matching algorithm designed to address the load balancing challenge by using two rounds of communication between the coordinator and each site. This approach allows for more efficient processing and reduces the amount of data shipped.
5ffecb6f-0109-54a3-b651-81093dffd02f|dGPM|dGPM is a distributed graph pattern matching algorithm designed to address the load balancing challenge by using a combination of partial evaluation and message passing. This approach allows for more efficient processing and reduces the amount of data shipped.
a977dd18-0796-56c5-9a45-f646b8831a2e|Incremental Local Evaluation|The authors propose an optimization strategy called incremental local evaluation, which reduces unnecessary computation of lEval following the idea of incremental pattern matching. This strategy is applied upon receiving a message with updated Boolean variables.
27b8c300-eb1c-5491-ae82-f644aa145d20|Tunable Performance Optimization|The authors introduce a tunable performance optimization strategy that allows a site to send not only evaluated Boolean variables but also Boolean equations. This strategy enables a trade-off between data shipment and response time.
28badfc3-d61d-5ffb-8608-c8f7965d9f5f|Message Merging and Batching|The authors propose a message merging and batching strategy, which reduces the number of messages sent by scheduling the shipment of updated Boolean variables following the topological ranks of query nodes in Q.
3f8faba6-4b36-5cd4-b3c1-f73c865b9ca7|Local Dependency Graphs|The authors propose the use of local dependency graphs to keep track of the sites with virtual nodes as in nodes at each site. This allows for more efficient communication and reduces the overhead of shipping data.
2791734f-d104-59cc-a11b-dc1a3c4a1dc8|Distributed MIS Algorithm for Random Geometric Graphs|The authors propose a distributed algorithm for finding a maximal independent set (MIS) in random geometric graphs, which is a fundamental problem in distributed computing. The algorithm is designed to address the scalability challenges of processing large-scale graphs by minimizing awake complexity and optimizing data structures and algorithms.
fca8a947-9fca-5374-ba5b-994c91763368|Modified Distributed MIS Algorithm for Random Geometric Graphs|The authors propose a modified version of their distributed MIS algorithm for random geometric graphs, which achieves a significantly better time complexity of O(d log n polyloglog n) rounds with high probability, where d is the dimension of the random geometric graph.
a18ac5b2-f092-58c3-8a85-e6feeba72c02|Distributed MIS Algorithm for Augmented Erdos-Renyi Random Graphs|The authors propose a distributed algorithm for finding a maximal independent set (MIS) in augmented Erdos-Renyi random graphs, which have a large clustering coefficient.
3b0e3bba-2fb0-5ed3-ab17-a261761600c0|Distributed MIS Algorithm with O polyloglog n Awake Complexity|The authors propose a distributed algorithm for the Maximal Independent Set (MIS) problem in the sleeping model, which aims to minimize the communication overhead by reducing the number of awake rounds. The algorithm partitions the nodes into disjoint sets, called classes, and solves the MIS problem sequentially in each class. This approach allows nodes to enter the asleep state when not needed, reducing the communication overhead.
d829e7de-f756-5d74-9abd-15379df2f45b|Distributed MIS Algorithm with O d polyloglog n Awake Complexity and O d log n polyloglog n Time Complexity|The authors propose a modified version of the previous algorithm, which achieves a slightly worse awake complexity of O d polyloglog n but improves the time complexity to O d log n polyloglog n rounds. This algorithm is designed for random geometric graphs of arbitrary dimension d.
2356b6c0-f054-5cfc-8bde-21bc44789a22|Pipelined Parallelism with Filter Stream Programming|The authors propose a pipelined parallelism approach using filter stream programming to address scalability challenges. This solution involves decomposing the computation into independent tasks, called filters, which are connected through logical streams. Each filter processes data in parallel, allowing for efficient overlap of computation and communication.
c26bf79f-e4cb-51d4-88f7-6bb13de8b9b1|Shared Memory Awareness with NUMA Effects|The authors propose a solution that takes into account the shared memory architecture of modern clusters, specifically the Non-Uniform Memory Access (NUMA) effects. This solution involves splitting the ComputeCC filter into two separate filters, Preparator and Executor, to improve cache utilization and reduce memory access overhead.
0a5fe6f6-0845-52cf-a0dc-ce93d061e342|Replicated Parallelism with DataCutter|The authors propose a solution that uses replicated parallelism with DataCutter, a component-based middleware tool. This solution involves replicating the ComputeCC filter across multiple nodes, allowing for efficient parallelization of the computation.
35cbb8c3-305c-5fa9-a83a-abca3d70c8c7|Shared Memory Awareness with NUMA-Aware Placement|The authors propose a shared memory awareness solution with NUMA-aware placement to address the load balancing challenge. This solution involves placing the ComputeCC filter on multiple NUMA domains to increase the throughput of the system. By taking the hierarchical composition of the architecture into account, the system can minimize memory access latency and maximize memory bandwidth.
315fd02e-2373-5d9d-b8d6-674a87653c0f|Replicated Parallelism with Multiple Cores|The authors propose a replicated parallelism solution with multiple cores to address the load balancing challenge. This solution involves replicating the ComputeCC filter on multiple cores to increase the throughput of the system. By using multiple cores, the system can process multiple iterations of the work simultaneously, allowing for overlap between computation and communication.
11f1a391-5861-59b1-a6dc-75bda2dfdf5f|Pipelined Parallelism|The authors propose using pipelined parallelism to overlap the process of filtering the work and computing the updates on the graph. This approach allows the system to process multiple Streaming Events concurrently, reducing the communication overhead between nodes.
fa26765f-6fb1-5778-ad99-fa279c611bf6|Replicated Parallelism|The authors propose using replicated parallelism to increase the throughput of the system. This approach involves replicating the ComputeCC filter, which performs the real work of computing the new CC scores after each graph modification.
ff7dc391-8d51-5d0e-be14-7257ec84dd56|Shared Memory Awareness|The authors propose taking into account the shared memory architecture of modern clusters to improve the performance of the system. This approach involves optimizing the placement of filters and data to minimize communication overhead.
9e3de840-11a5-58ad-8128-84354e1a5b7c|Kokkos-based Parallel Coloring|The authors propose a Kokkos-based parallel coloring approach to address the scalability challenges in graph processing. This solution utilizes the Kokkos performance portability framework to enable on-node parallelism and Trilinos for distributed MPI-based parallelism.
5ab4c7c4-561b-51c3-b94f-57c88d528bf6|Speculative and Iterative Coloring|The authors propose a speculative and iterative coloring approach to address the scalability challenges in graph processing. This solution involves coloring all local vertices first, then communicating colors of boundary vertices, and finally resolving conflicts through recoloring.
dbd8c621-5179-5317-be7e-5d8e56b14f07|Two-Ghost-Layer Coloring|The authors propose a two-ghost-layer coloring approach to address the scalability challenges in graph processing. This solution involves constructing a second ghost layer to reduce the number of rounds of communication needed.
f007d00b-4dac-5d0a-b67d-62774710ba36|Hybrid Distance-2 Coloring|The authors propose a hybrid distance-2 coloring approach to address the scalability challenges in graph processing. This solution involves using a combination of speculative and iterative coloring and two-ghost-layer coloring.
4e2f7966-a202-56f5-be43-a8dca0a05f55|Two-Ghost-Layer Coloring (D1 2GL)|The authors propose a novel approach to reduce communication overhead by adding a second ghost layer to the subgraphs on each process. This method allows for more efficient conflict resolution and reduces the number of collective communications required.
d234c52f-4b59-5688-b7a5-e1dc6716f148|Optimized Conflict Detection|The authors optimize conflict detection by looking through only the ghost vertices’ adjacencies, as they neighbor all local boundary vertices. This approach reduces the overhead of conflict detection and resolution.
4a84a7c0-7861-54ce-aa34-24d9ec6246ea|Speculative Coloring with Reduced Communication|The authors propose a speculative coloring approach that reduces communication overhead by communicating only recolored owned vertices. This approach ensures that recoloring changes only owned vertices, minimizing the need for additional communication.
a685abdc-2b07-5134-a7e9-d7009f4e77a5|Dynamic Virtual Warp|The authors propose a dynamic virtual warp technique to address the scalability challenges in graph processing. This technique involves dynamically adjusting the size of virtual warps based on the degree distribution of the visited graph, allowing for more efficient processing of high-degree vertices.
ef16600f-18b5-51f6-93f9-66f9789efcc2|Edge Discover|The authors propose an edge discover technique to optimize the visit of middle-size degree vertices. This technique involves assigning threads to edges rather than vertices, reducing thread divergence and improving workload balance.
0d216ad8-9821-5a89-b43b-4d3ae55e9f86|Duplicate Detection and Correction|The authors propose a duplicate detection and correction technique to reduce redundant work in graph processing. This technique involves using a hash table to detect and correct duplicates, reducing the overhead of redundant work.
266384af-8af9-5ac8-b731-728efaf6bdce|Coalesced Read/Write Memory Accesses|The authors propose a technique to induce coalescence in global memory accesses, reducing the overhead of memory accesses.
a749b62b-e699-5dba-b544-baa18a99c609|Single/Multi Block Kernel Switch|The authors propose a technique to switch between single and multi-block kernels based on the frontier size, reducing the overhead of kernel launches and improving performance.
25cc6d29-57d3-566b-8f0a-fc94acdb93c7|Dynamic Virtual Warps|Dynamic Virtual Warps is a solution that addresses the load balancing challenge by dynamically adjusting the warp size based on the graph characteristics. This approach ensures that the workload is evenly distributed across threads, reducing thread divergence and improving overall performance.
83cce2d0-9a71-5ac5-9288-6612f6a3cbbc|Edge Discover Technique|The Edge Discover Technique is a solution that addresses the load balancing challenge by assigning threads to edges rather than vertices. This approach reduces thread divergence and improves workload balancing, especially in graphs with high-degree vertices.
ddcc49f2-b152-5bfd-a2c8-4f48d2a0f977|Dynamic Parallelism|Dynamic Parallelism is a solution that addresses the load balancing challenge by dynamically creating child kernels to manage the workload imbalance due to different vertex degrees. This approach ensures that the workload is evenly distributed across threads, reducing thread divergence and improving overall performance.
bd28fd3f-7f64-58b7-a6fe-1a9e7bde889d|Exclusive Pre x Sum|Exclusive Pre x Sum is a solution that addresses the load balancing challenge by improving data access time and thread concurrency during frontier propagation. This approach reduces thread divergence and improves workload balancing, especially in graphs with irregular structures.
52dd1907-80cc-532c-9873-73f5fbc20368|Dynamic Message Aggregation|The authors propose a dynamic message aggregation technique to reduce the number of messages sent between processors, which is a major contributor to scalability challenges. This technique aggregates multiple small messages designated for the same receiver into a single message, reducing the startup overhead and improving scalability.
0b66f98c-d309-5353-82c2-a221d8c82b06|Indirect Messaging|The authors propose the use of indirect messaging to reduce the number of messages sent between processors. This approach allows messages to be routed through intermediate processors, reducing the number of direct messages and improving scalability.
befbcb1a-5398-5080-bb72-6afa11a07ed2|Contraction-Based Algorithm (CETRIC)|The authors propose a contraction-based algorithm (CETRIC) that reduces the communication volume by contracting the graph and removing non-cut edges. This approach improves scalability by reducing the amount of data that needs to be communicated between processors.
53c548a2-51bd-507d-8217-7dbc80bd804e|Hybrid Parallelism|The authors propose the use of hybrid parallelism to improve scalability. This approach combines thread-level parallelism with distributed memory parallelism to achieve higher performance.
42b21cc2-aa4b-580c-bf5b-0a18df0493b3|Adaptive Thread-Level Load Balancing|The authors propose an adaptive approach on the thread level to reduce the effects of work imbalances caused by skewed degree distributions in the input graph. Instead of partitioning the local subgraph based on vertices, they partition the edge list consisting of local edges during the local phase. For each directed edge u, v, they perform the intersection in parallel, using work stealing to omit the preprocessing for distributing work evenly.
d7e7b9e3-3f3b-5472-b9fc-9de5ea0d345e|Indirect Message Delivery|The authors propose a grid-based indirect message delivery protocol to reduce startup overheads and improve load balancing. This protocol allows messages to be routed through intermediate nodes, reducing the number of direct messages and minimizing communication overhead.
cb502dfd-31b0-56b1-b8ec-09669a5315d1|Message Aggregation with Dynamic Buffering|The authors propose a message aggregation technique that reduces the number of messages sent between processors by buffering multiple small messages and sending them in a single, larger message. This approach is combined with dynamic buffering, where the buffer size is adjusted based on the number of messages to be sent.
8aa53eb8-3924-5ca9-a39f-787440e60301|Indirect Message Delivery using Grid-Based Redirection|The authors propose an indirect message delivery technique that uses a grid-based redirection scheme to reduce the number of messages sent between processors. This approach involves redirecting messages through intermediate processors, reducing the number of direct messages sent between processors.
b869d684-4366-5b41-a556-d42b8de54b36|Contraction-Based Triangle Counting with Reduced Communication Volume|The authors propose a contraction-based triangle counting algorithm that reduces the communication volume by only sending cut edges between processors. This approach involves contracting the graph by removing non-cut edges, reducing the amount of data sent between processors.
80fd1eb6-a29c-5ea2-94fb-8d4ed0e1fe36|Hybrid Parallelism with Task Stealing|The authors propose a hybrid parallelism approach that combines distributed memory parallelism with shared memory parallelism using task stealing. This approach involves using multiple threads per MPI rank to improve parallelism and reduce communication overhead.
84c0a72c-6399-5b3f-8f9b-4533ef53af12|Task Fragmentation|Task fragmentation is a load balancing technique designed to deal with deceitfully parallel problems in graph search. It subdivides a sequential task into a sequence of sequential subtasks, allowing workers to execute a balanced amount of work and push the rest to other workers.
b832f567-69dc-5630-b11d-3937edcedca8|Replicating the Input Graph|Replicating the input graph at each worker allows QFrag to reuse decades of research in sequential algorithms for subgraph isomorphism. This approach enables QFrag to distribute the computation, not the data, and avoids the need for expensive distributed joins.
80a9230c-81b7-5c19-bdf4-aefe739382fb|QFrag Framework|QFrag is a distributed system for graph search that is specifically designed to deal with computationally complex queries rather than extremely large graphs. It is based on the insight that graph search is inherently computationally expensive, and that many practical graphs fit the ever-growing main memory of a single server.
d2e5f2dd-57be-5032-adbc-c735960c499a|Embarrassingly Parallel|Embarrassingly Parallel is a load balancing technique that involves uniformly partitioning the workload among workers, with each worker running a sequential pattern matching algorithm in parallel. This approach is simple but may not be effective in achieving optimal load balancing due to the skew present in natural graphs.
963e75ed-66a5-5e47-b64e-4c9cb8909201|Radar Push Algorithm (RPA)|The Radar Push Algorithm is a distributed algorithm designed to efficiently estimate PageRank values in large-scale graphs. It addresses the scalability challenges by utilizing a novel approach to parallelize random walks, reducing the number of communication rounds and bandwidth requirements.
96d45e31-0d68-5d14-9fb3-6c21ba8eb6d6|Modified Radar Push Algorithm (MRPA)|The Modified Radar Push Algorithm is an extension of the Radar Push Algorithm, designed to further improve the scalability of PageRank estimation. It addresses the challenge by introducing a new approach to estimate PageRank values using a combination of short and long random walks.
06d3b0ef-566f-57b9-beb7-c9cfb706c435|Batch One-Hop Personalized PageRank (BPPR)|The Batch One-Hop Personalized PageRank algorithm is designed to efficiently estimate personalized PageRank values for a batch of nodes in a large-scale graph. It addresses the challenge by utilizing a novel approach to parallelize random walks and reduce the number of communication rounds.
cc703c4b-d068-5927-a504-40df51a4327a|Radar Push|Radar Push is a distributed algorithm designed to minimize communication overhead in graph processing systems. It achieves this by coordinating the extension of random walks in a way that scatters the walks at a node evenly to its neighbors, reducing the likelihood of flooding a node and channeling many walks to the same neighbor.
ea08d896-3c76-54cb-b842-a83b4b4d8f34|Multi-phase Radar Push (MRP)|MRP is an extension of the Radar Push algorithm, designed to further reduce communication overhead by stitching together more walks in fewer rounds. It achieves this by dividing the random walks into multiple phases, where each phase merges shorter walks to form longer ones, reducing the overall number of rounds required.
60a7013d-df7d-5b83-a021-403b2690eb78|Task-based Vertex Pulling API|The authors propose a task-based vertex pulling API that allows users to write distributed subgraph mining algorithms. This API enables tasks to request vertices and edges for subsequent mining, allowing for highly concurrent vertex accesses and efficient task scheduling.
7768d486-594c-5f0a-b6b0-0c9c8950c9ea|Vertex Cache Design|The authors propose a novel vertex cache design that supports highly concurrent vertex accesses by tasks. The design ensures that tasks can access vertices in parallel without conflicts, minimizing CPU idle time.
4d9e497e-c52a-5080-ab62-3da90d2c8f9b|Lightweight Task Scheduling Workflow|The authors propose a lightweight task scheduling workflow that balances workloads and minimizes CPU idle time. The workflow ensures that tasks are timely put back to task queues when their data becomes ready.
7745eb76-fc7a-50fe-b522-4b792bdbee70|Aggregator-based Result Aggregation|The authors propose an aggregator-based result aggregation approach that enables efficient result aggregation across machines. The approach allows users to aggregate results computed by tasks and synchronize the aggregated values periodically.
2b3cf987-ed56-555c-a64a-9dff507fad82|Work Stealing among Machines|The authors propose a work stealing mechanism among machines to address the load balancing challenge. When a machine is about to become idle, it steals tasks from busy machines for processing. This approach ensures that tasks are evenly distributed across machines, preventing any single machine from becoming a bottleneck.
106318ed-df66-50dc-8ff0-ff97c42bdba5|Task Spilling and Refilling|The authors propose a task spilling and refilling mechanism to address the load balancing challenge. When a task queue is full, tasks are spilled to disk, and when a machine needs more tasks, it refills its task queue from the spilled tasks. This approach ensures that tasks are processed efficiently and reduces the likelihood of bottlenecks.
ee65aea6-d755-59b5-8a2c-107f0e91cffa|Batching Vertex Requests and Responses|The authors propose batching vertex requests and responses to address the load balancing challenge. By batching requests and responses, the system can reduce communication overhead and improve overall performance.
15da7a16-06ab-5d58-9265-9ad63f3b8251|Prioritizing Task Refilling|The authors propose prioritizing task refilling to address the load balancing challenge. When a machine needs more tasks, it prioritizes refilling its task queue from spilled tasks to ensure that tasks are processed efficiently.
0fa11d71-3b8b-5fb4-98ed-6f87019f20ed|Task Stealing and Workload Balancing|The authors propose a task stealing mechanism to balance workloads between nodes. When a node becomes idle, it steals tasks from busy nodes to maintain a consistent workload and minimize communication overhead.
c63d859f-6cf6-543b-80c2-396fa4048f0a|Vertex Caching and Data Reuse|The authors propose a vertex caching mechanism to minimize communication overhead by reusing data. The cache stores frequently accessed vertices, reducing the need for repeated requests and responses.
69ed6eec-409c-5625-a68c-b832c677538a|Extendable Embedding Abstraction|The authors propose an extendable embedding abstraction to address the scalability challenges in graph pattern mining. This abstraction enables the efficient distributed execution of GPM algorithms by considering each extension as a fine-grained task.
35b2a2bb-7e78-5406-9449-faa24290aa20|BFS-DFS Hybrid Exploration|The authors propose a BFS-DFS hybrid exploration approach to address the scalability challenges in graph pattern mining. This approach performs DFS at a chunk granularity while exploring embeddings in a BFS manner within a chunk.
895c8574-c857-58ad-a8cd-9b6d5a06e66a|Static Data Cache|The authors propose a static data cache to address the scalability challenges in graph pattern mining. This cache is shared by all chunks, across different chunks at different levels, and is designed to reduce communication cost.
f9d189e8-2f58-55e0-87bf-acc6ab531d7c|Horizontal Data Sharing|The authors propose a horizontal data sharing approach to address the scalability challenges in graph pattern mining. This approach enables the sharing of edge lists among extendable embeddings in the same chunk.
fd54b1ff-a322-5564-b9d5-cb7cea30fad1|NUMA-Aware Support|The authors propose a NUMA-aware support approach to address the scalability challenges in graph pattern mining. This approach is designed to reduce the cost of cross-socket accesses in modern clusters.
435e6dc8-07ec-55e0-a684-a3d99e29f5ab|Hierarchical Data Representation for Vertical Data Reuse|This solution addresses the load balancing challenge by enabling intermediate result sharing between parent and child extendable embeddings. The hierarchical data representation allows for the storage of intermediate results in an extendable embedding, which can be accessed by its children, reducing the need for redundant computations and improving load balancing.
f9bbba99-f027-5b43-bf11-09cd3e21f493|Static Data Cache with Adaptive Replacement Policy|This solution addresses the load balancing challenge by reducing the communication overhead and improving data locality. The static data cache is designed to store frequently accessed graph data, reducing the need for remote data accesses and improving load balancing.
42a88007-e42d-5d6c-aeb8-acb61ba44d62|BFS-DFS Hybrid Exploration with Fixed-Size Chunks|This solution addresses the load balancing challenge by generating a large number of concurrent tasks with bounded memory consumption. The BFS-DFS hybrid exploration approach generates a large number of tasks that can be executed concurrently, improving load balancing and reducing the risk of bottlenecks.
86efd4ac-c2a9-5e8a-96d3-a2cdac139c1b|Task Pipeline Design|The authors propose a novel task pipeline design to address the scalability challenges in graph mining. This design allows CPU computation, network communication, and disk I/O to be processed asynchronously throughout the entire job process, enabling various resources to work on tasks concurrently.
3bc0714b-c04a-54f4-8121-878d56b5ad2a|BDG Partitioning|The authors propose a novel graph partitioning strategy called BDG partitioning to address the scalability challenges in graph mining. This strategy partitions the graph into blocks and distributes them across machines, reducing the number of vertices and edges that need to be processed.
0559f490-bc94-5e16-ad23-9f82e2f6751a|RCV Cache|The authors propose an RCV cache strategy to address the scalability challenges in graph mining. This strategy uses a cache to store remote vertices and reduce the overhead of network communication.
5f579c44-1711-54c3-97ab-c1e9e4239a13|Task Stealing|Task Stealing is a dynamic load balancing strategy that allows nodes to steal tasks from other nodes that are overloaded. This approach aims to balance the workload among nodes and reduce the overall processing time.
58be7c98-4186-5350-9f00-57ad7497e80a|Locality Sensitive Hashing (LSH)|LSH is a technique used to reduce the dimensionality of the task data and map similar tasks to the same node. This approach aims to improve the locality of the task data and reduce communication overhead.
b17f4018-e594-522a-8138-83218ac58744|Task Priority Queue with Locality Sensitive Hashing (LSH)|The authors propose a task priority queue that utilizes Locality Sensitive Hashing (LSH) to order tasks based on their remote candidates. This approach aims to minimize communication overhead by grouping tasks that require similar remote vertices together, reducing the number of times these vertices need to be pulled from remote machines.
2c3c7a3a-ff29-5dd2-9210-64aa67e0dd38|Process-Level Cache with Reference Counting|The authors propose a process-level cache that stores remote vertices and uses reference counting to manage the cache. This approach aims to minimize communication overhead by avoiding repetitive pulling of remote vertices and reducing the number of cache updates.
9c5ac134-55bd-54cb-9fdc-0dae9aa33ddd|Task Stealing with Cost Function and Local Rate|The authors propose a task stealing mechanism that uses a cost function and local rate to determine the tasks to be migrated between workers. This approach aims to minimize communication overhead by migrating tasks that are likely to benefit from being executed on a different worker.
dcce33ff-a6a5-5563-85f3-4efa2a4863c6|Distributed CONGEST Approximation of Weighted Vertex Covers and Matchings|The authors propose a distributed algorithm to approximate the minimum weighted vertex cover and maximum weighted matching problems in the CONGEST model. The algorithm is designed to efficiently handle large-scale graphs with billions of edges and vertices.
aa003385-99a5-5e8d-bc51-0b8c3c5de4e0|Diameter Reduction|The authors propose a method to reduce the diameter of the communication graph in the CONGEST model, which is essential for efficient distributed graph processing.
d6c74e54-fcdb-5209-9a3b-13db928cbb23|Fractional Approximation Algorithm|The authors propose a fractional approximation algorithm for the minimum weighted vertex cover problem.
6db375e1-e5ba-56f8-b2cb-3a239e3c11d8|Deterministic Distributed Vertex Coloring|The authors propose a deterministic distributed algorithm for vertex coloring, which is used as a subroutine in their algorithm for approximating the minimum weighted vertex cover.
f02c2e14-df2e-5c7e-824a-09e48ee37803|Randomized Distributed Vertex Coloring|The authors propose a randomized distributed algorithm for vertex coloring, which is used as a subroutine in their algorithm for approximating the minimum weighted vertex cover.
736b6c45-4d1f-57e9-bba3-1ef1ce96e4fd|Distributed Maximum Matching|The authors propose a distributed algorithm for maximum matching, which is used as a subroutine in their algorithm for approximating the minimum weighted vertex cover.
304f99dc-e37f-5012-8275-e11405b372d5|Distributed Minimum Vertex Cover|The authors propose a distributed algorithm for minimum vertex cover, which is used as a subroutine in their algorithm for approximating the minimum weighted vertex cover.
99c466a0-4ce1-5fe1-8406-14964a56c961|Distributed Maximum Weighted Matching|The authors propose a distributed algorithm for maximum weighted matching, which is used as a subroutine in their algorithm for approximating the minimum weighted vertex cover.
293b24de-3fd7-58ea-88ad-24bc2e20e269|Distributed Minimum Weighted Vertex Cover|The authors propose a distributed algorithm for minimum weighted vertex cover, which is used as a subroutine in their algorithm for approximating the minimum weighted vertex cover.
ca44b64e-c1d4-556a-af0c-feca0b4c8cd4|Distributed Bipartite Matching|The authors propose a distributed algorithm for bipartite matching, which is used as a subroutine in their algorithm for approximating the minimum weighted vertex cover.
86cca7d4-2d86-5d2b-bda7-3401fd9ae94b|Distributed Augmenting Paths|The authors propose a distributed algorithm for finding augmenting paths, which is used as a subroutine in their algorithm for approximating the minimum weighted vertex cover.
1155965e-451b-5141-9f42-bd4a69e2db10|Distributed Shortest Augmenting Paths|The authors propose a distributed algorithm for finding shortest augmenting paths, which is used as a subroutine in their algorithm for approximating the minimum weighted vertex cover.
ac02da28-2a25-56a4-a9e5-6708d91d2b6b|Distributed Vertex Cover Approximation|The authors propose a distributed algorithm for approximating the minimum weighted vertex cover problem.
c49d8b02-5f73-5c77-ace1-9e5713744694|Distributed Matching Approximation|The authors propose a distributed algorithm for approximating the maximum weighted matching problem.
be78ee13-abd4-58c5-9b30-f3fd7cd2106f|Distributed Vertex Coloring Approximation|The authors propose a distributed algorithm for approximating the vertex coloring problem.
d1bc5b3e-2579-5d44-92a8-756bfeee30ab|Distributed Maximum Matching Approximation|The authors propose a distributed algorithm for approximating the maximum matching problem.
511bf27f-51f8-550b-9659-d56e423b9cb6|Distributed Minimum Vertex Cover Approximation|The authors propose a distributed algorithm for approximating the minimum vertex cover problem.
ac8c6bb1-3d71-5d3d-aa23-2143dc9389f3|Distributed Maximum Weighted Matching Approximation|The authors propose a distributed algorithm for approximating the maximum weighted matching problem.
809171a3-ebd7-59b6-b234-1965c9a80ee5|Distributed Minimum Weighted Vertex Cover Approximation|The authors propose a distributed algorithm for approximating the minimum weighted vertex cover problem.
120e5909-e0d5-5387-a8e6-290c9d886ca4|Distributed Bipartite Matching Approximation|The authors propose a distributed algorithm for approximating the bipartite matching problem.
a31fcfef-5faf-56bb-b74a-a9ff97dbfc25|SUPPORTED CONGEST Model|The authors propose using the SUPPORTED CONGEST model, which allows for efficient communication between nodes by utilizing a logical graph and a communication graph. This model enables the reduction of communication overhead by allowing nodes to communicate with each other in a more efficient manner.
4ddee716-785b-54ea-8ccb-5e9eac40061f|Clustering-based Approach|The authors propose a clustering-based approach to reduce communication overhead. This approach involves dividing the graph into smaller clusters, each with a small diameter, and then processing each cluster separately.
affbcf50-7556-529a-b29e-a7e02b0b6332|Bipartite Vertex Cover Algorithm|The authors propose a bipartite vertex cover algorithm that can be used to reduce communication overhead. This algorithm involves finding a bipartite vertex cover of the graph and then using this cover to reduce the communication overhead.
d4374ba5-6a0d-51af-a9ee-82343baca6a0|Diameter Reduction Algorithm|The authors propose a diameter reduction algorithm that can be used to reduce communication overhead. This algorithm involves reducing the diameter of the graph and then using this reduced diameter to reduce the communication overhead.
3c099dfb-d276-5631-a0ee-de8ce575d417|Distributed Maximum Matching Algorithm|The authors propose a distributed maximum matching algorithm that can be used to reduce communication overhead. This algorithm involves finding a maximum matching of the graph and then using this matching to reduce the communication overhead.
4af69e0a-4f51-5ac2-9d96-47140e2be234|Augmenting Path Algorithm|The authors propose an augmenting path algorithm that can be used to reduce communication overhead. This algorithm involves finding augmenting paths in the graph and then using these paths to reduce the communication overhead.
fb332e21-58c3-5dd6-a16e-1c77fcad1b86|Set Cover Algorithm|The authors propose a set cover algorithm that can be used to reduce communication overhead. This algorithm involves finding a set cover of the graph and then using this cover to reduce the communication overhead.
9422133b-27dd-5ed1-b6e0-ea13c4670f21|Weighted Vertex Cover Algorithm|The authors propose a weighted vertex cover algorithm that can be used to reduce communication overhead. This algorithm involves finding a weighted vertex cover of the graph and then using this cover to reduce the communication overhead.
e7e71e96-62d6-592c-9cdc-b10bab74ea32|Distributed Approximation Algorithm|The authors propose a distributed approximation algorithm that can be used to reduce communication overhead. This algorithm involves finding a distributed approximation of the graph and then using this approximation to reduce the communication overhead.
ba5ef730-797a-5e04-a0e3-5e181bc86a20|Clustering-based Distributed Algorithm|The authors propose a clustering-based distributed algorithm that can be used to reduce communication overhead. This algorithm involves dividing the graph into smaller clusters and then processing each cluster separately.
8e6a5b9f-aaf3-54fb-8a14-d2ae20134be4|Distributed Vertex Cover Algorithm|The authors propose a distributed vertex cover algorithm that can be used to reduce communication overhead. This algorithm involves finding a distributed vertex cover of the graph and then using this cover to reduce the communication overhead.
e6ce4f2e-4471-591d-a2c0-3fc52ccbc505|Fractional Matching Algorithm|The authors propose a fractional matching algorithm that can be used to reduce communication overhead. This algorithm involves finding a fractional matching of the graph and then using this matching to reduce the communication overhead.
57fed028-e8ae-5c63-abe1-16ec1831591f|Distributed Maximum Weighted Matching Algorithm|The authors propose a distributed maximum weighted matching algorithm that can be used to reduce communication overhead. This algorithm involves finding a maximum weighted matching of the graph and then using this matching to reduce the communication overhead.
4e009964-39c8-564c-9455-4ef1257cbeb5|Augmenting Cycle Algorithm|The authors propose an augmenting cycle algorithm that can be used to reduce communication overhead. This algorithm involves finding augmenting cycles in the graph and then using these cycles to reduce the communication overhead.
04f35e1c-5c34-58fc-bdc5-482d9dda90d3|pL2AP|pL2AP is a parallel algorithm designed to solve the AllPairs similarity search problem in a multi-core environment. It employs cache tiling and dynamic task partitioning to improve cache locality and reduce memory access latency. It also uses a mask-based hash table to efficiently store and retrieve query object values and meta-data. pL2AP achieves speedups of 1.5x-238x over existing parallel baselines and 2x-34x over the fastest serial method on datasets with hundreds of millions of non-zeros.
aec21b75-233f-549f-b8c3-2f880dd191b4|pL2APrr|pL2APrr is a variant of pL2AP that uses a single inverted index and does not consider cache locality in its execution. It uses the same pruning bounds as pL2AP but does not employ cache tiling or dynamic task partitioning. pL2APrr achieves speedups of 1.5x-20x over existing parallel baselines on network datasets and 7x-238x on text datasets.
bb084839-42f2-54e8-8416-ef37150c179f|pAPT|pAPT is a parallel algorithm designed to solve the AllPairs similarity search problem in a multi-core environment. It uses index sharing and dynamic task partitioning to improve cache locality and reduce memory access latency. pAPT achieves speedups of 1.5x-20x over existing parallel baselines on network datasets and 7x-238x on text datasets.
80453a72-0111-5c0b-8225-598d4689ffb8|pIdxJoin|pIdxJoin is a parallel algorithm designed to solve the AllPairs similarity search problem in a multi-core environment. It uses cache tiling and sequential retrieval of objects to search against, but does not use any pruning when computing similarities. pIdxJoin achieves speedups of 1.5x-20x over existing parallel baselines on network datasets and 7x-238x on text datasets.
772f86f5-cc05-5835-aa70-b77ba60602a9|Dynamic Task Partitioning|The authors propose a dynamic task partitioning approach to address the load balancing challenge. This approach assigns a small set of objects to a thread to process as soon as it has finished processing its previous assigned set. This strategy prevents threads from getting assigned objects that finish processing quickly and may jump ahead many places in the processing order, which can lead to loss of cache locality. The dynamic task partitioning approach is unique in that it assigns tasks to threads based on their processing speed, rather than assigning a fixed set of tasks to each thread. This approach allows for more efficient use of processing resources and reduces the likelihood of threads becoming idle due to uneven workloads.
f81452e3-b337-5d9b-a153-ffd5b941968e|Round-Robin Task Assignment|The authors also propose a round-robin task assignment approach to address the load balancing challenge. This approach assigns tasks to threads in a round-robin fashion, ensuring that each thread processes a similar number of tasks. The round-robin task assignment approach is unique in that it ensures that each thread processes a similar number of tasks, regardless of the processing speed of the thread. This approach helps to prevent threads from becoming idle due to uneven workloads.
67c2109f-5f30-510f-bb35-ef8caeb6d684|Block-Based Data Decomposition|The authors propose a block-based data decomposition approach to address the load balancing challenge. This approach partitions the data into blocks and assigns each block to a thread for processing. The block-based data decomposition approach is unique in that it partitions the data into blocks based on the processing speed of the threads. This approach helps to prevent threads from becoming idle due to uneven workloads.
04956e6f-a0da-55de-ba13-6fba489d2fda|Star-Clique Preserved (SCP) Graph Storage Mechanism|The SCP graph storage mechanism is a novel approach to storing graph data in a distributed environment, allowing for the use of both star and clique as join units. This mechanism addresses the scalability challenge by reducing the number of execution rounds and intermediate results, making it more efficient for large-scale graph processing.
482b4e23-df9e-5562-b6b5-b8b632b4ed50|Optimal Bushy Join Plan|The optimal bushy join plan is a novel approach to join planning in distributed graph processing, which addresses the scalability challenge by reducing the number of execution rounds and intermediate results. This plan is more efficient than the left-deep join plan used in TwinTwigJoin.
a6ff2a28-c935-5e14-b128-df3fc43576d8|Clique Compression|Clique compression is a novel approach to reducing the number of intermediate results in distributed graph processing, which addresses the scalability challenge by minimizing memory requirements and optimizing data structures.
aae4a304-6734-518b-99d8-72a9612aee81|Distributed Graph Storage Mechanism|The distributed graph storage mechanism is a novel approach to storing graph data in a distributed environment, which addresses the scalability challenge by minimizing memory requirements and optimizing data structures.
e997126b-aca6-5c41-830f-d25c17da1da3|Dynamic Programming-based Cost Model|The dynamic programming-based cost model is a novel approach to estimating the cost of distributed graph processing, which addresses the scalability challenge by optimizing data structures and algorithms.
bdf841ee-550c-5d97-8ae2-17be1edff08e|Star-Clique Preserved (SCP) Storage Mechanism|The SCP storage mechanism is a technique used to store the data graph in a way that supports both star and clique as join units. This is done by introducing extra edges to the local graph used in TwinTwigJoin.
d1ec4922-ae40-5e37-9215-e24fca5c2da7|FlexMiner Architecture|The authors propose a novel hardware accelerator architecture called FlexMiner, which is designed to address the scalability challenges in graph pattern mining. FlexMiner consists of a collection of processing elements (PEs) that are specialized for GPM search, allowing for massive parallelism and high throughput. FlexMiner’s architecture is unique in that it uses a pattern-aware approach, where the hardware is customized for specific graph patterns. This is achieved through a software-hardware interface that generates a pattern-specific execution plan, which is then loaded onto the hardware. The PEs in FlexMiner are designed to perform set intersection and set difference operations efficiently, reducing memory accesses and improving performance. The authors demonstrate that FlexMiner achieves an average speedup of 10.6 over the state-of-the-art software system, GraphZero, on a variety of graph patterns and datasets.
be5a281f-7db0-5438-983f-deeb8c69de44|Connectivity Map (c-map) Extension|The authors propose an extension to the connectivity map (c-map) data structure, which is used to memoize reusable neighborhood connectivity information. The c-map extension allows for efficient storage and retrieval of connectivity information, reducing memory accesses and improving performance. The c-map extension uses a bitset-based implementation, which is more space-efficient than traditional vector-based implementations. The authors also propose a dynamic management scheme for the c-map, which adapts to the specific needs of the graph pattern mining algorithm. The authors demonstrate that the c-map extension achieves a significant reduction in memory accesses, resulting in improved performance and scalability.
11093903-91d4-5761-9f3f-3ac84b499aeb|Frontier List Memoization|The authors propose a memoization technique for the frontier list, which is used to store intermediate results during graph pattern mining. The frontier list memoization technique reduces redundant computation and improves performance. The authors propose a software-hardware interface that generates a pattern-specific execution plan, which includes the frontier list memoization technique. The technique is implemented using a cache hierarchy, which reduces memory accesses and improves performance. The authors demonstrate that the frontier list memoization technique achieves a significant reduction in redundant computation, resulting in improved performance and scalability.
c207fe6d-d159-5e62-b67d-0a2f2605737b|Pattern-Aware Execution Plan Generation|The authors propose a pattern-aware execution plan generation technique, which generates a customized execution plan for each graph pattern. The technique reduces redundant computation and improves performance. The authors propose a software-hardware interface that generates a pattern-specific execution plan, which includes the pattern-aware execution plan generation technique. The technique uses a dependency tree to represent the control flow dependencies between partial embeddings. The authors demonstrate that the pattern-aware execution plan generation technique achieves a significant reduction in redundant computation, resulting in improved performance and scalability.
9d436af6-7b1d-543b-bcd7-57bdbbaf2673|Graph Partitioning|The authors propose a graph partitioning approach to address the load balancing challenge. This approach involves dividing the graph into smaller subgraphs and assigning each subgraph to a separate node.
984f95f7-2935-5641-99b5-0f9fcf634468|Load-Aware Vertex-Cut Partitioning|The authors propose a load-aware vertex-cut partitioning approach to address the load balancing challenge. This approach involves partitioning the graph based on the load of each vertex and assigning each partition to a separate node.
b5928a54-77e6-59ec-9af4-7dc1a70f3a69|Pattern-Aware Hardware Accelerator|The authors propose a pattern-aware hardware accelerator to minimize communication overhead in distributed graph processing systems. This solution involves designing a specialized hardware accelerator that can efficiently process graph patterns, reducing the need for communication between nodes.
d5eb1023-9263-5cc1-897c-bf7514fa8f70|Connectivity Map (c-map)|The authors propose the use of a connectivity map (c-map) to minimize communication overhead in distributed graph processing systems. The c-map is a data structure that stores the connectivity information of vertices in the graph, allowing for fast and efficient lookup of neighboring vertices.
9e7f546d-57e4-54db-9e3a-5bb3cc4d6731|Task Parallelism|The authors propose the use of task parallelism to minimize communication overhead in distributed graph processing systems. This solution involves dividing the graph processing task into smaller sub-tasks that can be executed in parallel, reducing the need for communication between nodes.
be616def-09da-5a91-8128-67e91ae2ec8f|Memoization|The authors propose the use of memoization to minimize communication overhead in distributed graph processing systems. This solution involves storing the results of expensive function calls and reusing them when the same inputs occur again, reducing the need for communication between nodes.
e392d82b-75ea-5886-9b57-ce79637e86e8|GRAPE (Graph Parallel Engine)|GRAPE is a parallel graph processing system designed to address scalability challenges by parallelizing sequential graph algorithms.
dee4dac8-93fe-549c-a831-c85fdeee07b2|Bounded Incremental Evaluation|Bounded incremental evaluation is a technique used in GRAPE to reduce the cost of iterative graph computations by reusing partial results from previous supersteps.
9b445d45-556e-58e2-bd15-87111e4b429b|Graph Fragmentation|Graph fragmentation is a technique used in GRAPE to partition the graph into smaller fragments, which are then processed in parallel by multiple workers.
57bd38d1-f2ed-5e5a-8c88-0f4d1a38a65e|Message Passing Interface (MPI)|MPI is a standard interface used in GRAPE for message passing between workers.
47387990-9abd-5fda-91ea-9dbca9fa0dcf|Fault Tolerance|Fault tolerance is a technique used in GRAPE to ensure that the system can recover from worker failures.
64bcefd9-2015-5818-996d-7c3b35cabd87|Dynamic Grouping|Dynamic grouping is a solution proposed by the authors to address the load balancing challenge. It involves dynamically grouping a set of border nodes by adding a dummy node and sending messages from the dummy nodes in batches, instead of one by one. This approach effectively reduces the amount of message communication in each synchronization step.
87c8caf6-7c0d-51f0-ad8b-236a8adbfc36|Load Balancer|The Load Balancer is a solution proposed by the authors to address the load balancing challenge. It involves computing an assignment of work units to physical workers to minimize both computational cost and communication cost.
c6d48139-0fac-5d0b-977c-56a2983c81ae|Work Unit Estimation|Work Unit Estimation is a solution proposed by the authors to address the load balancing challenge. It involves estimating the cost at each virtual worker in terms of the fragment size, the number of border nodes, and the complexity of computation.
e423c980-ece7-59a5-984a-ecf27ba911f3|Bounded Incremental Evaluation (BIE)|BIE is a technique used in the GRAPE system to minimize communication overhead by reducing the amount of data exchanged between nodes. It achieves this by promoting bounded incremental algorithms for incremental evaluation, which can express their cost as a function of the size of the changed input.
fc33276c-b2ea-56c4-83ee-7b6f1a16a467|Message Composition and Routing|GRAPE composes messages by identifying variables in Ci.x with changed values, deducing their designations Pj by referencing GP, and grouping them into messages Mj. This approach minimizes communication overhead by reducing the number of messages transmitted.
b27d3e2c-6617-5b01-a233-70bf8ec1e246|Data Partitioning|GRAPE uses a data partitioning strategy to divide the graph into smaller fragments, which are then processed in parallel. This approach minimizes communication overhead by reducing the amount of data that needs to be exchanged between nodes.
23cc0c24-41cb-5ef9-8980-2bf07ed8119f|Query Preserving Compression|GRAPE uses query preserving compression to reduce the size of the graph data, which in turn reduces the communication overhead.
8b7267af-3b1c-5aa3-a371-3bae241df998|Multi-Level 2-Hop (ML2hop) Indexing|The authors propose a novel distributed indexing scheme called Multi-Level 2-Hop (ML2hop) to address the scalability challenges in processing large-scale graphs. ML2hop is designed to efficiently handle set reachability queries in a distributed environment.
e8c64175-2d82-5b69-ace8-67a443097a8b|Bi-Directional Query Algorithm (MLQA)|The authors propose a bi-directional query algorithm, called MLQA, to efficiently resolve positive and negative queries in a distributed environment. MLQA is designed to work with the ML2hop index and can simultaneously activate all elements in source and target vertices to execute message propagation.
87322dd6-f477-573b-9793-f37b9df35122|Partitioning Strategy|The authors propose a partitioning strategy to divide the graph into smaller subgraphs, which can be processed in parallel. This approach helps to reduce the memory requirements and improve the performance of the graph processing system.
4a6d0ea4-f0dd-5082-a37e-25549e6d4459|Incremental Index Maintenance|The authors propose an incremental index maintenance approach to update the ML2hop index when the graph is updated. This approach helps to reduce the overhead of index reconstruction and improve the performance of the graph processing system.
1977f2f0-d6cd-523a-9349-5c807871a057|Graph Partitioning with KaHIP|The authors use the KaHIP graph partitioning method to divide the data graph into smaller subgraphs, which are then distributed across different partitions. This approach helps to address the load balancing challenge by reducing the computational workload on each partition.
706214b8-fbbc-50e4-9ca3-252aa570368c|Pruning Strategies|The authors propose pruning strategies to minimize communication overhead in distributed graph processing systems. The pruning strategies aim to reduce the number of messages exchanged between nodes by eliminating unnecessary messages.
371c00f4-d035-5c98-a2ee-ba432a6f4840|Distributed Expander Decomposition Algorithm|The authors propose a distributed expander decomposition algorithm that can efficiently handle large-scale graphs by decomposing them into smaller subgraphs with high conductance and a small number of inter-component edges. The algorithm uses a combination of low-diameter decomposition, nearly most balanced sparse cut computation, and a hierarchical structure to achieve efficient graph processing. The unique aspect of this approach is the use of a distributed algorithm for computing a nearly most balanced sparse cut, which is a key component in the expander decomposition process. The authors demonstrate the effectiveness of their algorithm by showing that it can achieve a nearly optimal round complexity of O(n^{1/3}) for triangle enumeration in the CONGEST model, which is a significant improvement over previous results.
84356553-3496-527e-a89b-6843bee74b7c|Hierarchical Routing Algorithm|The authors propose a hierarchical routing algorithm that can efficiently route messages in a distributed graph processing system. The algorithm uses a hierarchical structure to reduce the number of rounds required for routing, making it more scalable for large-scale graphs. The algorithm uses a combination of portal assignment, routing, and aggregation to achieve efficient message routing. The unique aspect of this approach is the use of a hierarchical structure to reduce the number of rounds required for routing. The authors demonstrate the effectiveness of their algorithm by showing that it can achieve a round complexity of O(log n) for routing in the CONGEST model, which is a significant improvement over previous results.
51219a9f-c1f2-5c0e-ae0d-d8a5999f3d91|Distributed Data Structure for Routing|The authors propose a distributed data structure for routing that can efficiently store and retrieve routing information in a distributed graph processing system. The data structure uses a combination of portals and routing tables to achieve efficient routing. The data structure uses a combination of portal assignment, routing table construction, and aggregation to achieve efficient routing. The unique aspect of this approach is the use of a distributed data structure to store and retrieve routing information. The authors demonstrate the effectiveness of their data structure by showing that it can achieve a query time of O(log n) for routing in the CONGEST model, which is a significant improvement over previous results.
6087afed-9bd7-5c48-92a7-9ed1f68afbd4|Distributed Expander Decomposition|The authors propose a distributed expander decomposition algorithm to address the load balancing challenge in distributed graph processing systems. This algorithm decomposes the graph into smaller components with good expansion properties, ensuring that the computational workload is evenly distributed across processing units.
59d5844d-dcc5-5616-8c0f-48bb3c2d4a4b|Low-Diameter Decomposition|The authors also propose a low-diameter decomposition algorithm to address the load balancing challenge. This algorithm decomposes the graph into smaller components with low diameter, ensuring that the computational workload is evenly distributed across processing units.
9ef467f5-957f-58d4-bf68-c6c967f216e0|Routing Algorithm|The authors propose a routing algorithm to address the load balancing challenge in distributed graph processing systems. This algorithm routes messages between processing units in a way that minimizes communication overhead and ensures that the computational workload is evenly distributed.
e559aa31-29f8-5718-8ff5-ccfe1632b985|Hierarchical Routing Structure|The authors propose a hierarchical routing structure to minimize communication overhead in distributed graph processing systems. This structure consists of multiple layers, each with a smaller diameter than the previous one, allowing for more efficient communication between nodes.
a67f4bae-ecb2-5328-be42-8c3cc3214075|Distributed Data Structure|The authors propose a distributed data structure to minimize communication overhead in distributed graph processing systems. This data structure allows for efficient querying and updating of graph information, reducing the communication overhead between nodes.
e5da1060-66fa-56eb-a13b-bd1d4a31775b|Pre-partitioned Triangle Enumeration (PTE)|PTE is a distributed algorithm designed to efficiently enumerate triangles in massive graphs by minimizing the amount of shuffled data, total work, and network read. PTE uses a graph partitioning method to divide the input graph into sets of edges, which are then stored in a distributed storage. This approach reduces the amount of shuffled data and allows for more efficient triangle enumeration. PTE also employs a scheduling function to minimize network read and total work. The paper reports that PTE outperforms existing algorithms, including CTTP and MGT, by up to 47% and 17%, respectively. PTE is also able to enumerate more than 3 trillion triangles in the ClueWeb12 graph, which has 6.3 billion vertices and 72 billion edges.
41f03cf1-32a4-5392-a38f-3dbe0fab86d6|PTE with Color Direction (PTECD)|PTECD is a variant of PTE that takes into account the color direction of edges to remove redundant operations and minimize computations. PTECD uses a coloring function to assign colors to vertices and edges, which allows for the identification of redundant operations and the minimization of computations. The paper reports that PTECD shows a slight improvement in performance compared to PTE, but the difference is relatively insignificant.
ffbb997c-6662-56df-8548-2b43bb73806b|PTE with Scheduling (PTESC)|PTESC is a variant of PTE that uses a scheduling function to minimize network read and total work. PTESC uses a scheduling function to carefully schedule triangle computations in subproblems, which reduces the amount of network read and total work. The paper reports that PTESC shows the best performance among all variants of PTE, outperforming CTTP and MGT by up to 47% and 17%, respectively.
8ae5caa4-4db3-5850-b8f1-28a50c819e60|Color-Directed Triangle Enumeration (PTECD)|PTECD addresses the load balancing challenge by considering the color direction of edges to remove redundant operations and minimize computations. This approach ensures that each processing unit only processes the necessary edges, reducing the overall workload and improving load balancing.
cb1152a4-871d-50f2-bf1e-195b8df46b0e|Scheduling-based Triangle Enumeration (PTESC)|PTESC addresses the load balancing challenge by carefully scheduling triangle computations in subproblems to minimize network read. This approach ensures that each processing unit only reads the necessary edge sets, reducing the overall network read and improving load balancing.
8a96881c-a002-5823-b3eb-579bc90af0eb|Color Direction-based Triangle Enumeration (PTECD)|PTECD is an improved version of PTE that further reduces communication overhead by exploiting the color direction of edges. This approach minimizes the number of operations required for intersecting neighbor sets, resulting in improved performance.
5674db25-0ac7-5ee2-b1b0-e6dffb32e36b|Bulk Synchronous Parallel (BSP) Style Lazy Updating Scheme|The authors propose a BSP style lazy updating scheme to address the scalability challenges in graph partitioning. This solution involves updating vertex information only when vertices on the same machine are changed, and ignoring the neighbor vertices belonging to the VLLSV (Very Low Local Score Vertex) to reduce network communication.
ab6d9704-fe60-56c9-848b-4bf928b46e92|Score-Based Label Propagation (LP) Algorithm|The authors propose a score-based LP algorithm to address the scalability challenges in graph partitioning. This solution involves calculating a score for each vertex based on its local and remote edges, and advancing to the LP process only for vertices with a lower score to reduce computation and communication volume.
4b6b9bf7-01bb-511f-a9e5-a0550b3b09cf|Stabilization Process|The authors propose a stabilization process to address the scalability challenges in graph partitioning. This solution involves changing the graph topology based on the most required vertices in each partition, rather than on their lower VP score.
78248316-e022-52d6-ba80-8406910f7905|Parallel Graph Partitioning Algorithm|The authors propose a parallel graph partitioning algorithm to address the scalability challenges in graph partitioning. This solution involves distributing the graph data randomly and evenly among workers, and updating vertex information in parallel using a BSP style lazy updating scheme.
4580265c-fab4-5694-9464-6e318258a0cd|Bulk Synchronous Parallel (BSP) Style Lazy Updating|The authors propose using a BSP style lazy updating scheme to reduce network communication overhead and improve performance. This approach involves updating vertex positions and scores in bulk, rather than individually, to minimize the number of network communications.
f163a634-fd1a-5b79-b16a-d0575397c8bd|Score-Based Vertex Relocation|The authors propose a score-based vertex relocation approach to balance the workload across processing units. This approach involves calculating a score for each vertex based on its degree and the degrees of its neighbors, and then relocating vertices with high scores to reduce the workload imbalance.
9d170ab3-9b03-5e51-b365-b4c9aa3c5823|Edge-Balanced Partitioning Process|The authors propose an edge-balanced partitioning process to balance the workload across processing units. This process involves prioritizing edge balance over vertex balance and using a different metric to determine which vertices to relocate.
c3a8e8de-5252-52bc-8ff0-a8c9f011cd6d|Iteration-Based Network Communication|The authors propose an iteration-based network communication approach to minimize communication overhead. This approach involves sharing only the necessary information, such as the partition positions of changed vertices and the VP Score of their neighbors, after each iteration.
6b77d535-afb8-59f8-a736-53c0ab87d3bc|GraphX-based Graph Parallel Computation|The authors propose using GraphX, a graph abstraction library on top of Spark, to address scalability challenges in graph processing. GraphX allows for seamless combination of graph parallel and data parallel computation in a single system, enabling efficient processing of large-scale graphs.
6ef67a3d-715b-510e-a2f9-1d11ae5f3902|Property Graph Representation of RDF|The authors propose a property graph representation of RDF data, which combines the graph structure with vertex and edge properties. This representation enables efficient processing of large-scale RDF graphs by minimizing memory requirements and optimizing data structures and algorithms.
99e8eeea-4dc6-52b4-853d-7154126a87d5|BGP Matching Algorithm|The authors propose a BGP matching algorithm that uses a vertex-centric approach to match triple patterns in RDF graphs. The algorithm uses a combination of local and remote match sets to efficiently match triple patterns and reduce memory requirements.
4a00483d-03a2-5a14-8910-08ff92cf4136|Data Parallel Computation using Spark|The authors propose using Spark, a general-purpose in-memory cluster computing system, to address scalability challenges in graph processing. Spark enables efficient processing of large-scale graphs by minimizing memory requirements and optimizing data structures and algorithms.
1fc74360-1eee-5ab5-83ae-9ddba4684fab|Combination of Graph Parallel and Data Parallel Computation|The authors propose combining graph parallel and data parallel computation to address scalability challenges in graph processing. This approach enables efficient processing of large-scale graphs by minimizing memory requirements and optimizing data structures and algorithms.
bc7252a3-8fbd-55c2-ac2a-274ea4e4f9e9|Caching Match Sets of Neighbor Vertices|The authors propose caching the match sets of neighbor vertices and only sending the changes from one superstep to the next to minimize communication overhead. This solution involves storing the match sets of adjacent vertices in a cache, allowing the system to only transmit the changes between supersteps instead of the entire match sets. This approach reduces the amount of data exchanged between nodes, thereby minimizing communication overhead.
7575572b-3543-5212-ba58-7bd0e0dcd24a|Hypergraph MIS Algorithm|The authors propose a hypergraph MIS algorithm that can efficiently handle large-scale hypergraphs with billions of edges and vertices. The algorithm is designed to improve performance, minimize memory requirements, and optimize data structures and algorithms.
600823c5-0a4e-5b0d-89df-966f35b16864|Distributed Hypergraph Coloring Algorithm|The authors propose a distributed hypergraph coloring algorithm that can efficiently color large-scale hypergraphs in a distributed setting. The algorithm is designed to address the challenges of scalability, memory bottlenecks, and random access issues.
226c1106-6675-51df-b479-56ad3581b2c4|Round Elimination Framework|The authors propose a round elimination framework that can be used to prove lower bounds on the time required to solve locally checkable problems in hypergraphs. The framework is designed to address the challenges of scalability and complexity of graph data.
58778028-f0e4-5ec4-bca2-094cb98a3200|Hypergraph Maximal Matching Algorithm|The authors propose a hypergraph maximal matching algorithm that can efficiently handle large-scale hypergraphs with billions of edges and vertices. The algorithm is designed to improve performance, minimize memory requirements, and optimize data structures and algorithms.
3394e03c-6649-5360-b758-44a8bbad18f7|Hypergraph MIS Algorithm with Reduced Communication Overhead|The authors propose a distributed algorithm for computing a maximal independent set (MIS) in hypergraphs, which is designed to minimize communication overhead between nodes. The algorithm works by iteratively selecting nodes to join the MIS, while ensuring that each node communicates only with its neighbors and the nodes that are incident to the same hyperedges.
ce7cabbb-6c05-5922-8748-d9c1a416c7b5|Distributed Hypergraph Coloring with Reduced Communication Overhead|The authors propose a distributed algorithm for coloring hypergraphs, which is designed to minimize communication overhead between nodes. The algorithm works by iteratively assigning colors to nodes, while ensuring that each node communicates only with its neighbors and the nodes that are incident to the same hyperedges.
1dd31236-201e-5393-888b-24945271b017|Graph and Workload Collaborative Partitioning|This solution addresses the scalability challenges by proposing a graph and workload collaborative partitioning design. The authors integrate their graph partitioning technique with their workload partitioning design, distributing each 2-hop neighbor partition to one GPU, so that all GPUs work on different workloads of the same hashTable.
1218990c-1a7c-5c42-9ef0-9a2927e402b8|Hashing-Based 2D Partitioning Scheme|This solution addresses the scalability challenges by proposing a hashing-based 2D partitioning scheme. The authors use a hashing-based approach to partition the graph into smaller subgraphs that can fit in GPU memory, allowing for efficient processing of large-scale graphs.
b34655cd-33d8-5ac8-902e-346ec39e7967|Workload Partitioning|This solution addresses the scalability challenges by proposing a workload partitioning approach. The authors distribute the vertices into m subsets and assign each subset to a GPU, allowing for efficient processing of large-scale graphs.
ebf02375-f197-5ee8-b037-a424b7a21e14|Degree-Aware Resource Allocation|The authors propose a degree-aware resource allocation mechanism to mitigate the load balancing challenge. This approach assigns more resources, such as hash buckets, shared memory, and threads, to vertices with higher degrees, thereby balancing the workload across processing units.
75fa1742-a752-57a5-a8dd-473ca56a5916|Atomic Operation-Based Dynamic Workload Assignment|The authors introduce an atomic operation-based dynamic workload assignment method to balance the workload across processing units. This approach allows each warp or CTA to get a chunk of vertices atomically at a time, depending on the graph structure.
b7a43bb3-3cba-5d76-9e99-ef646eddbe0a|Virtual Combination for Intra-Vertex Workload Balancing|The authors propose a virtual combination method to balance the intra-vertex workload. This approach virtually combines the 2-hop neighbors of each vertex to balance the workload across processing units.
57c92dcc-12c6-53cd-b651-c5bc34a6b777|Hashing-Based 2D Graph Partitioning|The authors propose a hashing-based 2D graph partitioning method to balance the workload across processing units. This approach partitions the graph into smaller subgraphs using a hashing-based method, which helps to balance the workload across processing units.
094fc9ef-4e1f-5b40-99a0-a95c6c43617f|2D Graph Partitioning Algorithm|The authors propose a 2D graph partitioning algorithm that partitions the 1-hop neighbors and uses the 1-hop neighbor partitions to build the 2-hop ones, ensuring that the vertex range partitions of 1-hop neighbors are the same as those of 2-hop neighbors. This solution involves a novel approach to graph partitioning, which reduces the number of fetched 2-hop neighbor partitions and minimizes communication overhead. The algorithm ensures that the vertex range of the hashTable overlaps with that of the 2-hop neighbor, reducing the number of partitions that need to be fetched. The paper does not provide specific quantitative results for this solution, but it is mentioned that the 2D graph partitioning algorithm is essential for achieving communication-free and workload-balanced graph partitioning.
df172da0-1e99-55fc-ba88-682a2e0154c4|Distributed Maximal Clique Computation Algorithm|The authors propose a distributed algorithm for computing maximal cliques in large-scale graphs, which addresses the scalability challenges by dividing the task of maximal clique enumeration into many sub-tasks to be computed in parallel. The algorithm uses a share-nothing architecture, where each worker machine computes the maximal cliques for a subset of vertices. The authors also propose a data distribution strategy that reduces the amount of data to be distributed and a workload balancing technique that ensures each worker has a similar amount of work. The authors report a significant reduction in running time when using more machines, with an average reduction of 1.60 times when doubling the number of machines.
bc8b9434-fcc1-59ee-a407-7371f8f9fedf|Vertex Ordering Techniques|The authors propose using vertex ordering techniques, such as core number ordering, degree ordering, and degeneracy ordering, to reduce the complexity of maximal clique enumeration. The authors show that these ordering techniques can significantly reduce the number of set intersections required during the maximal clique enumeration process, which is a major bottleneck in existing algorithms. The authors report that core number ordering achieves the best performance consistently in most cases, with a significant reduction in running time compared to other ordering techniques.
ef2d57a4-29ec-523d-85c7-b30eee150697|Prefix Tree Data Structure|The authors propose using a prefix tree data structure to store the set of maximal cliques, which addresses the scalability challenges by reducing the memory requirements and improving the query performance. The prefix tree data structure allows for efficient storage and retrieval of maximal cliques, and the authors propose algorithms for updating the prefix tree when the graph is updated. The authors report that the prefix tree data structure can efficiently store and retrieve maximal cliques, and the update algorithms can handle graph updates efficiently.
7e6b6c67-124f-5dba-86bb-309f21ad26ac|Work-Efficient Algorithm|The authors propose a work-efficient algorithm for computing maximal cliques, which addresses the scalability challenges by minimizing the amount of work required to compute the maximal cliques. The authors show that the algorithm achieves a work efficiency of O(d^3), where d is the degeneracy of the graph, which is a significant improvement over existing algorithms. The authors report that the algorithm achieves a significant reduction in running time compared to existing algorithms, with a maximum reduction of 1.60 times when doubling the number of machines.
c6e0cbef-0f5c-5264-970d-f7db23a4b31c|Vertex Ordering-based Load Balancing|The authors propose using vertex ordering to address the load balancing challenge. They suggest using core number ordering, degree ordering, or degeneracy ordering to assign IDs to vertices. This ordering helps to reduce the complexity of maximal clique enumeration (MCE) and achieves a better balance of workload across processing units.
e6a6e9c6-5dd8-5a69-9052-b4f9e9fba8de|Data Distribution Strategy|The authors propose a data distribution strategy to address the load balancing challenge. They suggest distributing the data necessary for MCE at each worker machine based on the adjacency list representation of the graph.
d1c5d3b6-9609-5524-b97c-425c8b040967|Vertex Ordering-Based Data Distribution|The authors propose a data distribution strategy that leverages vertex ordering to minimize communication overhead. By ordering vertices based on their core number, degree, or degeneracy, the algorithm can reduce the amount of data exchanged between nodes.
ae34fe54-f942-5676-9b35-1f711ad0d94e|Prefix Tree-Based Maximal Clique Representation|The authors propose a prefix tree-based representation of maximal cliques to reduce communication overhead. By storing maximal cliques in a prefix tree structure, the algorithm can efficiently share common subsets among maximal cliques and reduce the amount of data exchanged between nodes.
db7d2f7f-9950-5ea8-bcc1-6351f097840e|LocalMCE Algorithm with Reduced Set Intersections|The authors propose a modified version of the LocalMCE algorithm that reduces the number of set intersections required during maximal clique enumeration. By intersecting smaller sets, the algorithm can minimize communication overhead and improve performance.
3703d027-8be0-5d11-97c9-f7f217c40a84|Distributed Graph Partitioning Algorithm|The authors propose a distributed graph partitioning algorithm that partitions the edge set of the network into three parts: Em, Es, and Er. This algorithm addresses the scalability challenges by allowing for efficient processing of large-scale graphs.
75108c1c-f5d5-586c-b9bc-01c1106768bc|Triangle Enumeration Algorithm|The authors propose a triangle enumeration algorithm that can efficiently enumerate triangles in a graph. This algorithm addresses the scalability challenges by allowing for fast enumeration of triangles in large-scale graphs.
907f063e-ab31-5e4d-87bc-763115ea34a7|Subgraph Enumeration Algorithm|The authors propose a subgraph enumeration algorithm that can efficiently enumerate s-vertex subgraphs in a graph. This algorithm addresses the scalability challenges by allowing for fast enumeration of subgraphs in large-scale graphs.
2fd33684-49a5-5254-b2af-0bd5c982bc84|Distributed Nibble|Distributed Nibble is a distributed graph partitioning algorithm that addresses the load balancing challenge by partitioning the edge set of the network graph into three parts: Em, Es, and Er. Em is further divided into several connected components, each with minimum degree n and conductance 1/polylog(n). Es induces a low arboricity subgraph, and Er is the remaining edge set. Distributed Nibble uses a combination of random walks, graph clustering, and recursive partitioning to achieve load balancing. The algorithm starts by identifying a set of vertices with high degree and then applies a random walk-based clustering algorithm to partition the graph into smaller components. The components are then further partitioned using a recursive approach, ensuring that the load is balanced across the processing units. The paper demonstrates that Distributed Nibble can be implemented in the CONGEST model using O(D log^9 m) rounds, with a success probability of 1 - 1/poly(m), where D is the diameter of the graph.
f6e8e8da-efce-5182-8688-ae74d95432cd|Routing Algorithm for High Conductance Graphs|The routing algorithm for high conductance graphs is a technique used to simulate CONGESTED CLIQUE algorithms with small overhead in high conductance graphs. This algorithm addresses the load balancing challenge by efficiently routing messages between vertices in the graph, ensuring that the load is balanced across the processing units. The algorithm uses a combination of random walks and graph clustering to identify high conductance components in the graph. Once these components are identified, the algorithm uses a routing scheme to efficiently route messages between vertices within these components. The paper demonstrates that the routing algorithm can be used to solve various graph problems, including triangle enumeration and minimum spanning tree, in O(mix_G * 2^O(log n)) rounds, where mix_G is the mixing time of the graph.
8d10efa4-ec85-5aae-9092-ed2af08f570c|ID Assignment and Vertex Class Allocation|The ID assignment and vertex class allocation solution is a technique used to allocate vertices to different classes based on their degrees. This solution addresses the load balancing challenge by ensuring that vertices with similar degrees are allocated to the same class, reducing the load imbalance across processing units. The solution uses a combination of ID assignment and vertex class allocation to ensure that vertices with similar degrees are allocated to the same class. The ID assignment scheme ensures that vertices with similar degrees have similar IDs, making it easier to allocate them to the same class. The paper demonstrates that the ID assignment and vertex class allocation solution can be used to allocate vertices to different classes in O(D log n) time, where D is the diameter of the graph.
e3bfa995-f54d-577c-a1a7-ccb682fe546f|Edge Pruning (EP)|Edge Pruning is a solution proposed by the authors to address the scalability challenges in processing large graphs. It involves removing unnecessary edges from the graph, which reduces the amount of computation required and minimizes memory usage.
13ceb7dc-1b12-5730-941f-75eefffacf7e|Oblivious Seed (OS)|Oblivious Seed is another solution proposed by the authors to address the scalability challenges in processing large graphs. It involves identifying and propagating seed nodes that can help in discovering connected components.
618177ad-3eaa-5bf9-bc88-ffcc0f8161fa|Finish Computation Sequentially (FCS)|Finish Computation Sequentially is a solution proposed by the authors to address the scalability challenges in processing large graphs. It involves finishing the computation sequentially when the size of the active subgraph goes below a certain threshold.
f9825317-489f-5ff6-9d15-8b2954a747f7|SALTY CRACKER|SALTY CRACKER is a solution proposed by the authors to address the scalability challenges in processing large graphs. It involves combining the EP, OS, and FCS solutions to achieve better performance and scalability.
19f5766e-8d31-550a-962c-2065e8a948ac|Edge Propagation (EP) Optimization|The EP optimization is a solution proposed by the authors to address the load balancing challenge in distributed graph processing. It involves propagating the smallest node identifier seen so far to neighbors, which helps to create new connections among nodes and reduce the degree of high-degree vertices.
a4d8a52d-4b45-5d75-8721-0fc22c17a59d|Optimize Seeds (OS) Optimization|The OS optimization is another solution proposed by the authors to address the load balancing challenge. It involves avoiding potential seed-to-seed connections, which can create high-degree vertices and lead to load imbalance.
b21e3835-9f19-5808-b835-671b1280fd1e|Finish Computation Serially (FCS) Optimization|The FCS optimization is a solution proposed by the authors to address the load balancing challenge. It involves monitoring the size of the active subgraph and sending it to a machine for serial processing when it goes below a certain threshold.
674f6d3e-8f0c-5c37-921f-f39b39538d2f|Distributed Graph Initialization|The authors propose a distributed graph initialization technique to address the scalability challenges in graph processing. This technique is designed to efficiently handle large-scale graphs with billions of edges and vertices.
5bd73e4e-9511-5a40-adea-dcb7f91e5524|Frame-based Scheduling Algorithm|The Frame-based Scheduling Algorithm is proposed to address the load balancing challenge in distributed graph processing systems. This algorithm is designed to distribute the computational workload evenly across processing units by scheduling packets in a frame-based manner. The algorithm ensures that no single node becomes a bottleneck due to uneven data distribution or varying workloads.
0756e06e-7734-5871-8f23-a72476fdedfa|Small Separator Decomposition|The authors propose a small separator decomposition technique to address the scalability challenges in distributed vertex cover reconfiguration. This technique involves decomposing the graph into smaller clusters and a small separator set, allowing for efficient computation of reconfiguration schedules.
d7bc945f-2481-550a-b666-7b5eac50f9be|Distributed Computation of Schedules|The authors propose a distributed computation approach to address the scalability challenges in vertex cover reconfiguration. This approach involves distributing the computation of reconfiguration schedules across multiple nodes, allowing for parallel processing and improved scalability.
7686219e-ff0f-5ea7-996c-95a6f02c3875|Batch Reconfiguration Scheduling|The authors propose a batch reconfiguration scheduling approach to address the scalability challenges in vertex cover reconfiguration. This approach involves grouping nodes into batches and reconfiguring them in parallel, allowing for improved scalability and reduced computation time.
4cd420f4-be03-5762-bc20-950ec90b63cb|Poly-Logarithmic Time Distributed Algorithm|The authors propose a poly-logarithmic time distributed algorithm to address the scalability challenges in vertex cover reconfiguration. This algorithm involves using a combination of distributed algorithms and data structures to enable efficient computation of reconfiguration schedules.
71940a3f-1244-598d-95f9-0838896fa0c8|Distributed Vertex Cover Reconfiguration|The authors propose a distributed algorithm for vertex cover reconfiguration, which involves transforming one vertex cover into another while maintaining a valid vertex cover at all times. This solution addresses the load balancing challenge by allowing nodes to reconfigure their vertex covers in parallel, reducing the computational workload on individual nodes.
d1a1b3b4-2713-5bd6-8971-fb14dafbe9e6|Batch Reconfiguration Schedules|The authors introduce the concept of batch reconfiguration schedules, which allow multiple nodes to reconfigure their vertex covers simultaneously. This solution addresses the load balancing challenge by enabling parallel processing of nodes, reducing the overall computational workload.
85d9e4f1-0731-5afc-8c72-1f028f3ab52b|Svelto Architecture|The Svelto architecture is a custom accelerator design for graph analytics that addresses scalability challenges by providing a dynamic task scheduling mechanism, a parallel memory subsystem, and a multi-channel memory interface.
3b99af16-7031-521e-90c4-991058bcd269|Context Switching Mechanism|The context switching mechanism is a technique used in the Svelto architecture to provide latency tolerance and improve memory bandwidth utilization.
9e578e56-7a6c-5ce9-9990-e8c78200ea96|Multi-Channel Memory Interface|The multi-channel memory interface is a component of the Svelto architecture that provides support for fine-grained atomic memory operations and decouples communication from computation.
8ad113ef-4968-5c2b-9c3b-49ebb2aa1519|Dynamic Task Scheduling with Round Robin Issuing Mechanism|The authors propose a dynamic task scheduling approach that utilizes a Round Robin issuing mechanism to distribute tasks across workers. This solution addresses the load balancing challenge by ensuring that tasks are evenly distributed across workers, reducing the likelihood of any single worker becoming a bottleneck.
131a4158-4a8f-57ac-a254-4733c0f95e53|Context Switching with Multi-Threaded Workers|The authors propose a context switching approach that enables multi-threaded workers to execute tasks concurrently, hiding external memory access latency. This solution addresses the load balancing challenge by allowing workers to switch between tasks, reducing idle time and increasing overall system utilization.
4ee138d5-db49-5d31-8243-060defd1f9f2|Hierarchical Memory Controller with Indirect Routing Mechanism|The authors propose a hierarchical memory controller that utilizes an indirect routing mechanism to manage memory accesses. This solution addresses the load balancing challenge by reducing memory contention and increasing memory bandwidth utilization.
c06f202d-32b1-504b-82b6-12205a093779|Indirect Routing Mechanism|The authors propose an indirect routing mechanism to minimize communication overhead between nodes or processors in distributed graph processing systems. Mechanisms/Techniques: The indirect routing mechanism involves using a chain of arbiters to manage outgoing requests from workers, allowing for efficient routing and arbitration of requests. This approach reduces the latency and overhead associated with communication between nodes. Results: The paper does not provide specific quantitative results for this solution. However, it mentions that the added latency due to the indirect routing mechanism is negligible when the system is fully running, as it is masked by the larger memory latency.
d174053a-9d09-5a9d-8535-614481a51b3a|Parallel Memory Controller|The authors propose a parallel memory controller to optimize data exchange and coordinate tasks between nodes in distributed graph processing systems. Mechanisms/Techniques: The parallel memory controller manages outgoing requests with a Round Robin policy and contains M Round Robin arbiters, one for each channel. Each arbiter is attached to the K/M workers, allowing for efficient arbitration and routing of requests. Results: The paper does not provide specific quantitative results for this solution. However, it mentions that the parallel memory controller is designed to handle the arbitration of requests from workers and manage contention on memory resources, reducing communication overhead.
7c769404-f4ee-5e44-ace0-ae3324b17bd8|G-thinker Architecture|The authors propose a novel graph processing framework called G-thinker, which is designed to address the scalability challenges of large-scale graph processing. G-thinker features a task-based computation model, a highly concurrent vertex cache, and a lightweight task scheduling workflow.
14a54548-274f-5283-8bac-9fe7daa11923|Load Balancing Mechanism|The authors propose a load balancing mechanism to address the scalability challenges of large-scale graph processing. The mechanism involves adding a global task queue to each machine for prioritized scheduling of big tasks, which helps to balance the workloads and minimize CPU idle time.
b7d5334c-1a65-5826-8c1b-9d69c737abda|Task Decomposition Strategy|The authors propose a task decomposition strategy to address the scalability challenges of large-scale graph processing. The strategy involves decomposing big tasks into smaller tasks, which helps to balance the workloads and minimize CPU idle time.
beb97a9a-7e97-577d-8490-dc034e4c1c7a|Global Task Queue with Prioritized Scheduling|The authors propose a solution to the load balancing challenge by introducing a global task queue that allows for prioritized scheduling of tasks. This approach enables the system to move expensive tasks around to idle threads for timely processing, preventing them from being buffered in the local queue of a comper.
57b38c27-b175-5638-b486-958b1a49da1a|Timeout Mechanism for Task Decomposition|The authors propose a timeout mechanism to locate and decompose expensive tasks, which helps to improve load balancing. This mechanism allows the system to adaptively decompose tasks based on their computational cost, preventing straggler tasks from dominating the workload.
82bcd3ab-ec5b-523b-9a3c-a26edae8bfd3|Hybrid Task Decomposition Strategy|The authors propose a hybrid task decomposition strategy that combines the color-ignorant strategy with a new strategy that adopts a more aggressive task decomposition approach when the subgraph is large. This approach helps to improve load balancing by reducing the computational cost of tasks.
2cca6f08-24b9-5bad-b97d-95d3fddf18bf|Remote Vertex Cache|The authors propose a remote vertex cache to minimize communication overhead by storing frequently accessed vertices and their adjacency lists in a cache, reducing the need for repeated requests. The cache is designed to store vertices and their adjacency lists, which are frequently accessed by tasks. This approach reduces the need for repeated requests, minimizing communication overhead. The paper reports that the remote vertex cache leads to a significant reduction in communication overhead, with a 2-3x speedup in performance compared to existing systems.
5d5b8188-eab0-5325-90a8-c4a3bd7b72cd|Task Scheduling with Load Balancing|The authors propose a task scheduling approach with load balancing to minimize communication overhead by prioritizing tasks based on their computational requirements and available resources. The approach prioritizes tasks based on their computational requirements and available resources, ensuring that tasks are executed efficiently and minimizing communication overhead. The paper reports that this approach leads to a significant reduction in communication overhead, with a 2-3x speedup in performance compared to existing systems.
b9397f17-25c3-5263-86b1-2afba640bf8d|Task Decomposition with Timeout Mechanism|The authors propose a task decomposition approach with a timeout mechanism to minimize communication overhead by decomposing tasks into smaller sub-tasks and executing them in parallel. The approach decomposes tasks into smaller sub-tasks and executes them in parallel, using a timeout mechanism to ensure that tasks are executed efficiently and minimizing communication overhead. The paper reports that this approach leads to a significant reduction in communication overhead, with a 2-3x speedup in performance compared to existing systems.
edc743db-0d89-54c9-b9da-605a893e83d5|Heuristic Weighted Memory Based Algorithm (HWMA)|The HWMA is a distributed algorithm designed to address the scalability challenges in the Minimum Weighted Vertex Cover (MWVC) problem. It utilizes a combination of local rules, including a perturbation rule and a weighted memory rule, to improve the efficiency of the solution.
3108c950-cc59-5819-b6c3-98b02c9001c9|Restricted Greed and Memory-Based Algorithm (RGMA)|The RGMA is another distributed algorithm proposed by the authors to address the communication overhead minimization challenge. It operates by utilizing a restricted greed rule to update the actions of nodes based on local information from their neighbors.
c5756067-c19a-57f6-a626-3a617f4ef3be|Fully Best Response (FBR)|The FBR is a distributed algorithm proposed by the authors to address the communication overhead minimization challenge. It operates by utilizing a best response rule to update the actions of nodes based on local information from their neighbors.
7053296a-e142-57d8-b773-f151627e58fa|Distributed Time-Variant Binary Log-Linear Learning Algorithm (TVBLLA)|The TVBLLA is a distributed optimization algorithm designed to solve the vertex cover problem in complex networks. It addresses the scalability challenges by allowing each vertex to update its strategy independently, without relying on a central administrator. This approach enables the algorithm to handle large-scale networks with billions of edges and vertices.
38691116-7739-5c3d-8e52-effad72599db|Time-Variant Binary Log-Linear Learning Algorithm (TVBLLA)|The TVBLLA is a distributed optimization algorithm designed to minimize the communication overhead in distributed graph processing systems. It achieves this by allowing each node to update its strategy based on local information, reducing the need for global communication. The algorithm uses a time-variant learning mechanism, which adapts to the changing environment and optimizes the communication overhead.
11f6de5b-7833-507f-9914-7945bbbb7b5a|Hybrid Communication Layer|The authors propose a hybrid communication layer that supports both pushing and pulling communication modes to address scalability challenges. This layer allows for efficient data transfer and reduces communication overhead.
8f545a15-8ad3-543b-bf48-19f4c8aa8aab|Bounded-Memory Execution|The authors propose a bounded-memory execution technique that ensures the memory usage of subgraph enumeration is bounded by a constant factor. This technique addresses the scalability challenge of handling large-scale graphs with limited memory resources.
1f5cb979-2482-528f-8985-2a8c6301cc75|Work-Stealing Load Balancing|The authors propose a work-stealing load balancing technique that addresses the scalability challenge of handling irregular graph computations. This technique ensures that the load is balanced across machines, improving performance and scalability.
422c7f43-5b72-5a55-ac4d-f4952809e45a|Least Recent Batch Used (LRBU) Cache|The authors propose an LRBU cache that addresses the scalability challenge of handling large-scale graphs with limited memory resources. This cache ensures that the most recently accessed data is cached, improving performance and reducing memory usage.
1347b25a-7194-5320-b3f4-0ade2c711853|Two-Layer Intra- and Inter-Machine Work Stealing|The authors propose a two-layer work stealing approach to address the load balancing challenge. This approach involves maintaining a deque in each worker, where partial results are injected and popped out for processing. The current worker can steal unprocessed tasks locally and send them to a remote machine for load balancing.
daa4dade-0c6d-5b27-968c-d56651cf370e|Work Stealing with Dynamic Scheduling|The authors also propose a work stealing approach with dynamic scheduling to address the load balancing challenge. This approach involves dynamically controlling the memory usage of subgraph enumeration and adapting to DFS-style scheduling if the memory usage exceeds a constant threshold.
2f043271-465d-5200-b45a-d336df9ac136|Batching RPC Requests|The authors propose batching RPC requests to reduce communication overhead. By aggregating multiple RPC requests into a single batch, the system can reduce the number of network requests and improve communication efficiency.
94446bb7-1765-5d2d-adcc-a94448ead3b1|Adaptive Scheduling|The authors propose an adaptive scheduling technique that dynamically adjusts the scheduling strategy based on the system's workload and communication patterns. This approach enables the system to optimize communication overhead and improve overall performance.
7980d7a6-4bfe-5c89-8ab5-d6cc82fb3614|Bulk Synchronous Parallel (BSP) Model|The authors propose using the BSP model to address scalability challenges in graph processing. The BSP model is a parallel computing model that allows for efficient processing of large-scale graphs by dividing the graph into smaller subgraphs and processing them in parallel.
9834bc65-168f-5cc1-9fba-688a32507cfb|Distributed Vertex-Centric Approach|The authors propose a distributed vertex-centric approach to address scalability challenges in graph processing. This approach involves processing the graph in a vertex-centric manner, where each vertex is processed independently, and the results are combined to obtain the final result.
0b2ceae4-6d0f-54c3-80a3-d8e15961aa68|Unidirectional Relaxation Simulation (URS)|The authors propose using URS to address scalability challenges in graph processing. URS is a relaxation simulation algorithm that allows for efficient processing of large-scale graphs by relaxing the matching conditions.
41cc4a73-c8b6-56f6-a720-179d2d01e73f|Dual Relaxation Simulation (DRS)|The authors propose using DRS to address scalability challenges in graph processing. DRS is a relaxation simulation algorithm that allows for efficient processing of large-scale graphs by relaxing the matching conditions.
50bd2516-3874-5932-92cf-a91c80a8587c|Vertex-Centric Parallel Pattern Matching|The authors propose a vertex-centric parallel pattern matching approach to address the load balancing challenge. This approach involves assigning each vertex in the graph to a separate processing unit and processing the graph in parallel.
d9360848-e68b-5565-a4fb-1225e9c0ea1c|Hama BSP Framework|The authors propose using the Hama BSP framework to address the load balancing challenge. Hama is a general BSP framework on top of Hadoop that allows for the efficient processing of large-scale graph data.
7eb60330-2606-5199-a4f5-dc7570f02e11|Hierarchical Graph Partitioning Algorithm (HGPA)|HGPA is a distributed algorithm designed to address scalability challenges in graph processing. It partitions the graph into a hierarchy of subgraphs, allowing for efficient computation of personalized PageRank vectors (PPVs) on large-scale graphs.
bd8e9296-70b4-56f2-a604-e8ae3a146cdf|Graph Partition-based Algorithm (GPA)|GPA is a distributed algorithm that addresses scalability challenges by partitioning the graph into disjoint subgraphs. It allows for efficient computation of PPVs on large-scale graphs.
2a0cdb22-d535-54b7-9bbf-510d906cf359|Hub Distributed Hierarchical Graph Partitioning (HDHGP)|HDHGP is a solution that addresses the load balancing challenge by partitioning the graph into a hierarchy of subgraphs and distributing the hub nodes across machines. This approach ensures that each machine handles a balanced workload, reducing the likelihood of bottlenecks.
307b0c20-50dc-54e2-a533-a81173857b3e|Hierarchical Graph Partitioning (HGP)|The authors propose a hierarchical graph partitioning approach to minimize communication overhead in distributed graph processing systems. This solution involves partitioning the graph into a hierarchy of subgraphs, where each subgraph is further divided into smaller subgraphs. This hierarchical structure allows for more efficient communication and computation by reducing the number of nodes that need to communicate with each other.
9c4ebc94-5aea-5e2a-a7f8-8a89f5a9c27d|Hub Node Partitioning (HNP)|The authors propose a hub node partitioning approach to minimize communication overhead in distributed graph processing systems. This solution involves identifying hub nodes in the graph and partitioning them across processors to reduce communication overhead.
4cb75a67-28bd-5ef6-8216-75c9a10dc137|Distributed PPV Computation (DPPV)|The authors propose a distributed PPV computation approach to minimize communication overhead in distributed graph processing systems. This solution involves computing personalized PageRank vectors in a distributed manner, using a combination of local and global computations.
47f5ed97-0368-5c43-9b23-f646540503cc|Distributed Betweenness Centrality Algorithm|The authors propose a distributed algorithm for computing betweenness centrality in large-scale graphs. This algorithm is designed to address the scalability challenges by enabling every node to compute its own centrality value in a distributed manner. The algorithm uses a modified version of the Bellman-Ford algorithm, which is a well-known algorithm for computing shortest paths in graphs. The authors modify this algorithm to compute betweenness centrality by exchanging messages with a bounded number of entries. This approach allows the algorithm to scale to large graphs while minimizing memory requirements. The authors demonstrate the effectiveness of their algorithm through simulations on various networks, including synthetic and real-world graphs. The results show that the algorithm converges in a number of distance vector phases proportional to the diameter of the network, and the amount of computations performed at each node is amortized constant.
da8625fb-5f57-56fa-aee9-cc0b0789ef05|MapReduce-based Algorithm for Approximate k-core Decomposition|The authors propose a MapReduce-based algorithm to compute an approximate k-core decomposition of a graph in O(log n) rounds of computation. This solution specifically addresses the challenge of scalability by utilizing the MapReduce framework to process large-scale graphs in parallel, minimizing memory requirements, and optimizing data structures and algorithms.
5c5c1673-05bf-5161-af36-2e0bc272054c|Streaming Algorithm for Approximate k-core Decomposition|The authors also propose a streaming algorithm to compute an approximate k-core decomposition of a graph in one pass using O(n) space. This solution specifically addresses the challenge of scalability by processing the graph in a streaming fashion, minimizing memory requirements, and optimizing data structures and algorithms.
842d516f-4234-5d17-b48b-887887ba8dd7|Distributed Algorithm for Approximate k-core Decomposition|The authors propose a distributed algorithm to compute an approximate k-core decomposition of a graph in O(log n) rounds of computation. This solution specifically addresses the challenge of scalability by utilizing a distributed framework to process large-scale graphs in parallel, minimizing memory requirements, and optimizing data structures and algorithms.
3725b8c2-dbf3-5174-9208-0adee2908734|Adaptive Edge Sampling|The authors propose an adaptive edge sampling technique to minimize communication overhead in distributed graph processing systems. This technique involves sampling edges with different probabilities based on the density of the graph area, allowing for more aggressive sampling in denser areas and less aggressive sampling in sparser areas.
1b78b961-af76-55e3-9589-2145edfcf54f|ScaleG-based Distributed Framework|The authors propose a distributed framework for MIS computation based on ScaleG, a synchronization-based model for vertex-centric computation. This framework allows for efficient processing of large-scale graphs by maintaining a copy of each vertex in each machine, enabling local access to all neighbors' information in the next superstep.
3f353c34-a91e-512d-b6c1-43daf92230ec|Lower-Ranking Activation|The authors propose an optimization technique called Lower-Ranking Activation, which selectively activates vertices in each superstep to reduce computation and communication cost.
2117dd0e-2f19-593f-bee8-dc1a54c83003|Same-Status Activation|The authors propose another optimization technique called Same-Status Activation, which further reduces the number of activated vertices by only activating vertices with the same status.
b8990836-3b5a-51c5-a965-134966090aad|Batch Update Algorithm|The authors propose a batch update algorithm that efficiently processes multiple edge updates in a single batch.
b48b7ad1-9260-5169-b7c7-11099836ea39|ScaleG-based Load Balancing|The authors propose using ScaleG, a synchronization-based model for vertex-centric computation, to address the load balancing challenge. ScaleG maintains a copy of each vertex in each machine, allowing for local access to all neighbors’ information in the next superstep. This approach enables efficient load balancing by reducing communication overhead and ensuring that each vertex can process its neighbors’ information without relying on remote access.
579f9040-634f-589b-befd-5e1c60f39011|ScaleG-based Computing Model|The authors propose using a ScaleG-based computing model to minimize communication overhead. This solution involves using a synchronization-based model for vertex-centric computation, which allows each vertex to locally access all neighbors' information in the next superstep.
7f5894fb-d8c7-54b5-871c-f9c168745452|Localized Distributed Algorithm for Vertex Cover Problem|The authors propose a localized distributed algorithm that uses 2-hop local neighborhood information to construct the vertex cover of a given network. This algorithm is designed to address the scalability challenges in distributed graph processing systems. The algorithm uses a scoring-based mechanism to minimize the number of nodes in the vertex cover by identifying the nodes that cover more edges than others. Each node decides about its neighbors’ status in or out of the vertex cover based on the total edges and 1-hop neighbors in the subgraph. The algorithm has a time complexity of O(Δ) and a space complexity of O(Δ^2), where Δ is the maximum node degree. The authors also provide experimental results showing that the proposed algorithm can find up to 11 smaller solutions than existing distributed algorithms with less than 1 difference of optimum solutions in most of the evaluated graphs.
d95dd427-f3cb-5216-a597-3b4e9659c872|2-Hop Local Neighborhood Information-Based Distributed Algorithm|The proposed algorithm uses 2-hop local neighborhood information to construct the vertex cover of a given network, reducing the need for global information and minimizing communication overhead. The algorithm employs a scoring-based mechanism to minimize the number of nodes in the vertex cover by identifying nodes that cover more edges than others. Each node decides about its neighbors' status in or out of the vertex cover based on the total edges and 1-hop neighbors in the subgraph. The algorithm has a time complexity of O(Δ) and 1 in the worst and best cases, respectively, and a bit complexity of O(2^2). The comprehensive simulation results showed that the proposed algorithm could find up to 11 smaller solutions than the existing distributed algorithms with less than 1 difference of optimum solutions in most of the evaluated graphs.
a3b6a4c7-cf34-5c22-9b61-7e0cd87e88a9|Message Aggregation Strategy|The authors propose a message aggregation strategy to reduce massive redundant messaging and avoid the memory over ow problem while processing large scale graphs. This strategy involves each vertex maintaining a message vector according to the priority of neighbors and sending the message vector to each machine instead of its neighbors.
e88e6ba6-2cfe-5aa4-bcc1-35848d261924|Task-Split Strategy|The authors propose a task-split strategy to maintain tip numbers when given bipartite graphs are updated. This strategy involves reconstructing a subgraph consisting of candidate vertices and peeling them according to their 2-hop neighbors’ tip numbers.
e56e8da2-f3f9-5143-b710-e4694a9736d2|Distributed Tip Maintenance Algorithm (DTMA)|The authors propose a distributed tip maintenance algorithm (DTMA) to maintain the tip numbers of vertices in a distributed environment. This algorithm involves a task-split strategy and a message aggregation strategy to reduce the number of messages and avoid the memory over ow problem.
c8206b45-276f-5f87-bbc2-185c55ce95d7|Task Split Strategy|The authors propose a task split strategy to address the load balancing challenge in distributed tip maintenance. This strategy involves dividing the task of maintaining tip numbers into smaller sub-tasks based on the topology structure of the original bipartite graph. Each sub-task is then processed independently, allowing for parallel processing and reducing the workload on individual nodes.
24fd445f-5693-5c0b-8b99-6de7952e8f42|Candidate Sharing Theory|The authors propose a candidate sharing theory to reduce redundant computation in sub-tasks with the same endpoints. This theory allows for the sharing of candidate vertices between sub-tasks, reducing the overall computational workload.
6cb72587-d8ee-5eb4-9bcf-0f9261d70653|Vertex Buffering|The authors propose vertex buffering as a solution to address the scalability challenges in graph processing. This involves buffering vertex data in the on-chip RAMs to reduce memory accesses and improve performance.
b60e1659-d2ad-5e29-95a8-797d5874be04|Update Combination Mechanism|The authors propose an update combination mechanism to reduce data communication between the FPGA and external memory. This involves combining updates that have the same destination vertex before writing them into the external memory.
e1c03b93-38d9-592d-845d-97d2c445efa8|Inter-Partition Parallelism|The authors propose inter-partition parallelism as a solution to address the scalability challenges in graph processing. This involves processing distinct partitions in parallel to increase throughput.
a45de90e-2541-57e9-918c-53f7a516955f|Intra-Partition Parallelism|The authors propose intra-partition parallelism as a solution to address the scalability challenges in graph processing. This involves processing edges and updates in parallel within each partition to increase throughput.
5327b52a-df81-577b-bb93-8f274e4e5736|Optimized Data Layout|The authors propose an optimized data layout to improve external memory performance and reduce data communication between nodes or processors. This data layout sorts the edges of each shard based on the destination vertices, allowing for consecutive updates with the same destination vertex to be easily combined.
3cf9d6f2-b26a-501b-9772-7b356d4bbcac|Partitioning Approach|The authors propose a simple vertex index-based partitioning approach to reduce data communication between nodes or processors. This approach enables an efficient use of the on-chip RAMs to buffer vertex data.
c026657d-cc2f-59c1-9ccf-31544fbaf836|TrIndexing|TrIndexing is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves precomputing and indexing the triangles (3-cycles) of the graph to facilitate pruning and reduce the search space.
57ed1b8a-4f00-590e-a01b-5dbd01f790fa|Batching|Batching is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves dividing the whole computation into sub-tasks that can be evaluated independently, allowing for better overlap of computation and communication.
0c6b5e81-19e2-5503-8328-e660afee3d2f|Compression|Compression is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves maintaining the intermediate results in a compressed form to reduce the maintaining and communication cost.
f72a8c3f-9968-5e89-8379-45e3fde1855f|CrystalJoin|CrystalJoin is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves computing the matches of the vertex cover of the query graph and then compressing the remaining vertices’ matches.
e501a51b-433b-5726-b3ab-d981f6392a49|FullRep|FullRep is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves maintaining the whole graph in each partition and parallelizing embarrassingly.
4986d569-152b-5f28-af74-84f8074a6c3e|WOptJoin|WOptJoin is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves computing all matches of the query vertices in a specific order, reducing the search space and improving performance.
f484e206-a962-5e6c-b5fa-42e42fb96d18|BinJoin|BinJoin is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves decomposing the query graph into a set of join units and solving a series of binary joins.
ae01ed9d-4a69-597a-890b-3086a6006a0e|ShrCube|ShrCube is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves dividing the computation into shares and assigning each share to a worker.
ed4b6764-c99a-51da-9ce5-414587853ca1|TwinTwigJoin|TwinTwigJoin is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves using a twin-twig join approach to reduce the search space and improve performance.
757b308a-3251-597f-9955-a978951b32e5|CliqueJoin|CliqueJoin is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves using a clique-based approach to reduce the search space and improve performance.
daf30653-4440-50fa-a72c-b2953a9cc614|BiGJoin|BiGJoin is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves using a worst-case optimal join algorithm to reduce the search space and improve performance.
364aca1f-e2c8-5ed2-8de0-1ab89d2583f7|PSgL|PSgL is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves using a parallel subgraph listing approach to reduce the search space and improve performance.
f29a56d9-2ae0-5fd2-a900-70721da8e09c|MultiwayJoin|MultiwayJoin is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves using a multiway join approach to reduce the search space and improve performance.
5521da59-f543-521b-8803-292a0284aee1|StarJoin|StarJoin is a solution proposed by the authors to address the scalability challenges in distributed subgraph matching. It involves using a star-based approach to reduce the search space and improve performance.
ac9f2ad9-18b0-56fb-9a82-6e0564bdda77|Workload Aware Expanding Strategy|The authors propose a workload aware expanding strategy to address the load balancing challenge. This strategy is used in the WOptJoin algorithm to select the next vertex to expand among all current gray vertices based on a greedy heuristic to minimize the communication cost. The strategy uses a greedy heuristic to select the next vertex to expand, which is different from existing approaches that use a fixed order or random selection. This approach allows for adaptive load balancing based on the current workload and graph structure. The paper does not provide specific results for this solution, but it is mentioned as a key component of the WOptJoin algorithm, which is shown to be effective in achieving good performance and scalability.
c8489667-2aab-5f3c-a587-215e0cd20c41|Hypercube Optimization|The authors propose a hypercube optimization technique to address the load balancing challenge. This technique is used in the ShrCube strategy to assign proper bucket numbers to each query vertex, which helps to balance the workload across workers. The technique uses a hypercube optimization algorithm to assign bucket numbers, which is different from existing approaches that use a simple round-robin or random assignment. This approach allows for more efficient load balancing and reduces the communication overhead. The paper does not provide specific results for this solution, but it is mentioned as a key component of the ShrCube strategy, which is shown to be effective in achieving good performance and scalability.
c5d3b74f-acd3-51f6-a36b-672882a2d9ee|Round Robin Strategy|The authors propose a round-robin strategy to address the load balancing challenge. This strategy is used in the BinJoin algorithm to divide the candidate data vertices for each query vertex evenly across workers. The strategy uses a simple round-robin assignment to divide the workload, which is different from existing approaches that use more complex algorithms or heuristics. This approach allows for easy implementation and scalability. The paper does not provide specific results for this solution, but it is mentioned as a key component of the BinJoin algorithm, which is shown to be effective in achieving good performance and scalability.
a78fd621-25c0-51b9-a779-03e5849fe53c|Proxy Edge-Based Partitioning Policy|The authors propose a novel application-agnostic graph partitioning strategy that eliminates almost all inter-host communication during triangle counting. This approach is based on the proxy model of partitioning, where edges are distributed among host machines and cached copies of the endpoints (called proxy vertices) are created.
fc0fac10-2019-51c0-8418-0082326bc686|Multi-Machine Multi-GPU Triangle Counting Implementation|The authors propose a distributed triangle counting implementation that leverages the proxy edge-based partitioning policy to eliminate inter-host communication. This implementation uses multiple GPUs across multiple machines to process large graphs in parallel.
4b6fa0f3-8005-5410-90f4-827a6a0f92f2|Binary Search-Based Intersection Method|The authors propose a binary search-based intersection method for finding triangles in the graph, which improves performance on GPUs due to improved exploitation of memory bandwidth.
c03d99c9-088e-5494-9903-38dc4b7261b9|Dynamic Load Balancing using Independent Units of Work|The authors propose a dynamic load balancing strategy that assigns independent units of work to GPUs on the fly. This approach allows for efficient utilization of GPU resources and minimizes idle time.
ccf86691-9aa3-5a54-b7c6-96e78c875b7f|Distributed Defective Coloring Algorithm|The authors propose a distributed defective coloring algorithm that can efficiently handle large-scale graphs with billions of edges and vertices. This algorithm is designed to address the scalability challenges by reducing the number of colors used in the coloring process, which in turn reduces the memory requirements and improves performance.
810c965e-663f-5d72-8cbf-c0796dedd891|Defective Coloring Algorithm|The authors propose a defective coloring algorithm to minimize communication overhead in distributed graph processing systems. This algorithm allows for a trade-off between the number of colors used and the running time, enabling the system to adapt to different scenarios and optimize communication.
92168e9c-11f7-551f-a921-529e71422145|Union 1-Cover Free Family Construction|The authors propose a method for constructing a union 1-cover free family, which is used to minimize communication overhead in distributed graph processing systems. This construction enables the system to efficiently select colors for nodes while minimizing conflicts.
d73c8153-17ce-5e0c-9308-a3600537711f|Distributed Delegate Partitioning|Distributed delegate partitioning is a technique used to partition an input graph by distributing high-degree vertices (hubs) across multiple partitions, while low-degree vertices remain 1D partitioned. This approach aims to balance the processing workload and reduce communication associated with high-degree vertices.
03ad88bc-f226-5a5a-b332-abc4c34c75b5|Asynchronous Visitor Queue Abstraction|The asynchronous visitor queue abstraction is a framework used to drive graph traversal and provide parallelism. It allows algorithm designers to define vertex-centric procedures to execute on traversed vertices with the ability to pass visitor state to other vertices.
41743196-12f4-5eb8-9922-8bd0de63e86f|Routed Point-to-Point Communication|Routed point-to-point communication is a technique used to reduce dense communication requirements by routing messages through a synthetic network, reducing the number of communicating pairs and increasing message latency.
6b4d1ebe-bcea-57e2-bfff-295e65eedaf8|Delegate Visitor Behaviors|Delegate visitor behaviors are a set of techniques used to coordinate the computation among delegates and their controller, including pre-visit parent, lazy merge parent, and post-merge behaviors.
91da70f7-62ec-5f13-8857-afd6312e0a38|Delegate Degree Threshold|The Delegate Degree Threshold is a technique that addresses the load balancing challenge by dynamically adjusting the degree threshold for distributing high-degree vertices across partitions. This approach involves setting a threshold value, dhigh, which determines the degree above which vertices are delegated to multiple partitions.
318a25a7-6c66-5784-b42b-993bf9394dcc|Asynchronous Broadcast and Reduction Operations|The authors propose using asynchronous broadcast and reduction operations to minimize communication overhead in distributed graph processing systems. This solution involves distributing high-degree vertices (hubs) across multiple partitions, allowing each partition to process a portion of the hub’s adjacency list. The controller and its delegates coordinate using asynchronous broadcast and reduction operations, reducing the volume of communication required.
01e5bc63-117d-5d6d-9ab4-48f66aed47e1|Delegate Partitioning|The authors propose a delegate partitioning technique to minimize communication overhead in distributed graph processing systems. This solution involves partitioning high-degree vertices (hubs) across multiple partitions, creating a local representative (delegate) for each partition. The delegates coordinate with the controller using asynchronous broadcast and reduction operations.
db4d9b43-ab89-518a-beec-783f4fbccc1f|ALGdmp|ALGdmp is a distributed algorithm designed to process multimodal path queries over large transportation networks. It addresses the scalability challenges by partitioning the graph into smaller fragments, processing each fragment in parallel, and combining the results to obtain the final answer.
b2d3c5a7-8ac8-5a4e-b972-bfeaff3c52c7|Graph Partitioning Approach|The graph partitioning approach is a technique used to divide the graph into smaller fragments, each of which is processed by a separate machine. This approach addresses the scalability challenges by reducing the amount of data that needs to be processed by each machine, thereby improving the overall performance.
bac30f23-e3d1-5115-be19-60bb0b79f3c7|Parallel Computing Framework|The parallel computing framework is a technique used to process each fragment of the graph in parallel, using multiple machines. This approach addresses the scalability challenges by minimizing the communication overhead and improving the overall performance.
22c66b03-ce33-5231-b853-38a2fa989e28|Distributed Multimodal Path Query Algorithm (ALGdmp)|ALGdmp is a distributed algorithm designed to process multimodal path queries over large transportation networks. It addresses the load balancing challenge by distributing the computational workload evenly across processing units, ensuring that no single node becomes a bottleneck. ALGdmp uses a combination of graph partitioning, parallel computing, and distributed processing to achieve load balancing. It partitions the multimodal graph into smaller fragments, each processed by a separate machine, and uses a coordinating machine to assemble the partial results. This approach allows for efficient processing of large-scale graphs and adapts to varying workloads. The paper presents experimental results showing that ALGdmp achieves better load balancing and scalability compared to existing algorithms, such as GraphLab, Pregel, and Quegel. Specifically, ALGdmp reduces the query time and network overhead, demonstrating its effectiveness in addressing the load balancing challenge.
69281c6a-f2e3-5b89-9a6e-d9eb933e5b14|Graph Partitioning Algorithm (ALGprt)|ALGprt is a graph partitioning algorithm designed to partition the multimodal graph into smaller fragments, each processed by a separate machine. It addresses the load balancing challenge by ensuring that the fragments are balanced in terms of the number of nodes and edges, reducing the likelihood of bottlenecks. ALGprt uses a combination of label propagation and multilevel partitioning to achieve balanced fragments. It assigns labels to nodes based on their connectivity and iteratively coarsens the graph until the coarsened graph is small enough, then uses a high-quality off-the-shelf partitioning algorithm to generate the final partitioning. The paper presents experimental results showing that ALGprt achieves better graph partitioning quality compared to existing algorithms, such as xtraPuLP, Fennel, and Ginger. Specifically, ALGprt reduces the replication ratio and balance factor, demonstrating its effectiveness in addressing the load balancing challenge.
6eabd579-a0ca-566c-af87-2fed38223fd2|Pregel|Pregel is a distributed graph processing system that uses a message-passing model to minimize communication overhead.
56bcf0dd-0065-58c0-b962-17379d9e5922|Quegel|Quegel is a distributed graph processing system that uses a query-centric framework to minimize communication overhead.
edefe9f6-c297-5073-b617-44b3efbccd04|Hybrid Approach using BFS and SV Algorithm|The authors propose a hybrid approach that combines parallel Breadth-First Search (BFS) and Shiloach-Vishkin (SV) algorithm to address the scalability challenges in computing connected components in undirected graphs. This approach dynamically selects the appropriate algorithm based on the graph topology, using BFS for scale-free graphs and SV for large diameter graphs.
88653148-0a94-554d-a862-b57db352f382|Load Balancing using Partitioning and Bucket Updates|The authors propose a load balancing technique that partitions the graph into smaller sub-graphs and updates the partition ids using bucket updates to reduce the communication volume and improve scalability.
e895ba64-014f-5dcb-9b42-c56e9d185733|Dynamic Approach using Degree Distribution Statistics|The authors propose a dynamic approach that uses degree distribution statistics to predict the graph structure and select the appropriate algorithm, using BFS for scale-free graphs and SV for large diameter graphs.
e2e1ef6b-a338-5b7d-8518-a637bc6b4fd5|Parallel SV Algorithm using Edge-Based Adaptation|The authors propose a parallel SV algorithm that uses an edge-based adaptation to improve scalability and reduce communication volume.
23a1fb42-2489-5f16-bc5f-06d0fe0ae843|Load Balancing through Iterative Removal of Completed Partitions|The authors propose a load balancing solution that involves iteratively removing completed partitions from the graph, thereby reducing the workload and achieving a more even distribution of tasks across processing units. The solution involves a dynamic approach where the algorithm continuously monitors the progress of each partition and removes completed ones, ensuring that the workload is rebalanced across the remaining partitions. This approach is different from existing methods that rely on static partitioning or load balancing strategies. The paper presents results showing that this solution leads to improved load balancing, with a reduction in the maximum workload by up to 50% and an increase in the minimum workload by up to 20% (Figure 5).
217c4855-e56d-5734-9685-bba50cee35fd|Bulk Synchronous Communication|The authors propose using bulk synchronous communication to reduce communication overhead and achieve better load balancing. The solution involves using a bulk synchronous communication model, where processors communicate in bulk, reducing the number of messages and overhead associated with communication. This approach is different from existing methods that rely on asynchronous communication or point-to-point messaging. The paper presents results showing that this solution leads to improved performance, with a reduction in communication overhead and an increase in scalability (Figure 8).
554877ec-4b40-54a8-b495-d0c670dfd020|Dynamic Preprocessing Phase|The authors propose a dynamic preprocessing phase to guide the algorithm selection at runtime, which helps in achieving better load balancing. The solution involves a dynamic approach where the algorithm analyzes the graph structure and selects the optimal algorithm based on the graph’s characteristics, ensuring that the workload is distributed evenly across processing units. This approach is different from existing methods that rely on static algorithm selection or preprocessing phases. The paper presents results showing that this solution leads to improved performance, with a reduction in runtime and an increase in scalability (Figure 7).
78830a14-3528-5412-be1b-6a2f58662b19|Parallel Pre x Scan Operations|The authors propose using parallel pre x scan operations to minimize communication overhead in distributed graph processing systems. This solution involves using custom operators to achieve independence from the size of partitions, requiring at most O log r communication steps in addition to the local linear time processing time.
90f99634-02cc-552e-80b7-20a1de46f1cd|Load Balancing through Dynamic Partitioning|The authors propose a load balancing technique that dynamically partitions the graph to minimize communication overhead. This solution involves removing completed components along iterations, reducing the size of the working set per each iteration.
e8c18934-0eed-56dd-806c-e23bdb3d12f4|Hybrid Implementation using BFS|The authors propose a hybrid implementation that uses Breadth-First Search (BFS) to minimize communication overhead in distributed graph processing systems. This solution involves executing a BFS iteration before switching to the SV algorithm to process the remaining graph.
7f9118c1-3320-5956-aaad-59a3fcf092ea|Distributed Core Groups Detection Algorithm|The authors propose a distributed core groups detection algorithm to address the scalability challenges in community detection. This algorithm is designed to process large-scale graphs with billions of edges and vertices by identifying core groups, which are small cohesive groups of vertices that belong to the same community.
c92672a1-07b3-5e2a-9fc2-c8232b33c23e|Hadoop-based Graph Processing Framework|The authors propose a Hadoop-based graph processing framework to address the scalability challenges in graph processing. This framework is designed to efficiently handle large-scale graphs by utilizing the distributed computing capabilities of Hadoop.
ea188d22-8e2f-51e4-ac38-d1c4540eb2bc|Ensemble Learning Scheme for Community Detection|The authors propose an ensemble learning scheme for community detection to address the scalability challenges in community detection. This scheme is designed to combine the information from multiple community detection algorithms, creating a diverse ensemble of base partitions.
a7614f76-0bd1-5313-9cc5-73f52ce3107a|Bundling Label Propagation|The authors propose bundling the propagation of labels in a way that increases the bit size of the value label vector instead of single labels of the key-value pairs. This approach reduces the total amount of data to be transferred, making larger data blocks more efficient to transfer and sorting key-value pairs only once.
6bc345e2-03c9-51b5-b578-92328f03c84e|Synchronous Label Update|The authors use a synchronous label update strategy, where a vertex gets in iteration t the label most of its neighbors had after iteration t-1. This approach reduces the need for heavy communication between nodes during an iteration of label updates.
3918678a-619b-54a4-897d-8a438eaecd61|Distributed Minor Aggregation Model|The authors propose a new interface to the low congestion shortcut framework called the Distributed Minor Aggregation model. This model restricts the operations nodes can perform, allowing them to only compute aggregates over their incident edges. The model provides powerful high-level primitives like graph contractions, enabling succinct descriptions of otherwise complicated distributed algorithms.
3f2b1c7b-5384-5c9a-b5f1-bd639cd3f3be|Low Congestion Shortcut Framework|The authors build upon the low congestion shortcut framework, which identifies the part-wise aggregation task as the crucial communication task in many optimization problems. They propose a new construction of 1-oblivious routings that is graph-based and can be efficiently implemented in the distributed setting.
ae257e4a-a963-507b-a562-a968bec45e22|1-Oblivious Routing Construction|The authors propose a new construction of 1-oblivious routings that is graph-based and can be efficiently implemented in the distributed setting. This construction is based on routing the demand along several low-diameter decompositions (LDDs) in a bottom-up way.
08c81894-0574-5733-a795-3cd6c7fb348f|Distributed Implementation of SSSP|The authors propose a distributed implementation of the single-source shortest paths (SSSP) algorithm, which is based on the low congestion shortcut framework and the Distributed Minor Aggregation model.
842db874-b727-59ba-b219-21d1fea77fbb|Graph-Based 1-Oblivious Routing|The authors propose a graph-based 1-oblivious routing algorithm that can be used to distribute computational workload evenly across processing units. This algorithm is designed to work in conjunction with the Distributed Minor Aggregation Model and provides a way to route demands in a distributed graph processing system.
694a32f6-4171-50ba-a11d-3cc5ba95d9eb|LDD Sampling Algorithm|The authors propose an LDD sampling algorithm that can be used to sample low-diameter decompositions in a distributed graph processing system. This algorithm is designed to work in conjunction with the graph-based 1-oblivious routing algorithm and provides a way to efficiently sample LDDs.
09cee210-bb09-54f1-8d4d-e26b09b7bf77|Low Congestion Shortcuts|The authors propose the use of low congestion shortcuts to reduce communication overhead in distributed graph processing systems. These shortcuts are designed to minimize the number of hops required for data to travel between nodes, thereby reducing the communication overhead.
f893494d-bb98-5aa8-ad76-d18ef8a00bb3|Distributed Deterministic 2-Approximation Algorithm for Weighted Vertex Cover|The authors propose a distributed deterministic 2-approximation algorithm for the weighted vertex cover problem, which can efficiently handle large-scale graphs. The algorithm is designed to work in the CONGEST model of distributed computation, where the communication network is the graph itself.
e9d3c942-fdb9-5228-91c0-00fd9c77a0e8|Modified BCS Algorithm|The authors propose a modified version of the BCS algorithm, which is a distributed deterministic 2-approximation algorithm for the weighted vertex cover problem. The modified algorithm is designed to work in the CONGEST model of distributed computation and can efficiently handle large-scale graphs.
7323551b-eb42-558a-994e-c9eb1a78c8ae|Level-Based Offer Allocation|The authors propose a level-based offer allocation mechanism to minimize communication overhead. In this approach, each vertex is assigned a level based on its weight, and offers are sent only to neighbors with the smallest level. This technique reduces the number of offers sent and received by each vertex, resulting in lower communication overhead.
69e53d6e-7fcb-58c3-8ce9-d0329e55fa2a|Vault and Bank Mechanism|The authors introduce a vault and bank mechanism to manage the weight of each vertex. The vault represents the portion of the weight available for offer allocation, while the bank represents the remaining weight. This mechanism ensures that the weight of each vertex is reduced efficiently, minimizing the number of iterations and communication overhead.
d369e172-fcbf-534f-a761-03b63b2c8246|Dynamic Level Adjustment|The authors propose a dynamic level adjustment mechanism, where the level of each vertex is adjusted based on its weight and the number of iterations. This mechanism ensures that the levels of vertices are updated efficiently, reducing the number of iterations and communication overhead.
0bf3ce21-b4c0-5954-8c73-62e77ed80e88|Random Layering Technique|The Random Layering Technique is a novel approach for analyzing the connectivity of a graph obtained through random edge sampling. This technique is used to develop a distributed minimum cut approximation algorithm that can efficiently handle large-scale graphs.
d9505809-f92a-5945-aecf-ec8c9a1aab0e|Distributed Minimum Cut Approximation Algorithm|The Distributed Minimum Cut Approximation Algorithm is a distributed algorithm that uses the Random Layering Technique to find a minimum cut in a graph. The algorithm is designed to handle large-scale graphs and can be used in distributed systems.
1c0302b8-7294-5fc4-8546-60776a16f5fa|Matula's Approach|Matula's Approach is a centralized algorithm that uses a maximum adjacency search procedure to find a 2 minimum cut in a graph. The algorithm is used as a basis for the Distributed Minimum Cut Approximation Algorithm.
7107b0c4-ed41-54df-af14-742ddeec085d|Lower Bound for Approximating Minimum Cut|The Lower Bound for Approximating Minimum Cut is a lower bound on the time complexity of approximating the minimum cut in a graph. The lower bound is used to show that the Distributed Minimum Cut Approximation Algorithm is almost optimal.
12a73b7b-6211-52b6-8b7e-21802e99b485|Random Layering Approach|The authors propose a random layering approach to minimize communication overhead in distributed graph processing systems. This approach involves randomly sampling edges from the graph and layering them to reduce the number of connected components. The algorithm iteratively adds sampled edges to the graph, ensuring that the number of connected components decreases by at least a constant factor with high probability.
bc9ac7fc-8fae-5cc6-aa0b-9f217f8dfb06|Matula's Approach with Distributed Sparse Certificates|The authors adapt Matula's centralized algorithm for finding a 2-approximation of the minimum cut to the distributed setting. The approach involves using distributed sparse certificates to identify a set of edges that are likely to be part of the minimum cut. The algorithm then uses these certificates to find a 2-approximation of the minimum cut.
a28d1734-15c9-5735-93c0-c6a067498217|Distributed Minimum Cut Approximation via Random Contraction|The authors propose a distributed algorithm for approximating the minimum cut using random contraction. The algorithm involves randomly contracting edges in the graph and then using a distributed sparse certificate algorithm to find a 2-approximation of the minimum cut.
c99986d3-27e2-5f7d-bbde-4fcd3693365f|Push-Pull Optimization|The Push-Pull optimization is a novel method for reducing communication in distributed triangle identification. It allows for the choice of direction for sending adjacency information, which can minimize communication volume.
2696caaa-f512-58de-af7f-b46916f2ac4c|Distributed Map Container|The distributed map container is a data structure designed to store key-value pairs at deterministic MPI ranks based on a hash of the keys. It enables efficient storage and retrieval of metadata associated with vertices and edges.
f802b2c3-06b8-5d2a-ae6a-fc0714b2e0be|Cyclic Partitioning|Cyclic partitioning is a graph partitioning technique that reduces the degree of high-degree vertices, alleviating computation and storage imbalances.
3b2f25a4-963b-5aa7-88f7-c5eb26c03ac9|Asynchronous Communication using YGM|YGM is an asynchronous communication library that enables scalable performance by leveraging buffering techniques and message serialization.
0c09e7f5-eb6b-5040-9ea4-9716c07bea00|Message Buffering|The authors employ a message buffering strategy to reduce communication overhead. This involves aggregating small messages into larger ones to minimize the number of messages exchanged between nodes.
be64a28e-00a8-51ab-8e18-6684e5bc1a5e|Serialization|The authors use serialization to enable the communication of structured data, including variable-length objects and heterogeneous types.
d143783e-32d5-5c07-8899-210edccdd1c9|RPC Semantics|The authors utilize RPC semantics to enable the execution of user-provided functions on remote nodes.
626b2305-f860-57f2-87fb-974ff0ebface|Message-Splitting Strategy|The authors propose a message-splitting strategy to address the scalability challenges in distributed graph processing. This strategy involves separating local messages and remote messages according to their destination vertices, which determines the destination machine to which the message needs to be sent. By separating local and remote messages, the strategy enables each machine to send and receive messages to/from only one machine in each round, eliminating the need to cache messages in the message buffer. This approach differs from existing methods, which often require caching messages in the message buffer, leading to memory overflow issues. The paper demonstrates the effectiveness of the message-splitting strategy through experiments on real-world graphs, showing that it can significantly reduce memory usage and improve performance.
985a1660-7715-5448-83e6-ba465a86223b|Priority-Based Task Assignment|The authors propose a priority-based task assignment strategy to address the scalability challenges in distributed graph processing. This strategy involves assigning tasks to machines based on their priority, which is determined by the edge core of the task. By prioritizing tasks with smaller edge cores, the strategy enables the algorithm to focus on the most critical tasks first, reducing the number of candidates and improving performance. The paper demonstrates the effectiveness of the priority-based task assignment strategy through experiments on real-world graphs, showing that it can significantly improve performance and reduce the number of candidates.
100dbfee-473b-52be-8f31-86f5ec4f95f1|Superstep Sharing|The authors propose a superstep sharing strategy to address the scalability challenges in distributed graph processing. This strategy involves sharing supersteps among tasks to reduce the number of supersteps required. By sharing supersteps among tasks, the strategy enables the algorithm to process multiple tasks in parallel, reducing the number of supersteps and improving performance. The paper demonstrates the effectiveness of the superstep sharing strategy through experiments on real-world graphs, showing that it can significantly reduce the number of supersteps and improve performance.
fab6a6cc-2cdd-544c-8e49-da77ac107976|Distributed Batch-Stream Combined Algorithm (DBCA)|The authors propose a distributed batch-stream combined algorithm (DBCA) to address the scalability challenges in distributed graph processing. This algorithm involves processing tasks in a batch-stream combined model, where tasks are processed in batches and streaming tasks are stored in a task map. By processing tasks in a batch-stream combined model, the algorithm enables the system to process multiple tasks in parallel, reducing the number of supersteps and improving performance. The paper demonstrates the effectiveness of the DBCA algorithm through experiments on real-world graphs, showing that it can significantly improve performance and reduce the number of supersteps.
a26fb5ad-ad97-5270-b9f7-bbc4557dbf4e|Message Splitting Strategy|The message splitting strategy is designed to address the load balancing challenge by efficiently distributing messages across machines in a distributed graph processing system. This strategy separates local messages and remote messages according to their destination vertices, which determines the destination machine to which the message needs to be sent.
0f5529ae-270d-51d8-bb83-b9ab04b3b70e|Task Assignment Strategy|The task assignment strategy is designed to address the load balancing challenge by dynamically assigning tasks to machines based on the diversity of the edge-cores of updated edges. This strategy ensures that tasks with different edge-cores are executed in parallel, reducing the overall processing time and improving system efficiency.
ed041b2f-9e32-575f-a129-518aac0969d9|Priority Strategy for Interference Reduction|The authors propose a priority strategy to reduce interference between tasks and minimize communication overhead. This strategy involves assigning priorities to tasks based on their edge cores and processing them in order of priority.
01b766d0-c80a-52df-b80b-abc04f719e6c|Message Interaction Protocol|The authors propose a message interaction protocol to minimize communication overhead by reducing the number of messages exchanged between nodes. This protocol involves using a novel message format and processing messages in a specific order to reduce communication overhead.
32c021d9-c06f-5445-8548-089aa0a9cb85|Bounded Rational Behavioral (BRB) Update Rule|The BRB update rule is a distributed algorithm designed to address the scalability challenges in solving the vertex cover problem in large-scale graphs. It uses a bounded rational behavioral approach, where each vertex makes decisions based on local information and a finite memory of past interactions.
c100807e-4b08-57f3-a1a5-59ef739c2bf2|Memory-Based Best Response (MBR) Update Rule|The MBR update rule is a distributed algorithm designed to address the scalability challenges in solving the vertex cover problem in large-scale graphs. It uses a memory-based approach, where each vertex records the best responses of its neighbors and updates its strategy accordingly.
6c68bd19-c56a-5581-afd2-ecdfbecf43e7|Asynchronous Computation with Remote Memory Access (RMA)|The authors propose an asynchronous computation approach that utilizes Remote Memory Access (RMA) to retrieve remote parts of the graph without involving target nodes. This solution addresses the scalability challenge by removing synchronization overheads and allowing different processes to progress asynchronously while storing only partitions of the graph.
8293f6c0-e8ae-585d-8f9d-92ee90270995|Caching Mechanism for RMA|The authors propose a caching mechanism for RMA to reduce communication time by storing frequently accessed remote data. This solution addresses the scalability challenge by minimizing memory requirements and optimizing data structures.
27e26de8-f911-5b07-b37c-79d83a86ac08|Hybrid Strategy for Triangle Computation|The authors propose a hybrid strategy for triangle computation that combines binary search and sorted set intersection (SSI) methods. This solution addresses the scalability challenge by optimizing algorithms and maximizing bandwidth utilization.
93be38c5-0e2d-518f-91e6-dc2188593cbb|Application-Defined Scores for Cached Entries|The authors propose using application-defined scores for cached entries to improve caching efficiency. This solution addresses the scalability challenge by optimizing data structures and algorithms.
c3868762-dce9-5dac-947c-a5a765299bcd|1D Partitioning with Vertex Delegation|The authors propose a 1D partitioning approach where vertices are distributed among computing nodes, and each node is responsible for computing the local clustering coefficient (LCC) of its assigned vertices. To address the load balancing challenge, the authors introduce a vertex delegation mechanism, where a node can delegate the computation of a vertex's LCC to another node if the vertex has a high degree and is likely to cause load imbalance.
70af4186-5e4b-5df4-95cf-63e79d252dea|Cyclic Distribution|The authors mention cyclic distribution as an alternative approach to 1D partitioning, which can achieve more balanced partitions. However, they note that this approach requires sorting vertices, introducing additional computation and communication.
dc3278bd-24fe-557f-a560-7fc6564cd8f7|Asynchronous RMA-based Distributed LCC Computation|The authors propose an asynchronous distributed memory algorithm for local clustering coefficient (LCC) computation, which minimizes communication overhead by utilizing Remote Memory Access (RMA) one-sided operations to retrieve remote parts of the graph.
87987f9d-487a-5dc3-912d-c46236c5cadd|Caching Mechanism for RMA Accesses|The authors propose a caching mechanism for RMA accesses, which reduces communication overhead by storing frequently accessed remote data in a local cache.
2235673d-fee9-5b23-af40-c2d7de9dcc4d|TwinTwigJoin Algorithm|The TwinTwigJoin algorithm is a novel approach to subgraph enumeration in MapReduce, designed to address scalability challenges by utilizing a left-deep join framework and a TwinTwig decomposition strategy.
0b897965-0ed7-521d-bd87-5d74f39bb494|Order-Aware Cost Reduction|The order-aware cost reduction technique is a mechanism used in the TwinTwigJoin algorithm to further reduce the computational cost by considering the partial order of the pattern graph.
2ce1e7fd-dc99-5d37-9fba-7ac48c2ecd0d|Workload Skew Reduction|The workload skew reduction technique is a mechanism used in the TwinTwigJoin algorithm to reduce the workload skew caused by high-degree nodes in the data graph.
12a4de72-631b-5002-be1a-9c042f6dc8aa|Early Filtering|The early filtering technique is a mechanism used in the TwinTwigJoin algorithm to further reduce the number of partial results generated during the join process.
08eb8859-ac56-5675-884e-ff385b731f78|TwinTwig Decomposition|TwinTwig decomposition is a pattern decomposition strategy that decomposes the pattern graph into TwinTwigs, which are either an edge or two incident edges of a node. This decomposition strategy is used to reduce the communication overhead by minimizing the number of edges that need to be transmitted between nodes.
2ffa0458-fb6f-5612-a52a-f0a1a9e8ec13|Distributed Vertex Elimination Algorithm|The authors propose a distributed vertex elimination algorithm to address the scalability challenges in graph pattern matching. This algorithm iteratively eliminates vertices that do not meet the local constraints of the query pattern, reducing the search space and improving scalability.
d48830d1-c92b-5eb9-87c9-877ab9b5486f|Cycle Checking Visitor|The authors propose a cycle checking visitor algorithm to address the scalability challenges in graph pattern matching. This algorithm checks for the presence of cycles in the graph, which is a critical step in graph pattern matching.
506d41fb-9f55-5d4d-b207-d1af87df7974|Metadata Store|The authors propose a metadata store to address the scalability challenges in graph pattern matching. This store provides efficient access to vertex metadata, which is critical for graph pattern matching.
077f87eb-9088-5dff-9ea8-97f6511de485|Delegate Partitioned Graph|The authors propose a delegate partitioned graph approach to address the load balancing challenge. This involves distributing the edges of each high-degree vertex across compute nodes to achieve load balancing. The delegate partitioned graph approach is unique in that it focuses on distributing the edges of high-degree vertices, rather than simply partitioning the graph based on vertices. This approach is designed to mitigate the impact of high-degree vertices on load balancing. The authors demonstrate that this approach achieves excellent scalability and load balancing, as shown in their strong scaling experiments.
f27ecb37-d182-50e8-9ff0-32a18b92873e|Asynchronous Visitor Abstraction|The authors propose an asynchronous visitor abstraction to address the load balancing challenge. This involves allowing the implementation of graph algorithms as vertex programs using an asynchronous visitor abstraction, which enables the execution of a user-defined vertex program on traversed vertices. The asynchronous visitor abstraction is unique in that it allows for the execution of vertex programs in an asynchronous manner, which helps to reduce communication overhead and improve load balancing. The authors demonstrate that this approach achieves excellent scalability and load balancing, as shown in their strong scaling experiments.
19cb6a2e-5b22-5a01-981f-74183b1c02e5|Vertex Elimination|The authors propose a vertex elimination approach to address the load balancing challenge. This involves iteratively eliminating vertices that do not meet local constraints, which helps to reduce the computational workload and improve load balancing. The vertex elimination approach is unique in that it focuses on eliminating vertices that do not meet local constraints, rather than simply distributing the workload across nodes. This approach is designed to reduce the computational workload and improve load balancing. The authors demonstrate that this approach achieves excellent scalability and load balancing, as shown in their strong scaling experiments.
e33e48bb-976f-5f1d-83a8-10ccfcda045d|Distributed Cycle Checking|The authors propose a distributed cycle checking algorithm to minimize communication overhead in graph pattern matching. This approach iterates over the set of cycle constraints to be checked and validates each cycle constraint one at a time, reducing the need for excessive communication.
b3f1cfce-75de-59db-981f-59a1f2441e72|Local Constraint Checking|The authors propose a local constraint checking algorithm to minimize communication overhead in graph pattern matching. This approach iteratively excludes vertices that do not meet the constraints specified by the pattern, reducing the need for excessive communication.
5873dd6d-a76f-523e-911a-421078683ee0|Distributed Aggregation Scheduling (DAS) Algorithm|The DAS algorithm is designed to address the scalability challenges in wireless sensor networks by providing a distributed scheduling algorithm that can efficiently handle large-scale networks.
01d98177-4b7e-5912-ab5d-2d051c5919b1|Adaptive DAS Algorithm|The adaptive DAS algorithm is designed to address the scalability challenges in wireless sensor networks by providing an adaptive scheduling algorithm that can efficiently handle dynamic network changes.
da9ca2d9-471d-5fdb-9279-a72c6070a368|Adaptive Scheduling using DAS|The adaptive scheduling method is designed to handle dynamic network changes, such as node failures or new nodes joining the network. It updates the aggregation tree and schedules using the DAS algorithm to minimize communication overhead.
c14d8734-8f54-5475-a159-3c936b820cca|Trie-Based Data Structure|The authors propose a novel trie-based data structure to address the scalability challenges in subgraph isomorphism. This data structure is designed to reduce the memory footprint while maintaining good memory access efficiency. The trie-based data structure is built on the realization that as the length of the partial match path increases, the number of times the same prefix match path is used also increases. This allows for reusing all prefix paths without the disadvantages of Compressed Sparse Fibre (CSF) format. The data structure is capable of representing the data in a way that minimizes memory usage and enables efficient access. The authors demonstrate that their trie-based data structure achieves a compression ratio of up to 2.456664, as shown in Table 1, which indicates a significant reduction in memory usage.
437c91fd-25f0-5ea4-ab76-d570a93b20d7|Distributed GPU Implementation|The authors propose a distributed GPU implementation to address the scalability challenges in subgraph isomorphism. This implementation allows for handling larger data graphs and complex query graphs by distributing the workload across multiple GPUs. The distributed implementation uses a protocol to ensure that only one busy node sends data to a given free node, and a given busy node only sends data to one free node. This approach minimizes synchronization requirements and achieves good load balancing. The authors demonstrate that their distributed implementation achieves close to linear speedup on big data graphs, with a speedup of up to 3.1x on four nodes, as shown in Figure 4.
5e63086d-e484-5b9d-88aa-56f81f035021|Chunking-Based Load Balancing|The authors propose a chunking-based load balancing approach to address the scalability challenges in subgraph isomorphism. This approach divides the workload into smaller chunks and assigns them to different GPUs to achieve good load balancing. The chunking-based approach uses a chunk size of 512, which is empirically found to achieve good performance. This approach allows for processing bigger graphs without sacrificing performance. The authors demonstrate that their chunking-based load balancing approach achieves good performance and scalability, as shown in the single node results in Table 3.
aa86abba-0836-5ec4-a726-e19528c7e71a|Work Distribution Strategy|The authors propose a work distribution strategy to address the load balancing challenge. This strategy involves splitting each outer iteration into several chunks, and each chunk is processed sequentially. At the end of each chunk, every busy node checks if any free node is available to share the workload. A node is considered as free if it has finished all its work across all iterations. Once the node finishes all its work, it saves the final results and disconnects from the system.
dd959439-f5bf-5a2e-8b2d-d3b9e0dfd50a|Bin-Based Load Balancing|The authors propose a bin-based load balancing approach to address the load balancing challenge. This approach involves grouping the paths based on the work into bins and using virtual warps to process the bins. For example, if the number of neighbors of a node is eight, the partial path is put in bin eight. In the next iteration, all paths in bin eight will be processed by a virtual warp of 8 threads.
b3c71f63-e28f-5194-a769-2757b8d17d0e|Asynchronous Protocol for Work Distribution|The authors propose an asynchronous protocol for work distribution to minimize communication overhead between nodes. This protocol allows busy nodes to send a portion of their work to free nodes along with the required trie, without requiring synchronization.
f32626b4-aab7-5ba8-af49-b96d8be98f3b|Partition-Centric Processing Methodology (PCPM)|PCPM is a novel approach that perceives a graph as a set of links between nodes and partitions, rather than nodes and edges. This abstraction enables the development of efficient algorithms for graph processing.
f6fc8797-0468-59a7-ace2-f1e48c9ca7c2|Node Ordering using GOrder Algorithm|The GOrder algorithm is used to reorder the nodes in the graph, which improves the spatial locality of the graph data and reduces the number of random memory accesses.
8c58434a-a42f-5463-9121-73e31d6c4028|Binning with Partition-Centric Abstraction|Binning is a technique used to reduce the number of random memory accesses by grouping the nodes in the graph into bins based on their partitions.
a01cde0e-25f9-51f0-b08d-552c75681597|Streaming Memory Access Patterns|The streaming memory access patterns are used to improve the memory bandwidth utilization by accessing the graph data in a sequential manner.
15ab9153-9972-5717-877f-ceb77324ec29|Cache-Aware Locality-Based Graph Processing|The cache-aware locality-based graph processing approach is used to improve the performance of graph processing algorithms by reducing the number of cache misses.
8eb2979d-5695-5676-bbd9-a4c799ce5901|Dynamic Load Balancing using OpenMP|The authors propose using OpenMP dynamic scheduling to achieve load balancing in the PCPM scatter and gather phases. This approach allows for dynamic distribution of workload across threads, ensuring that no single thread becomes a bottleneck due to uneven data distribution.
35114c2c-678e-5dd6-a073-c6bcab16686b|Partition-Centric Update Propagation|This solution involves transmitting a single update from a node to a neighboring partition, rather than updating all outgoing edges of a node. This approach reduces the number of DRAM accesses and gets rid of the inherent sub-optimality of the GAS model.
4a44f628-2a4a-5f43-8eab-5f93b6703d12|Binning Mechanism|The binning mechanism involves inserting messages into bins of destination nodes, which reduces the number of random memory accesses and improves spatial locality.
2577c54b-6946-5918-8803-954b68d169a0|Branch Avoidance Mechanism|The branch avoidance mechanism involves removing data-dependent branches in the PCPM gather phase, which improves the sustained memory bandwidth.
c5648344-a30a-55ec-97a9-afffe19964e3|Hybrid Parallelization Approach|The authors propose a hybrid parallelization approach that combines thread-based parallelism within each compute node and distributed computation across multiple compute nodes. This approach allows for efficient processing of large-scale graphs by leveraging both shared-memory and distributed systems.
17f2e7e3-de14-5518-a6ec-817b37e000b1|External Neighbor Expansion|The authors propose an external neighbor expansion technique to avoid false negatives when determining frequent patterns. This technique ensures that all edges needed for computing embeddings are available locally.
5fc46733-0d56-5857-8232-3efe25f3326e|Support Bounding Technique|The authors propose a support bounding technique to minimize communication and determine the global support of patterns. This technique involves computing lower and upper bounds for the global support of each pattern.
52dae157-9bbd-55cb-b9d1-7c1277c28fff|Embedding-Centric Approach|The authors propose an embedding-centric approach to graph mining, which focuses on the embeddings of patterns rather than the patterns themselves. This approach allows for more efficient processing of large-scale graphs.
dd499252-5795-57ea-b9e9-07711e6d04a7|Hybrid Load Balancing Approach|The authors propose a hybrid load balancing approach that combines thread-based parallelism within each compute node and distributed computation across multiple compute nodes. This approach allows for a flexible distribution of workload across the system, enabling efficient processing of large-scale graphs.
5b1addc4-1ead-5a9b-a517-3bd1c48d2f04|Adaptive Partitioning|The authors propose an adaptive partitioning strategy that dynamically adjusts the number of partitions based on the graph structure and available computational resources. This approach ensures that the workload is evenly distributed across processing units, minimizing the impact of irregular graph structures and varying vertex degrees.
3651c986-ab12-5925-af9d-c6799bf47e70|Collective Communication Operations|The authors propose the use of collective communication operations, such as AllToAll and AllGather, to minimize communication overhead and ensure efficient data exchange between processing units. This approach enables the system to scale to large-scale graphs while maintaining optimal performance.
f9e21883-8172-515a-94e4-2b03cabded44|Support Bound Pruning|The authors propose a support bound pruning technique that reduces the number of candidate patterns by pruning those that are unlikely to be frequent. This approach minimizes the computational workload and reduces the communication overhead.
67b33573-dd69-5e49-8f17-4467cee5c660|Collective Communication Primitives|The authors propose the use of collective communication primitives, such as AllToAll and AllGather, to minimize communication overhead. These primitives enable the authors to communicate data between nodes in a efficient and scalable manner.
